{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practice 1: RNN Fundamentals - The Vanishing Gradient Problem"
      ],
      "metadata": {
        "id": "RZ58JbYIfPcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 1: Setup and Imports"
      ],
      "metadata": {
        "id": "2pLhfEy17QtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import time\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "0VWxqev-7aEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 2: Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "fCzL6AAc7tmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading IMDB dataset...\")\n",
        "\n",
        "# TODO: Load the IMDB dataset\n",
        "df = ...\n",
        "\n",
        "print(f\"Dataset loaded: {len(df)} reviews\")\n",
        "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")\n",
        "\n",
        "# TODO: Convert sentiment labels to binary\n",
        "df['label'] = ...\n",
        "\n",
        "# TODO: Use 30,000 samples\n",
        "df = ...\n",
        "\n",
        "# Simple text preprocessing\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Basic text preprocessing\"\"\"\n",
        "    text = text.lower()\n",
        "    # TODO\n",
        "    text = ... # Remove HTML tags\n",
        "    text = ...  # Keep only letters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "# Build vocabulary\n",
        "print(\"Building vocabulary...\")\n",
        "\n",
        "# TODO: Tokenize reviews and build vocabulary\n",
        "all_words = []\n",
        "...\n",
        "\n",
        "# Create vocabulary with special tokens\n",
        "word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "for idx, (word, count) in enumerate(most_common, start=2):\n",
        "    word_to_idx[word] = idx\n",
        "\n",
        "vocab_size = len(word_to_idx)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# TODO: Tokenize and convert to indices\n",
        "def text_to_indices(text, word_to_idx, max_len=None):\n",
        "    \"\"\"Convert text to sequence of indices\"\"\"\n",
        "    words = text.split()\n",
        "    indices = ...\n",
        "    ...\n",
        "    return indices\n",
        "\n",
        "# Split data: 80% train, 10% validation, 10% test\n",
        "train_size = int(0.8 * len(df))\n",
        "val_size = int(0.1 * len(df))\n",
        "\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:train_size + val_size]\n",
        "test_df = df[train_size + val_size:]\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "yMwDW0x17wrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 3: Dataset Class and DataLoaders"
      ],
      "metadata": {
        "id": "SzJ9RAFz-0Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_to_idx, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        indices = text_to_indices(text, self.word_to_idx, self.max_len)\n",
        "        return torch.LongTensor(indices), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# TODO\n",
        "def create_dataloaders(train_df, val_df, test_df, word_to_idx, max_len, batch_size=32):\n",
        "    \"\"\"Create dataloaders for a specific sequence length\"\"\"\n",
        "    train_dataset = ...\n",
        "    val_dataset = ...\n",
        "    test_dataset = ...\n",
        "\n",
        "    train_loader = ...\n",
        "    val_loader = ...\n",
        "    test_loader = ...\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "JbOYb28c-2FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 4: Model Definitions"
      ],
      "metadata": {
        "id": "QO_gm8CV-9sQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=1):\n",
        "        super(SimpleRNNClassifier, self).__init__()\n",
        "        # TODO\n",
        "        self.embedding = ...\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Define Simple RNN layer\n",
        "        self.rnn = ...\n",
        "\n",
        "        self.fc = ...\n",
        "        self.sigmoid = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = ...  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # TODO: Pass through RNN and get output\n",
        "        ...\n",
        "\n",
        "        # TODO: Use the last timestep's output\n",
        "        ...\n",
        "\n",
        "        out = self.fc(rnn_out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out.squeeze()\n",
        "\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=1):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        # TODO\n",
        "        self.embedding = ...\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # TODO: Define LSTM layer\n",
        "        self.lstm = ...\n",
        "\n",
        "        self.fc = ...\n",
        "        self.sigmoid = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len)\n",
        "        embedded = ...\n",
        "\n",
        "        # TODO: Pass through LSTM and get output\n",
        "        ...\n",
        "\n",
        "        # Use the last timestep's output\n",
        "        ...\n",
        "\n",
        "        out = self.fc(lstm_out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out.squeeze()"
      ],
      "metadata": {
        "id": "gqCzOTzs-_Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SECTION 5: Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "-3OUqZvoAY8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
        "        # TODO\n",
        "        data, target = ...\n",
        "\n",
        "        # TODO: Complete the training steps\n",
        "        ...\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (output > 0.5).float()\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "    return total_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            # TODO:\n",
        "            data, target = ...\n",
        "            ...\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predicted = (output > 0.5).float()\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    return total_loss / len(data_loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs, device):\n",
        "    \"\"\"Complete training loop\"\"\"\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses, val_accs = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    return train_losses, train_accs, val_losses, val_accs\n"
      ],
      "metadata": {
        "id": "uq7fX0nnAZ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENT A: Sequence Length Impact Analysis"
      ],
      "metadata": {
        "id": "JiVWkudGAm_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT A: Sequence Length Impact Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sequence lengths to test\n",
        "sequence_lengths = [50, 100, 200, 400]\n",
        "results = {\n",
        "    'sequence_length': [],\n",
        "    'simple_rnn_acc': [],\n",
        "    'lstm_acc': []\n",
        "}\n",
        "\n",
        "# Hyperparameters\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Train models for each sequence length\n",
        "for seq_len in sequence_lengths:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training models with sequence length: {seq_len}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # TODO: Create dataloaders\n",
        "    ...\n",
        "\n",
        "    # TODO: Train SimpleRNN\n",
        "    print(f\"\\nTraining SimpleRNN (seq_len={seq_len})...\")\n",
        "    simple_rnn = ...\n",
        "    criterion = ...\n",
        "    optimizer = ...\n",
        "\n",
        "    start_time = time.time()\n",
        "    ...\n",
        "    rnn_train_time = time.time() - start_time\n",
        "\n",
        "    # TODO: Evaluate SimpleRNN on test set\n",
        "    ...\n",
        "    print(f\"SimpleRNN Test Accuracy: {rnn_test_acc:.2f}%\")\n",
        "    print(f\"Training time: {rnn_train_time:.2f}s\")\n",
        "\n",
        "    # TODO: Train LSTM\n",
        "    print(f\"\\nTraining LSTM (seq_len={seq_len})...\")\n",
        "    lstm_model = ...\n",
        "    optimizer = ...\n",
        "\n",
        "    start_time = time.time()\n",
        "    ...\n",
        "    lstm_train_time = time.time() - start_time\n",
        "\n",
        "    # TODO: Evaluate LSTM on test set\n",
        "    ...\n",
        "    print(f\"LSTM Test Accuracy: {lstm_test_acc:.2f}%\")\n",
        "    print(f\"Training time: {lstm_train_time:.2f}s\")\n",
        "\n",
        "    # Store results\n",
        "    results['sequence_length'].append(seq_len)\n",
        "    results['simple_rnn_acc'].append(rnn_test_acc)\n",
        "    results['lstm_acc'].append(lstm_test_acc)\n",
        "\n",
        "# Create visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results['sequence_length'], results['simple_rnn_acc'],\n",
        "         marker='o', linewidth=2, markersize=8, label='Simple RNN')\n",
        "plt.plot(results['sequence_length'], results['lstm_acc'],\n",
        "         marker='s', linewidth=2, markersize=8, label='LSTM')\n",
        "plt.xlabel('Sequence Length (tokens)', fontsize=12)\n",
        "plt.ylabel('Test Accuracy (%)', fontsize=12)\n",
        "plt.title('Model Performance vs Sequence Length', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiment_a_accuracy_vs_length.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print results table\n",
        "print(\"\\nResults Summary:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'Seq Length':<15} {'SimpleRNN Acc':<20} {'LSTM Acc':<20}\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(len(results['sequence_length'])):\n",
        "    print(f\"{results['sequence_length'][i]:<15} {results['simple_rnn_acc'][i]:<20.2f} {results['lstm_acc'][i]:<20.2f}\")\n"
      ],
      "metadata": {
        "id": "5pBJK5ldApHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENT B: Gradient Flow Measurement"
      ],
      "metadata": {
        "id": "qqzVgow_A5bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT B: Gradient Flow Measurement\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# We'll use sequence length 200 for this experiment\n",
        "SEQ_LEN = 200\n",
        "\n",
        "# TODO: Create dataloaders\n",
        "...\n",
        "\n",
        "# TODO: Initialize fresh models\n",
        "...\n",
        "\n",
        "# TODO: Train models\n",
        "print(\"Training SimpleRNN for gradient analysis...\")\n",
        "...\n",
        "\n",
        "print(\"\\nTraining LSTM for gradient analysis...\")\n",
        "...\n",
        "\n",
        "# Measure gradients w.r.t. embeddings at each timestep\n",
        "def measure_gradient_flow(model, data, target, seq_len):\n",
        "    \"\"\"\n",
        "    Measure gradient magnitude at each timestep by computing gradients\n",
        "    with respect to the embedding at each position.\n",
        "\n",
        "    This properly captures the vanishing gradient problem by showing\n",
        "    how gradient signal decays as it propagates backwards through time.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Get embeddings with gradient tracking\n",
        "    embedded = ...  # (batch_size, seq_len, embedding_dim)\n",
        "    embedded.retain_grad()  # Important: retain gradients for intermediate tensor\n",
        "\n",
        "    # TODO: Forward pass through RNN/LSTM\n",
        "    ...\n",
        "\n",
        "    # TODO: Use last timestep for prediction (as in original model)\n",
        "    ...\n",
        "    out = model.fc(last_output)\n",
        "    out = model.sigmoid(out)\n",
        "\n",
        "    # TODO: Compute loss\n",
        "    loss = ...\n",
        "\n",
        "    # TODO: Backward pass\n",
        "    ...\n",
        "\n",
        "    # Extract gradients at each timestep position\n",
        "    if embedded.grad is not None:\n",
        "        # TODO: Compute L2 norm at each timestep, averaged over batch and embedding dimension\n",
        "        ...\n",
        "    else:\n",
        "        return np.zeros(seq_len)\n",
        "\n",
        "\n",
        "# Get a batch from validation set\n",
        "val_iter = iter(val_loader)\n",
        "data_batch, target_batch = next(val_iter)\n",
        "\n",
        "# TODO\n",
        "print(\"\\nCapturing gradients for SimpleRNN...\")\n",
        "rnn_grads = ...\n",
        "\n",
        "# TODO\n",
        "print(\"Capturing gradients for LSTM...\")\n",
        "lstm_grads = ...\n",
        "\n",
        "# VISUALIZATION\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Linear scale\n",
        "ax1 = axes[0]\n",
        "timesteps = np.arange(1, SEQ_LEN + 1)\n",
        "\n",
        "ax1.plot(timesteps, rnn_grads, linewidth=2, label='Simple RNN', alpha=0.8, color='#e74c3c')\n",
        "ax1.plot(timesteps, lstm_grads, linewidth=2, label='LSTM', alpha=0.8, color='#3498db')\n",
        "\n",
        "ax1.set_xlabel('Timestep (earlier ← → later)', fontsize=12)\n",
        "ax1.set_ylabel('Gradient Magnitude (L2 Norm)', fontsize=12)\n",
        "ax1.set_title('Gradient Flow Through Time - Linear Scale', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11, loc='upper right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotation pointing to early timesteps\n",
        "ax1.annotate('Early timesteps\\n(vanishing gradients)',\n",
        "             xy=(20, rnn_grads[19]), xytext=(50, rnn_grads[19] * 3),\n",
        "             arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
        "             fontsize=10, color='red')\n",
        "\n",
        "# Plot 2: Log scale (better for seeing vanishing gradients)\n",
        "ax2 = axes[1]\n",
        "\n",
        "ax2.plot(timesteps, rnn_grads, linewidth=2, label='Simple RNN', alpha=0.8, color='#e74c3c')\n",
        "ax2.plot(timesteps, lstm_grads, linewidth=2, label='LSTM', alpha=0.8, color='#3498db')\n",
        "\n",
        "ax2.set_xlabel('Timestep (earlier ← → later)', fontsize=12)\n",
        "ax2.set_ylabel('Gradient Magnitude (L2 Norm)', fontsize=12)\n",
        "ax2.set_title('Gradient Flow Through Time - Log Scale', fontsize=14, fontweight='bold')\n",
        "ax2.set_yscale('log')\n",
        "ax2.legend(fontsize=11, loc='upper right')\n",
        "ax2.grid(True, alpha=0.3, which='both')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiment_b_gradient_flow_corrected.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ANALYSIS AND STATISTICS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GRADIENT FLOW ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate gradient decay ratios at different points\n",
        "early_timestep = 10\n",
        "mid_timestep = 100\n",
        "late_timestep = 190\n",
        "\n",
        "print(f\"\\n1. GRADIENT MAGNITUDES AT KEY TIMESTEPS:\")\n",
        "print(f\"   {'Timestep':<15} {'SimpleRNN':<20} {'LSTM':<20}\")\n",
        "print(f\"   {'-'*55}\")\n",
        "print(f\"   {f't={early_timestep}':<15} {rnn_grads[early_timestep-1]:<20.8f} {lstm_grads[early_timestep-1]:<20.8f}\")\n",
        "print(f\"   {f't={mid_timestep}':<15} {rnn_grads[mid_timestep-1]:<20.8f} {lstm_grads[mid_timestep-1]:<20.8f}\")\n",
        "print(f\"   {f't={late_timestep}':<15} {rnn_grads[late_timestep-1]:<20.8f} {lstm_grads[late_timestep-1]:<20.8f}\")\n",
        "\n",
        "# Calculate decay ratios\n",
        "rnn_early_to_late = rnn_grads[early_timestep-1] / rnn_grads[late_timestep-1]\n",
        "lstm_early_to_late = lstm_grads[early_timestep-1] / lstm_grads[late_timestep-1]\n",
        "\n",
        "print(f\"\\n2. GRADIENT DECAY (Early timestep / Late timestep):\")\n",
        "print(f\"   SimpleRNN: {rnn_early_to_late:.6f}x  ({'SEVERE VANISHING' if rnn_early_to_late < 0.01 else 'Moderate vanishing'})\")\n",
        "print(f\"   LSTM:      {lstm_early_to_late:.6f}x  ({'Good gradient flow' if lstm_early_to_late > 0.1 else 'Some vanishing'})\")"
      ],
      "metadata": {
        "id": "1GV1wrhefZXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPERIMENT C: Hidden State Dynamics Analysis"
      ],
      "metadata": {
        "id": "9i-f5FBiBZf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT C: Hidden State Dynamics Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# TODO: Select test sequences (5 positive, 5 negative, all at least 150 tokens)\n",
        "test_samples_filtered = ...\n",
        "...\n",
        "test_samples = pd.concat([positive_samples, negative_samples])\n",
        "\n",
        "print(f\"Selected {len(test_samples)} test sequences for analysis\")\n",
        "\n",
        "# Extract hidden states during inference\n",
        "def extract_hidden_states(model, texts, labels, word_to_idx, model_type='rnn'):\n",
        "    \"\"\"Extract hidden states at each timestep\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_hidden_norms = []\n",
        "    all_cell_norms = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, label in zip(texts, labels):\n",
        "            # TODO: Convert text to indices (don't pad, use natural length)\n",
        "            indices = ...\n",
        "            x = torch.LongTensor([indices]).to(device)\n",
        "\n",
        "            # TODO: Get embeddings\n",
        "            if model_type == 'rnn':\n",
        "                embedded = ...\n",
        "                ...\n",
        "                hidden_states = ...\n",
        "                cell_states = ...\n",
        "            else:  # lstm\n",
        "                embedded = ...\n",
        "                ...\n",
        "                hidden_states = ...\n",
        "\n",
        "                # To get all cell states, we need to process step by step\n",
        "                # Reinitialize and process manually\n",
        "                all_cells = []\n",
        "                # TODO: Initialize with correct number of layers\n",
        "                num_layers = ...\n",
        "                h_t = torch.zeros(...).to(device)\n",
        "                c_t = torch.zeros(...).to(device)\n",
        "\n",
        "                for t in range(embedded.size(1)):\n",
        "                    out, (h_t, c_t) = model.lstm(embedded[:, t:t+1, :], (h_t, c_t))\n",
        "                    # TODO: Store only the last layer's cell state\n",
        "                    all_cells.append(...)\n",
        "\n",
        "                cell_states = np.array(all_cells)\n",
        "\n",
        "            # TODO: Compute L2 norms at each timestep\n",
        "            ...\n",
        "            all_hidden_norms.append(hidden_norms)\n",
        "\n",
        "            if cell_states is not None:\n",
        "                cell_norms = np.linalg.norm(cell_states, axis=1)\n",
        "                all_cell_norms.append(cell_norms)\n",
        "\n",
        "            all_labels.append(label)\n",
        "\n",
        "    return all_hidden_norms, all_cell_norms, all_labels\n",
        "\n",
        "# TODO\n",
        "print(\"Extracting hidden states from SimpleRNN...\")\n",
        "...\n",
        "\n",
        "# TODO\n",
        "print(\"Extracting hidden states from LSTM...\")\n",
        "...\n",
        "\n",
        "# Create visualizations\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "\n",
        "# Plot 1: SimpleRNN Hidden States\n",
        "ax1 = axes[0]\n",
        "for i, (norms, label) in enumerate(zip(rnn_hidden_norms, rnn_labels)):\n",
        "    color = 'blue' if label == 1 else 'red'\n",
        "    label_text = 'Positive' if label == 1 else 'Negative'\n",
        "    ax1.plot(norms, color=color, alpha=0.6, linewidth=1.5,\n",
        "            label=label_text if i < 2 else \"\")  # Only label first of each\n",
        "\n",
        "ax1.set_xlabel('Timestep', fontsize=11)\n",
        "ax1.set_ylabel('Hidden State Norm (L2)', fontsize=11)\n",
        "ax1.set_title('SimpleRNN: Hidden State Dynamics', fontsize=13, fontweight='bold')\n",
        "ax1.legend(['Positive', 'Negative'], fontsize=10)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: LSTM Hidden States\n",
        "ax2 = axes[1]\n",
        "for i, (norms, label) in enumerate(zip(lstm_hidden_norms, lstm_labels)):\n",
        "    color = 'blue' if label == 1 else 'red'\n",
        "    label_text = 'Positive' if label == 1 else 'Negative'\n",
        "    ax2.plot(norms, color=color, alpha=0.6, linewidth=1.5,\n",
        "            label=label_text if i < 2 else \"\")\n",
        "\n",
        "ax2.set_xlabel('Timestep', fontsize=11)\n",
        "ax2.set_ylabel('Hidden State Norm (L2)', fontsize=11)\n",
        "ax2.set_title('LSTM: Hidden State Dynamics', fontsize=13, fontweight='bold')\n",
        "ax2.legend(['Positive', 'Negative'], fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: LSTM Cell States\n",
        "ax3 = axes[2]\n",
        "for i, (norms, label) in enumerate(zip(lstm_cell_norms, lstm_labels)):\n",
        "    color = 'blue' if label == 1 else 'red'\n",
        "    label_text = 'Positive' if label == 1 else 'Negative'\n",
        "    ax3.plot(norms, color=color, alpha=0.6, linewidth=1.5,\n",
        "            label=label_text if i < 2 else \"\")\n",
        "\n",
        "ax3.set_xlabel('Timestep', fontsize=11)\n",
        "ax3.set_ylabel('Cell State Norm (L2)', fontsize=11)\n",
        "ax3.set_title('LSTM: Cell State Dynamics', fontsize=13, fontweight='bold')\n",
        "ax3.legend(['Positive', 'Negative'], fontsize=10)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('experiment_c_hidden_state_dynamics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Compute statistics\n",
        "print(\"\\nHidden State Statistics:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "rnn_variances = [np.var(norms) for norms in rnn_hidden_norms]\n",
        "lstm_h_variances = [np.var(norms) for norms in lstm_hidden_norms]\n",
        "lstm_c_variances = [np.var(norms) for norms in lstm_cell_norms]\n",
        "\n",
        "print(f\"SimpleRNN hidden state variance: Mean={np.mean(rnn_variances):.4f}, Std={np.std(rnn_variances):.4f}\")\n",
        "print(f\"LSTM hidden state variance: Mean={np.mean(lstm_h_variances):.4f}, Std={np.std(lstm_h_variances):.4f}\")\n",
        "print(f\"LSTM cell state variance: Mean={np.mean(lstm_c_variances):.4f}, Std={np.std(lstm_c_variances):.4f}\")"
      ],
      "metadata": {
        "id": "7pxlWhkxBbL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArVtk9ScF6oL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}