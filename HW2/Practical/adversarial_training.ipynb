{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rallm/IUST-DL-Fall2025/blob/main/HW2/Practical/adversarial_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28fd50c5-97d1-4019-92b7-bd5c5a8bcf26",
      "metadata": {
        "id": "28fd50c5-97d1-4019-92b7-bd5c5a8bcf26"
      },
      "source": [
        "#Adversarial Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a97621b1-070d-471c-bb80-dadbbba3be3e",
      "metadata": {
        "id": "a97621b1-070d-471c-bb80-dadbbba3be3e"
      },
      "source": [
        "## Part 1 – Training a Convolutional Neural Network on the SVHN Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "144caadc-9c7b-4870-8094-44c2f265cac8",
      "metadata": {
        "id": "144caadc-9c7b-4870-8094-44c2f265cac8"
      },
      "source": [
        "We will train a classifier on the SVHN (Street View House Numbers) dataset, which can be accessed via torchvision.datasets. The model is expected to achieve at least 90% accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c61b36e-9a7e-4273-82aa-6bc28c7682c4",
      "metadata": {
        "id": "0c61b36e-9a7e-4273-82aa-6bc28c7682c4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b3cb28b3-3f63-4efa-899f-36a10e3702c8",
      "metadata": {
        "id": "b3cb28b3-3f63-4efa-899f-36a10e3702c8",
        "outputId": "9c06e7af-ea82-47f8-c144-6a47a15148d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182M/182M [00:17<00:00, 10.5MB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:11<00:00, 5.52MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data\n",
        "\n",
        "# Define Training Data Augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    # Hint: Add a common augmentation technique that flips images horizontally at random.\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    # Hint: Normalize the tensor images. A common practice is to use a mean and standard deviation of 0.5 for each of the three channels.\n",
        "    # This transform expects two tuples: (mean_R, mean_G, mean_B) and (std_R, std_G, std_B).\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Define Test Data Augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load SVHN Dataset\n",
        "trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=transform_train)\n",
        "\n",
        "# Hint: Create a data loader for the training set. Use a batch size of 128 and make sure to shuffle the data.\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7339610-84a4-4514-834e-5af61f868a68",
      "metadata": {
        "id": "a7339610-84a4-4514-834e-5af61f868a68",
        "outputId": "491c1ee9-e44f-4ac2-adee-486e5a71cc38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check for GPU availability and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Data Augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    # Hint: Set the maximum degrees for the random rotation. A value like 15 is a good start.\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Updated Model Architecture\n",
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
        "        x = x.view(-1, 512 * 2 * 2)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Updated Training Function with Learning Rate Scheduler\n",
        "def train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler, num_epochs=20):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Hint: Implement the standard training steps in order:\n",
        "            # zero gradients, forward pass, calculate loss, backward pass, and update weights.\n",
        "            # Then, add the batch loss to the 'running_loss'.\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Hint: After the optimizer step, update the scheduler as well.\n",
        "            scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(trainloader):.4f}\")"
      ],
      "metadata": {
        "id": "nyLL9wacb3xW"
      },
      "id": "nyLL9wacb3xW",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Execution\n",
        "model = ImprovedCNN().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "# Hint: Use a scheduler that multiplies the learning rate by a 'gamma' factor every 'step_size' epochs.\n",
        "# For example, reduce the LR by a factor of 0.1 every 10 epochs.\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "train_model_with_scheduler(model, trainloader, criterion, optimizer, scheduler, num_epochs=20)"
      ],
      "metadata": {
        "id": "Q9DZ7mlubwlh",
        "outputId": "4799b5be-d260-45df-b9b1-3c29607a81c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q9DZ7mlubwlh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.2524\n",
            "Epoch 2/20, Loss: 2.2528\n",
            "Epoch 3/20, Loss: 2.2528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c90f7fa-d094-4877-8296-25eeb960359e",
      "metadata": {
        "id": "4c90f7fa-d094-4877-8296-25eeb960359e"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testloader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    incorrect_samples = []\n",
        "    incorrect_true_labels = []\n",
        "    incorrect_pred_labels = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Hint: Use a torch context manager to disable gradient calculations for efficiency.\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Hint: From the raw 'outputs', find the index of the maximum value along dimension 1 to get the predicted labels.\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Calculate total and correct predictions\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Hint: Count how many predictions match the true labels in this batch and add the number to the 'correct' variable.\n",
        "            correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Collect incorrect predictions\n",
        "            # Hint: Create a boolean mask to identify which predictions were incorrect.\n",
        "            mismatches = (preds != labels)\n",
        "\n",
        "            # Hint: Use the 'mismatches' mask to select and store the input images that were misclassified.\n",
        "            incorrect_samples.extend(inputs[mismatches])\n",
        "            incorrect_true_labels.extend(labels[mismatches].cpu().numpy())\n",
        "            incorrect_pred_labels.extend(preds[mismatches].cpu().numpy())\n",
        "\n",
        "    # Calculate and print accuracy\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on the test set: {accuracy:.2f}%\")\n",
        "\n",
        "    return all_preds, all_labels, (incorrect_samples, incorrect_true_labels, incorrect_pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(labels, preds):\n",
        "    \"\"\"\n",
        "    Generates a confusion matrix using the true labels and the model's predictions.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))  # Assuming 10 classes\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pXRdWIyXcngH"
      },
      "id": "pXRdWIyXcngH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_incorrect_samples(incorrect_samples, true_labels, pred_labels, num_samples=10):\n",
        "    \"\"\"\n",
        "    Visualizes a subset of incorrectly classified samples.\n",
        "\n",
        "    Args:\n",
        "        incorrect_samples (list): List of incorrectly classified image tensors.\n",
        "        true_labels (list): List of true labels corresponding to the incorrect samples.\n",
        "        pred_labels (list): List of predicted labels corresponding to the incorrect samples.\n",
        "        num_samples (int): Number of samples to visualize.\n",
        "    \"\"\"\n",
        "    num_samples = min(len(incorrect_samples), num_samples)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Hint: Matplotlib expects image dimensions as (H, W, C), but PyTorch tensors are (C, H, W).\n",
        "        # You need to permute the dimensions before converting the tensor to a numpy array for plotting.\n",
        "        img = incorrect_samples[i].permute(#TODO).cpu().numpy()\n",
        "\n",
        "        # Hint: The original data was normalized. Reverse the normalization (mean=0.5, std=0.5) to display the correct colors.\n",
        "        img = #TODO\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(img)\n",
        "\n",
        "        # Hint: Set the title of the plot to show the actual label and the model's incorrect prediction.\n",
        "        plt.title(#TODO)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FJUriWp5ctDn"
      },
      "id": "FJUriWp5ctDn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea85484-defd-4b17-8009-18063847eb5b",
      "metadata": {
        "id": "5ea85484-defd-4b17-8009-18063847eb5b"
      },
      "outputs": [],
      "source": [
        "preds, labels, (incorrect_samples, incorrect_true_labels, incorrect_pred_labels) = evaluate_model(model, testloader)\n",
        "plot_confusion_matrix(labels, preds)\n",
        "visualize_incorrect_samples(incorrect_samples, true_labels=incorrect_true_labels, pred_labels=incorrect_pred_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3514b299-362c-4bf5-b60d-9c75b95fbfc9",
      "metadata": {
        "id": "3514b299-362c-4bf5-b60d-9c75b95fbfc9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(labels, preds):\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52786544-c714-4df2-9fdb-1a3b6b161875",
      "metadata": {
        "id": "52786544-c714-4df2-9fdb-1a3b6b161875"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_incorrect_predictions(model, testloader, device):\n",
        "    model.eval()\n",
        "    incorrect_images = []\n",
        "    incorrect_true_labels = []\n",
        "    incorrect_pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            # Collect incorrect predictions\n",
        "            # Hint: Create a boolean mask to find where the predictions do not match the true labels.\n",
        "            mismatches = #TODO\n",
        "\n",
        "            # Hint: Use the 'mismatches' mask to select the inputs that were incorrectly classified.\n",
        "            incorrect_images.extend(#TODO)\n",
        "            incorrect_true_labels.extend(labels[mismatches].cpu().numpy())\n",
        "            incorrect_pred_labels.extend(preds[mismatches].cpu().numpy())\n",
        "\n",
        "    # Display a subset of incorrect predictions\n",
        "    num_to_display = 10  # Adjust based on preference\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(min(len(incorrect_images), num_to_display)):\n",
        "\n",
        "        # Hint: Rearrange the tensor dimensions from (C, H, W) to (H, W, C) for matplotlib.\n",
        "        img = incorrect_images[i].permute(#TODO).cpu().numpy()\n",
        "\n",
        "        # Hint: Reverse the normalization (mean=0.5, std=0.5) to restore the original image colors.\n",
        "        img = #TODO\n",
        "\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"True: {incorrect_true_labels[i]}, Pred: {incorrect_pred_labels[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "display_incorrect_predictions(model, testloader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4105d897-e174-4d05-978d-de282ac566c6",
      "metadata": {
        "id": "4105d897-e174-4d05-978d-de282ac566c6"
      },
      "source": [
        "## Part 2 – Performing Adversarial Attacks on Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682c7f10-2033-49a9-8aef-3fa273a7c9f6",
      "metadata": {
        "id": "682c7f10-2033-49a9-8aef-3fa273a7c9f6"
      },
      "source": [
        "We will implement the Fast Gradient Sign Method (FGSM) and create a function `eval_adversarial(model, test_loader, epsilon)` that applies FGSM to the test dataset. The function produces perturbed versions of the original images and computes the model's accuracy only on those perturbed samples. After performing the attack with $\\varepsilon=0.1$, the test accuracy is expected to drop below 25%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae8df69-003e-4d69-8d18-c901b02d8859",
      "metadata": {
        "id": "aae8df69-003e-4d69-8d18-c901b02d8859"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c6578a-5df3-40eb-9752-080565c7e922",
      "metadata": {
        "id": "52c6578a-5df3-40eb-9752-080565c7e922"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define FGSM Attack\n",
        "def fgsm_attack(model, data, epsilon, data_grad):\n",
        "    sign_data_grad = #TODO\n",
        "    perturbed_data = #TODO\n",
        "    perturbed_data = #TODO\n",
        "    return perturbed_data\n",
        "\n",
        "def unnormalize(img):\n",
        "    \"\"\"\n",
        "    Reverse the normalization applied to an image tensor.\n",
        "\n",
        "    Args:\n",
        "        img (torch.Tensor): Normalized image tensor (C, H, W).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Unnormalized image tensor (C, H, W) with values in [0, 1].\n",
        "    \"\"\"\n",
        "    mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
        "\n",
        "    # Unnormalize the image\n",
        "    img = #TODO\n",
        "    return img.clamp(0, 1)  # Ensure values are within [0, 1]\n",
        "\n",
        "def eval_adversarial(model, test_loader, epsilon, device='cuda'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    wrong_after_attack = []\n",
        "    right_before_attack = []\n",
        "    original_images = []\n",
        "    perturbed_images = []\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Enable gradient calculation on the image tensor itself.\n",
        "        #TODO\n",
        "\n",
        "        # Perform a forward pass on the original, clean images.\n",
        "        outputs = #TODO\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Check which predictions were correct before the attack.\n",
        "        correct_before = #TODO\n",
        "\n",
        "        # Calculate the loss to be used for generating the gradient.\n",
        "        loss = #TODO\n",
        "\n",
        "        # Zero out all existing gradients.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform backpropagation to calculate gradients of the loss with respect to the images.\n",
        "        #TODO\n",
        "\n",
        "        # Extract the gradient data from the image tensor.\n",
        "        data_grad = #TODO\n",
        "\n",
        "        # Generate the perturbed image using the provided attack function.\n",
        "        perturbed_data = #TODO\n",
        "\n",
        "        # Get the model's predictions on the new, perturbed images.\n",
        "        outputs = #TODO\n",
        "        _, perturbed_predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total count of correct predictions on the adversarial examples.\n",
        "        correct += #TODO\n",
        "\n",
        "        # Identify samples that were correct before the attack but are now incorrect.\n",
        "        right_before = #TODO\n",
        "\n",
        "        if len(original_images) < 10:  # Limit the number of stored samples\n",
        "            # Using the 'right_before' mask, store the original images that were successfully attacked.\n",
        "            original_images.extend(#TODO)\n",
        "            # Store the corresponding perturbed images.\n",
        "            perturbed_images.extend(#TODO)\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy, all_predictions, all_labels, original_images, perturbed_images\n",
        "\n",
        "# Plot confusion matrix\n",
        "def plot_confusion_matrix(predictions, labels):\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix After FGSM Attack')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_perturbed_images(original_images, perturbed_images, epsilon):\n",
        "    \"\"\"\n",
        "    Visualize original and perturbed images that were right before but wrong after the attack.\n",
        "\n",
        "    Args:\n",
        "        original_images (list): List of original image tensors.\n",
        "        perturbed_images (list): List of perturbed image tensors.\n",
        "        epsilon (float): The FGSM attack epsilon value.\n",
        "    \"\"\"\n",
        "    n = min(5, len(original_images))\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Unnormalize original and perturbed images\n",
        "        original = unnormalize(original_images[i])\n",
        "        perturbed = unnormalize(perturbed_images[i])\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(original.permute(1, 2, 0).cpu().numpy())\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Perturbed image\n",
        "        plt.subplot(2, n, i + n + 1)\n",
        "        plt.imshow(perturbed.permute(1, 2, 0).cpu().numpy())\n",
        "        plt.title(f\"Perturbed (ε={epsilon})\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_multiple_epsilons(model, test_loader, epsilons, device='cuda'):\n",
        "    accuracies = []\n",
        "\n",
        "    for eps in epsilons:\n",
        "        accuracy, predictions, labels, original_images, perturbed_images = eval_adversarial(\n",
        "            model, test_loader, eps, device\n",
        "        )\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epsilon: {eps}, Accuracy: {accuracy:.2f}%\")\n",
        "        plot_confusion_matrix(predictions, labels)\n",
        "        visualize_perturbed_images(original_images, perturbed_images, eps)\n",
        "\n",
        "    # Plot accuracy vs. epsilon\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(epsilons, accuracies, marker='o', label=\"Accuracy\")\n",
        "    plt.title(\"Model Accuracy vs. Epsilon\")\n",
        "    plt.xlabel(\"Epsilon\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return accuracies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1490760-021d-490d-9b67-3054c762a313",
      "metadata": {
        "id": "a1490760-021d-490d-9b67-3054c762a313"
      },
      "outputs": [],
      "source": [
        "epsilons = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "accuracies = evaluate_multiple_epsilons(model, testloader, epsilons, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d2af37-44ed-4840-9ca4-6cf695457c4c",
      "metadata": {
        "id": "f3d2af37-44ed-4840-9ca4-6cf695457c4c"
      },
      "source": [
        "## Part 3 – Training our model using adversarial training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8964b69a-b4ae-4eea-ab9b-d59dd03288f1",
      "metadata": {
        "id": "8964b69a-b4ae-4eea-ab9b-d59dd03288f1"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8befc5b7-e7c0-43cf-9569-a6ff462a9a2f",
      "metadata": {
        "id": "8befc5b7-e7c0-43cf-9569-a6ff462a9a2f"
      },
      "source": [
        "We will visualize the confusion matrix alongside several examples of images that the model misclassified. Afterwards, we will discuss how the model’s performance has changed compared to its previous state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d858bb4-9afc-4867-8171-6f5296a935e6",
      "metadata": {
        "id": "7d858bb4-9afc-4867-8171-6f5296a935e6"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_adversarial_examples(model, inputs, labels, epsilon, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generate adversarial examples for training using FGSM.\n",
        "    \"\"\"\n",
        "    # Allow gradients to be computed with respect to the input images.\n",
        "    #TODO\n",
        "\n",
        "    # Get the model's initial predictions on the clean inputs.\n",
        "    outputs = #TODO\n",
        "\n",
        "    # Calculate the loss which will be used to generate the adversarial perturbation.\n",
        "    loss = #TODO\n",
        "\n",
        "    # Clear any previous gradients.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Compute the gradients of the loss with respect to the input images.\n",
        "    #TODO\n",
        "\n",
        "    # Extract the gradient data from the input tensor.\n",
        "    data_grad = #TODO\n",
        "\n",
        "    # Use the sign of the gradient to create the perturbed image (FGSM attack).\n",
        "    perturbed_data = #TODO\n",
        "\n",
        "    return perturbed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d47b23-0282-4e1f-8caf-710fb1511928",
      "metadata": {
        "id": "c5d47b23-0282-4e1f-8caf-710fb1511928"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def train_with_adversarial_training(model, train_loader, optimizer, criterion, epsilon, device='cuda', num_epochs=10):\n",
        "    \"\"\"\n",
        "    Train the model using adversarial training.\n",
        "\n",
        "    Args:\n",
        "        model: The neural network model.\n",
        "        train_loader: DataLoader for the training dataset.\n",
        "        optimizer: Optimizer for training.\n",
        "        criterion: Loss function.\n",
        "        epsilon: Strength of FGSM attack for adversarial training.\n",
        "        device: Device to train on (default: 'cuda').\n",
        "        num_epochs: Number of training epochs.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Generate adversarial examples for the current batch.\n",
        "            adv_images = #TODO\n",
        "\n",
        "            # Combine the original and adversarial images into a single batch tensor.\n",
        "            combined_images = #TODO\n",
        "\n",
        "            # Create the corresponding label tensor for the combined batch.\n",
        "            combined_labels = #TODO\n",
        "\n",
        "            # Perform the forward pass on the combined images.\n",
        "            outputs = #TODO\n",
        "\n",
        "            # Calculate the loss using the combined images and labels.\n",
        "            loss = #TODO\n",
        "\n",
        "            # Zero the gradients.\n",
        "            #TODO\n",
        "\n",
        "            # Perform the backward pass.\n",
        "            #TODO\n",
        "\n",
        "            # Update the model's weights.\n",
        "            #TODO\n",
        "\n",
        "            # Update statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += combined_labels.size(0)\n",
        "\n",
        "            # Update the total number of correct predictions for the epoch.\n",
        "            correct += #TODO\n",
        "\n",
        "        # Print epoch statistics\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd2729f-7fbc-4e79-ac7d-abaf26312039",
      "metadata": {
        "id": "efd2729f-7fbc-4e79-ac7d-abaf26312039"
      },
      "outputs": [],
      "source": [
        "def evaluate_robustness(model, test_loader, epsilon, device='cuda'):\n",
        "    \"\"\"\n",
        "    Assesses how well the model performs when tested on adversarial examples.\n",
        "    \"\"\"\n",
        "    accuracy, predictions, labels, _, _ = eval_adversarial(model, test_loader, epsilon, device)\n",
        "    print(f\"Model robustness for epsilon={epsilon}: Accuracy = {accuracy:.2f}%\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298b4a95-7fcd-4a92-8928-901416bd704e",
      "metadata": {
        "id": "298b4a95-7fcd-4a92-8928-901416bd704e"
      },
      "outputs": [],
      "source": [
        "model = ImprovedCNN().to('cuda')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "criterion = #TODO\n",
        "\n",
        "epsilon = 0.1\n",
        "train_with_adversarial_training(model, trainloader, optimizer, criterion, epsilon, device='cuda', num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48545f3c-44a7-4b89-a928-7afff988b078",
      "metadata": {
        "id": "48545f3c-44a7-4b89-a928-7afff988b078"
      },
      "outputs": [],
      "source": [
        "test_epsilons = [ 0.1]\n",
        "for eps in test_epsilons:\n",
        "    evaluate_robustness(model, testloader, eps, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1ada354-558c-4321-aa7e-b3f12fc3774a",
      "metadata": {
        "id": "b1ada354-558c-4321-aa7e-b3f12fc3774a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix for the given model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Get the predicted labels from the model's outputs.\n",
        "            _, predicted = #TODO\n",
        "\n",
        "            # Store the ground truth labels for the batch.\n",
        "            true_labels.extend(#TODO)\n",
        "\n",
        "            # Store the predicted labels for the batch.\n",
        "            pred_labels.extend(#TODO)\n",
        "\n",
        "    # Compute the confusion matrix using the collected true and predicted labels.\n",
        "    cm = #TODO\n",
        "\n",
        "    # Create a display object for the confusion matrix.\n",
        "    disp = #TODO\n",
        "\n",
        "    # Plot the confusion matrix.\n",
        "    disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
        "    plt.title(\"Confusion Matrix (After Adversarial Training)\")\n",
        "    plt.show()\n",
        "\n",
        "def plot_misclassified_images(model, dataloader, device='cuda', num_images=5):\n",
        "    \"\"\"\n",
        "    Plot examples of misclassified images with true and predicted labels.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    misclassified = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Identify misclassified images.\n",
        "            for img, true_label, pred_label in zip(images, labels, predicted):\n",
        "                # Check if the prediction is incorrect and if we have not yet collected enough images.\n",
        "                if #TODO:\n",
        "                    # If incorrect, append the image, true label, and predicted label to the list.\n",
        "                    misclassified.append(#TODO)\n",
        "\n",
        "    # Plot misclassified images.\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, (img, true_label, pred_label) in enumerate(misclassified):\n",
        "        img = img.numpy()\n",
        "\n",
        "        # Normalize the image to the [0, 1] range for proper display.\n",
        "        img = #TODO\n",
        "\n",
        "        plt.subplot(1, num_images, i + 1)\n",
        "\n",
        "        # Display the image, transposing its dimensions from (C, H, W) to (H, W, C).\n",
        "        plt.imshow(#TODO)\n",
        "\n",
        "        plt.title(f\"True: {true_label}, Pred: {pred_label}\")\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(\"Misclassified Images (After Adversarial Training)\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plot_confusion_matrix(model, testloader, device='cuda')\n",
        "\n",
        "# Visualize Misclassified Images\n",
        "plot_misclassified_images(model, testloader, device='cuda', num_images=5)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}