{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rallm/IUST-DL-Fall2025/blob/main/HW1/HWP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08dd43a2",
      "metadata": {
        "id": "08dd43a2"
      },
      "source": [
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "# ForwardOnly_MLP\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://mad-institute.com/images/universitys/iranuniversity2.png\" alt=\"IUST Logo\" width=\"180\" height=\"180\">\n",
        "</p>\n",
        "\n",
        "**University:** IRAN University of Science and Technology  \n",
        "**Course:** Deep Learning  \n",
        "**Term:** First semester of academic year 1404–1405\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986401ae",
      "metadata": {
        "id": "986401ae"
      },
      "source": [
        "\n",
        "## Notebook: Forward-Only MLP with Fixed Weights — *Evaluation-Only*\n",
        "\n",
        "We implement a small **two-layer MLP** and run it **forward-only** (no learning). You will:\n",
        "- Choose **activations** (ReLU / Tanh / Sigmoid / GELU) and **fixed weights/biases**.\n",
        "- Generate synthetic **regression** *or* **classification** data.\n",
        "- Compute **MSE/MAE** (regression) or **Cross-Entropy** (classification).\n",
        "- Visualize **pre-activations**, **activations**, and **loss contributions** to study **saturation** vs **linear** regimes.\n",
        "\n",
        "> **Forbidden in this unit:** model selection, regularization, dropout, label smoothing, adversarial training, backpropagation, and gradient-based optimizers. Everything here is **forward-only** and **evaluation-only**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f177cf34",
      "metadata": {
        "id": "f177cf34"
      },
      "source": [
        "\n",
        "### Learning Objectives\n",
        "1. Implement a configurable **forward** pass for a 2-layer MLP with **fixed** parameters.\n",
        "2. Compare activations by analyzing **output distributions** and **loss metrics**.\n",
        "3. Visualize and interpret **saturation** and its effect on predictions.\n",
        "4. Design small experiments (weight scale, input scale, activation choice) to produce insightful **plots** and **tables**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e1b0fd16",
      "metadata": {
        "id": "e1b0fd16",
        "outputId": "4b02fb3b-6177-4b66-888e-ccf87988eb66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2 | pandas: 2.2.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "print(\"NumPy:\", np.__version__, \"| pandas:\", pd.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64887c65",
      "metadata": {
        "id": "64887c65"
      },
      "source": [
        "\n",
        "## Part A — Mode & Data (Synthetic)\n",
        "\n",
        "Pick **one** task mode:\n",
        "- **Regression**: predict a continuous target; evaluate with **MSE/MAE**.\n",
        "- **Classification**: predict a class (binary or multiclass); evaluate with **Cross-Entropy**.\n",
        "\n",
        "**TODO ideas:**\n",
        "- Vary input noise, class overlap, or nonlinearity.\n",
        "- Keep weights **fixed** across experiments; change only data conditions (evaluation-only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cf69aa73",
      "metadata": {
        "id": "cf69aa73",
        "outputId": "aea68c7a-0a55-46d8-da64-80bd155ee1d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2030412801.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# S = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# y = ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ],
      "source": [
        "# ------------------------------ TODO: Select task mode --------------------------------\n",
        "MODE = \"classification\"  # TODO: choose \"regression\" or \"classification\" based on the experiment you want\n",
        "\n",
        "# ------------------------------ Data parameters ---------------------------------------\n",
        "n = 400                # number of samples (keep as-is; not educational to modify here)\n",
        "d = 4                  # input dimension\n",
        "C = 3                  # number of classes (used if MODE == \"classification\")\n",
        "\n",
        "# Regression ground-truth\n",
        "def f_reg(x):\n",
        "    # TODO: Implement a smooth nonlinear target mapping for regression.\n",
        "    # Suggestion:\n",
        "    # 1) Pick a fixed weight vector w of length d (e.g., manually set a few numbers).\n",
        "    # 2) Compute a scalar projection z = x @ w.\n",
        "    # 3) Return a combination like: a * tanh(z) + b * z + c (use small constants).\n",
        "    pass\n",
        "\n",
        "# Classification score generator (for synthetic labels)\n",
        "def f_cls_scores(x):\n",
        "    # TODO: Produce class scores S for C classes from inputs x with fixed, known parameters.\n",
        "    # Suggested steps:\n",
        "    # 1) Define a fixed weight matrix W_true of shape (C, d) with simple patterns (e.g., linspace rows).\n",
        "    # 2) Define a fixed bias vector b_true of length C (e.g., linspace).\n",
        "    # 3) Compute and return S = x @ W_true.T + b_true.\n",
        "    pass\n",
        "\n",
        "# ------------------------------ Generate inputs ---------------------------------------\n",
        "X = np.random.randn(n, d)  # standard normal inputs (keep; not pedagogically central)\n",
        "\n",
        "if MODE == \"regression\":\n",
        "    # TODO: Generate regression targets y using f_reg(X) and add small Gaussian noise.\n",
        "    # Example guidance: y = f_reg(X) + noise_level * np.random.randn(n)\n",
        "    # Then construct a DataFrame with columns x0..x{d-1}, y\n",
        "    # (You can keep the DataFrame creation as-is below once y exists.)\n",
        "    # y = ...\n",
        "    df = pd.DataFrame(np.c_[X, y], columns=[*(f\"x{i}\" for i in range(d)), \"y\"])\n",
        "\n",
        "elif MODE == \"classification\":\n",
        "    # TODO: Get raw class scores S = f_cls_scores(X) and optionally add moderate Gaussian noise to S.\n",
        "    # TODO: Convert scores to hard labels with argmax over classes.\n",
        "    # Then construct a DataFrame with columns x0..x{d-1}, y\n",
        "    # (You can keep the DataFrame creation as-is below once y exists.)\n",
        "    # S = ...\n",
        "    # y = ...\n",
        "    df = pd.DataFrame(np.c_[X, y], columns=[*(f\"x{i}\" for i in range(d)), \"y\"])\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"MODE must be 'regression' or 'classification'\")\n",
        "\n",
        "df.head()  # keep: quick sanity-check of the generated dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2cedd37",
      "metadata": {
        "id": "f2cedd37"
      },
      "outputs": [],
      "source": [
        "# ======================= Unit Tests (print-only PASS/FAIL) =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# 1) MODE should be valid\n",
        "tests.append(lambda: 'MODE' in globals() and MODE in (\"regression\", \"classification\"))\n",
        "\n",
        "# 2) X should have correct shape\n",
        "tests.append(lambda: 'X' in globals() and isinstance(X, np.ndarray) and X.shape == (n, d))\n",
        "\n",
        "# 3) df should have correct type and shape\n",
        "tests.append(lambda: 'df' in globals() and isinstance(df, pd.DataFrame) and df.shape[0] == n and df.shape[1] == d + 1)\n",
        "\n",
        "# 4) df columns should be x0..x{d-1}, y\n",
        "tests.append(lambda: list(df.columns) == [*(f\"x{i}\" for i in range(d)), \"y\"])\n",
        "\n",
        "# 5) y should exist and have length n\n",
        "tests.append(lambda: 'y' in globals() and (isinstance(y, (np.ndarray, pd.Series, list))) and len(y) == n)\n",
        "\n",
        "# 6) y should not be constant (positive variance)\n",
        "tests.append(lambda: np.var(np.asarray(y, dtype=float)) > 0)\n",
        "\n",
        "# 7) If regression mode, y should be float\n",
        "tests.append(lambda: (MODE == \"regression\" and np.issubdtype(np.asarray(y).dtype, np.floating)) or (MODE != \"regression\"))\n",
        "\n",
        "# 8) If classification mode, labels should be in [0, C-1]\n",
        "tests.append(lambda: (MODE == \"classification\" and\n",
        "                      np.all(np.asarray(y, dtype=int) >= 0) and\n",
        "                      np.all(np.asarray(y, dtype=int) < C)) or (MODE != \"classification\"))\n",
        "\n",
        "# 9) If classification mode, at least one and at most C unique classes\n",
        "tests.append(lambda: (MODE == \"classification\" and\n",
        "                      1 <= np.unique(np.asarray(y, dtype=int)).size <= C) or (MODE != \"classification\"))\n",
        "\n",
        "# 10) df['y'] should match y\n",
        "tests.append(lambda: np.all(np.asarray(df['y']) == np.asarray(y)))\n",
        "\n",
        "# 11) Generator functions should exist\n",
        "tests.append(lambda: 'f_reg' in globals() and callable(f_reg) and 'f_cls_scores' in globals() and callable(f_cls_scores))\n",
        "\n",
        "# 12) If classification, S should exist and have shape (n, C)\n",
        "tests.append(lambda: (MODE == \"classification\" and 'S' in globals() and isinstance(S, np.ndarray) and S.shape == (n, C)) or (MODE != \"classification\"))\n",
        "\n",
        "# Run tests (print only PASS/FAIL)\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "for r in results:\n",
        "    print(r)\n",
        "# ================================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1bfccd",
      "metadata": {
        "id": "ba1bfccd"
      },
      "source": [
        "\n",
        "## Part B — Split (Train/Dev/Test)\n",
        "\n",
        "We split randomly into train/dev/test for **evaluation-only** reporting (no fitting).  \n",
        "Weights are **fixed** and do not change across splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ade9aea",
      "metadata": {
        "id": "7ade9aea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Decide split sizes:\n",
        "#       - n_train = int(0.6 * n)\n",
        "#       - n_dev   = int(0.2 * n)\n",
        "#       - n_test  = n - n_train - n_dev  (implicitly via slicing)\n",
        "\n",
        "# TODO: Generate a random permutation of indices 0..n-1:\n",
        "#       - perm = np.random.permutation(n)\n",
        "\n",
        "# TODO: Create index splits from the permutation:\n",
        "#       - idx_train = perm[:n_train]\n",
        "#       - idx_dev   = perm[n_train : n_train + n_dev]\n",
        "#       - idx_test  = perm[n_train + n_dev :]\n",
        "\n",
        "# TODO: Split features by indexing:\n",
        "#       - X_train = X[idx_train]\n",
        "#       - X_dev   = X[idx_dev]\n",
        "#       - X_test  = X[idx_test]\n",
        "\n",
        "# TODO: Split targets by indexing:\n",
        "#       - y_train = y[idx_train]\n",
        "#       - y_dev   = y[idx_dev]\n",
        "#       - y_test  = y[idx_test]\n",
        "\n",
        "\n",
        "print(\"Splits:\", len(y_train), len(y_dev), len(y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad69e8b3",
      "metadata": {
        "id": "ad69e8b3"
      },
      "outputs": [],
      "source": [
        "# ======================= Unit Tests for Splits (print-only PASS/FAIL) =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# 1) Required globals exist\n",
        "tests.append(lambda: all(k in globals() for k in [\n",
        "    'n','d','X','y','perm','n_train','n_dev',\n",
        "    'idx_train','idx_dev','idx_test',\n",
        "    'X_train','X_dev','X_test','y_train','y_dev','y_test'\n",
        "]))\n",
        "\n",
        "# 2) perm is a permutation of 0..n-1\n",
        "tests.append(lambda: isinstance(perm, np.ndarray)\n",
        "             and perm.shape == (n,)\n",
        "             and np.array_equal(np.sort(perm), np.arange(n)))\n",
        "\n",
        "# 3) Split sizes align with indices\n",
        "tests.append(lambda: len(idx_train) == n_train and len(idx_dev) == n_dev\n",
        "             and len(idx_test) == (n - n_train - n_dev))\n",
        "\n",
        "# 4) Index ranges are valid (within [0, n))\n",
        "tests.append(lambda: all(0 <= int(i) < n for i in np.concatenate([idx_train, idx_dev, idx_test])))\n",
        "\n",
        "# 5) Splits are disjoint (no overlap)\n",
        "tests.append(lambda: len(set(idx_train)) + len(set(idx_dev)) + len(set(idx_test))\n",
        "             == len(set(np.concatenate([idx_train, idx_dev, idx_test]))))\n",
        "\n",
        "# 6) Splits cover exactly all indices in perm (order not required)\n",
        "tests.append(lambda: set(np.concatenate([idx_train, idx_dev, idx_test])) == set(perm))\n",
        "\n",
        "# 7) X_* have correct shapes\n",
        "tests.append(lambda: X_train.shape == (len(idx_train), d)\n",
        "             and X_dev.shape == (len(idx_dev), d)\n",
        "             and X_test.shape == (len(idx_test), d))\n",
        "\n",
        "# 8) y_* have correct lengths\n",
        "tests.append(lambda: len(y_train) == len(idx_train)\n",
        "             and len(y_dev) == len(idx_dev)\n",
        "             and len(y_test) == len(idx_test))\n",
        "\n",
        "# 9) X_* correspond to X indexed by the same idx_*\n",
        "tests.append(lambda: np.array_equal(X_train, X[idx_train])\n",
        "             and np.array_equal(X_dev, X[idx_dev])\n",
        "             and np.array_equal(X_test, X[idx_test]))\n",
        "\n",
        "# 10) y_* correspond to y indexed by the same idx_*\n",
        "tests.append(lambda: np.array_equal(np.asarray(y_train), np.asarray(y)[idx_train])\n",
        "             and np.array_equal(np.asarray(y_dev),   np.asarray(y)[idx_dev])\n",
        "             and np.array_equal(np.asarray(y_test),  np.asarray(y)[idx_test]))\n",
        "\n",
        "# 11) Label/value distributions are non-degenerate in each split (some variance)\n",
        "tests.append(lambda: np.var(np.asarray(y_train, dtype=float)) > 0\n",
        "             and np.var(np.asarray(y_dev, dtype=float)) > 0\n",
        "             and np.var(np.asarray(y_test, dtype=float)) > 0)\n",
        "\n",
        "# 12) Counts add up and match the print\n",
        "tests.append(lambda: (len(y_train) + len(y_dev) + len(y_test) == n)\n",
        "             and len(y_train) == n_train and len(y_dev) == n_dev\n",
        "             and len(y_test) == (n - n_train - n_dev))\n",
        "\n",
        "# 13) Indices are contiguous slices of perm (expected by construction)\n",
        "tests.append(lambda: np.array_equal(idx_train, perm[:n_train])\n",
        "             and np.array_equal(idx_dev,   perm[n_train:n_train+n_dev])\n",
        "             and np.array_equal(idx_test,  perm[n_train+n_dev:]))\n",
        "\n",
        "# Run tests\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "# Print only PASS/FAIL lines\n",
        "for r in results:\n",
        "    print(r)\n",
        "# ===========================================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b4a5b5d",
      "metadata": {
        "id": "2b4a5b5d"
      },
      "source": [
        "\n",
        "## Part C — MLP (Forward Only)\n",
        "\n",
        "Architecture: **d → m → out** with a chosen activation on the hidden layer. Output layer:\n",
        "- Regression: linear output (no activation).\n",
        "- Classification: raw logits → **softmax** only for **CE computation**.\n",
        "\n",
        "**TODO ideas:**\n",
        "- Change `m` (hidden size) and **weight scales** to expose saturation.\n",
        "- Try multiple activations and compare losses.\n",
        "- Optionally **load weights** from `.npy` (see hooks below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da347b66",
      "metadata": {
        "id": "da347b66"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ TODO: MLP configuration --------------------------------\n",
        "m = 32                         # low-value: keep as-is\n",
        "w_scale_1 = 0.8                # low-value: keep as-is\n",
        "w_scale_2 = 0.8                # low-value: keep as-is\n",
        "b_scale_1 = 0.0                # low-value: keep as-is\n",
        "b_scale_2 = 0.0                # low-value: keep as-is\n",
        "ACT = \"relu\"                   # TODO: choose from {\"relu\",\"tanh\",\"sigmoid\",\"gelu\"}\n",
        "\n",
        "# ------------------------------ Activations --------------------------------------------\n",
        "# TODO: Implement ReLU:\n",
        "#       - Input: z (array-like)\n",
        "#       - Output: elementwise max(z, 0)\n",
        "# def relu(z): ...\n",
        "\n",
        "# TODO: Implement tanh:\n",
        "#       - Use np.tanh(z)\n",
        "# def tanh(z): ...\n",
        "\n",
        "# TODO: Implement sigmoid:\n",
        "#       - Use 1 / (1 + exp(-z)) in a numerically stable way if needed\n",
        "# def sigmoid(z): ...\n",
        "\n",
        "# TODO: Implement GELU (approximate, Hendrycks & Gimpel):\n",
        "#       - 0.5 * z * (1 + tanh( sqrt(2/pi) * (z + 0.044715 * z^3 ) ))\n",
        "# def gelu(z): ...\n",
        "\n",
        "# TODO: Activation dispatch:\n",
        "#       - If name == \"relu\": call relu\n",
        "#       - If name == \"tanh\": call tanh\n",
        "#       - If name == \"sigmoid\": call sigmoid\n",
        "#       - If name == \"gelu\": call gelu\n",
        "#       - Else: raise ValueError(\"Unknown activation\")\n",
        "# def act_forward(z, name): ...\n",
        "\n",
        "# TODO: Stable softmax over rows:\n",
        "#       - Subtract row-wise max for stability\n",
        "#       - exp and normalize by row-wise sum\n",
        "# def softmax(Z): ...\n",
        "\n",
        "# ------------------------------ Fixed parameters (random, then *frozen*) ----------------\n",
        "rng = np.random.default_rng(123)   # low-value: keep as-is\n",
        "\n",
        "# TODO: Initialize first layer:\n",
        "#       - W1 shape: (d, m) ~ N(0,1) scaled by w_scale_1 using rng.standard_normal\n",
        "#       - b1 shape: (m,)   ~ N(0,1) scaled by b_scale_1\n",
        "# W1 = ...\n",
        "# b1 = ...\n",
        "\n",
        "# TODO: Initialize second layer depending on MODE:\n",
        "#       - If MODE == \"regression\":\n",
        "#           * W2 shape: (m, 1) ~ N(0,1) scaled by w_scale_2\n",
        "#           * b2 shape: (1,)   ~ N(0,1) scaled by b_scale_2\n",
        "#           * out_dim = 1\n",
        "#       - Else (classification):\n",
        "#           * W2 shape: (m, C) ~ N(0,1) scaled by w_scale_2\n",
        "#           * b2 shape: (C,)   ~ N(0,1) scaled by b_scale_2\n",
        "#           * out_dim = C\n",
        "# W2 = ...\n",
        "# b2 = ...\n",
        "# out_dim = ...\n",
        "\n",
        "# ------------------------------ Forward function ----------------------------------------\n",
        "# TODO: Implement mlp_forward(X_in, act=\"relu\", return_intermediates=False):\n",
        "#       1) Compute pre-activation of layer 1:\n",
        "#          - z1 = X_in @ W1 + b1\n",
        "#       2) Apply nonlinearity:\n",
        "#          - h1 = act_forward(z1, act)\n",
        "#       3) Compute layer-2 output (logits or regression output):\n",
        "#          - z2 = h1 @ W2 + b2\n",
        "#       4) Branch by MODE:\n",
        "#          - If MODE == \"classification\":\n",
        "#              * y_pred_probs = softmax(z2)     # for reporting / cross-entropy\n",
        "#              * out = y_pred_probs\n",
        "#          - Else (regression):\n",
        "#              * out = z2 reshaped to (N,)      # flatten single output\n",
        "#       5) If return_intermediates:\n",
        "#              * return out, (z1, h1, z2)\n",
        "#          Else:\n",
        "#              * return out\n",
        "# def mlp_forward(X_in, act=\"relu\", return_intermediates=False): ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bbf4c6",
      "metadata": {
        "id": "57bbf4c6"
      },
      "outputs": [],
      "source": [
        "# ======================= Unit Tests for MLP (print-only PASS/FAIL) =======================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# Convenience\n",
        "Ncheck = min(7, len(X))  # small batch to test forward\n",
        "X_small = X[:Ncheck]\n",
        "X_zero = np.zeros_like(X_small)\n",
        "\n",
        "# 1) Required globals and callables exist\n",
        "tests.append(lambda: all(k in globals() for k in [\n",
        "    'm','w_scale_1','w_scale_2','b_scale_1','b_scale_2','ACT',\n",
        "    'relu','tanh','sigmoid','gelu','act_forward','softmax',\n",
        "    'rng','W1','b1','W2','b2','out_dim','mlp_forward','MODE','d'\n",
        "]) and all(callable(globals()[k]) for k in ['relu','tanh','sigmoid','gelu','act_forward','softmax','mlp_forward']))\n",
        "\n",
        "# 2) Weight/bias shapes are correct\n",
        "tests.append(lambda: W1.shape == (d, m) and b1.shape == (m,) and\n",
        "             ((MODE == \"regression\" and W2.shape == (m, 1) and b2.shape == (1,) and out_dim == 1) or\n",
        "              (MODE == \"classification\" and 'C' in globals() and W2.shape == (m, C) and b2.shape == (C,) and out_dim == C)))\n",
        "\n",
        "# 3) Activations run and preserve shape\n",
        "tests.append(lambda: all(\n",
        "    getattr(np, 'all')(np.isfinite(fn(X_small))) and fn(X_small).shape == X_small.shape\n",
        "    for fn in [relu, tanh, sigmoid, gelu]\n",
        "))\n",
        "\n",
        "# 4) act_forward dispatch works for all known names\n",
        "tests.append(lambda: all(\n",
        "    act_forward(X_small, name).shape == X_small.shape\n",
        "    for name in [\"relu\",\"tanh\",\"sigmoid\",\"gelu\"]\n",
        "))\n",
        "\n",
        "# 5) softmax is row-stochastic on logits: rows sum to 1 and are nonnegative\n",
        "def _softmax_ok():\n",
        "    Z = np.random.randn(Ncheck, max(2, out_dim))\n",
        "    P = softmax(Z)\n",
        "    s = np.abs(P.sum(axis=1) - 1.0)\n",
        "    return P.shape == Z.shape and np.all(P >= 0) and np.all(s < 1e-6)\n",
        "tests.append(_softmax_ok)\n",
        "\n",
        "# 6) Forward pass returns correct shape/type for current MODE\n",
        "def _forward_shape_ok():\n",
        "    out = mlp_forward(X_small, act=ACT, return_intermediates=False)\n",
        "    if MODE == \"classification\":\n",
        "        return isinstance(out, np.ndarray) and out.shape == (Ncheck, out_dim) and np.allclose(out.sum(axis=1), 1.0, atol=1e-5)\n",
        "    else:\n",
        "        return isinstance(out, np.ndarray) and out.shape == (Ncheck,) and np.issubdtype(out.dtype, np.floating)\n",
        "tests.append(_forward_shape_ok)\n",
        "\n",
        "# 7) Forward pass with intermediates returns (out, (z1,h1,z2)) with proper shapes\n",
        "def _forward_intermediates_ok():\n",
        "    out, (z1, h1, z2) = mlp_forward(X_small, act=ACT, return_intermediates=True)\n",
        "    ok = (z1.shape == (Ncheck, m)) and (h1.shape == (Ncheck, m)) and (z2.shape == (Ncheck, out_dim))\n",
        "    if MODE == \"classification\":\n",
        "        ok = ok and out.shape == (Ncheck, out_dim)\n",
        "    else:\n",
        "        ok = ok and out.shape == (Ncheck,)\n",
        "    return ok\n",
        "tests.append(_forward_intermediates_ok)\n",
        "\n",
        "# 8) Different activation names should work in mlp_forward\n",
        "tests.append(lambda: all(\n",
        "    (lambda o: (o.shape == (Ncheck, out_dim)) if MODE == \"classification\" else (o.shape == (Ncheck,)))(mlp_forward(X_small, act=name))\n",
        "    for name in [\"relu\",\"tanh\",\"sigmoid\",\"gelu\"]\n",
        "))\n",
        "\n",
        "# 9) Forward on zero input should be finite and have correct shape\n",
        "def _zero_input_ok():\n",
        "    out = mlp_forward(X_zero, act=ACT)\n",
        "    finite = np.all(np.isfinite(out))\n",
        "    if MODE == \"classification\":\n",
        "        return finite and out.shape == (Ncheck, out_dim) and np.allclose(out.sum(axis=1), 1.0, atol=1e-5)\n",
        "    else:\n",
        "        return finite and out.shape == (Ncheck,)\n",
        "tests.append(_zero_input_ok)\n",
        "\n",
        "# 10) out_dim is consistent with MODE\n",
        "tests.append(lambda: (MODE == \"classification\" and out_dim == C) or (MODE == \"regression\" and out_dim == 1) or False is False)\n",
        "\n",
        "# 11) Softmax invariance to constant shift (numerical stability check)\n",
        "def _softmax_shift_invariance():\n",
        "    Z = np.random.randn(Ncheck, max(2, out_dim))\n",
        "    shift = 10.0\n",
        "    return np.allclose(softmax(Z), softmax(Z + shift), atol=1e-8)\n",
        "tests.append(_softmax_shift_invariance)\n",
        "\n",
        "# 12) ACT string is one of the allowed set\n",
        "tests.append(lambda: ACT in {\"relu\",\"tanh\",\"sigmoid\",\"gelu\"})\n",
        "\n",
        "# 13) Calling act_forward with unknown name raises ValueError\n",
        "def _dispatch_raises():\n",
        "    try:\n",
        "        act_forward(X_small, \"does-not-exist\")\n",
        "        return False\n",
        "    except ValueError:\n",
        "        return True\n",
        "tests.append(_dispatch_raises)\n",
        "\n",
        "# 14) Forward pass does not mutate inputs\n",
        "def _no_input_mutation():\n",
        "    x_copy = X_small.copy()\n",
        "    _ = mlp_forward(X_small, act=ACT)\n",
        "    return np.array_equal(X_small, x_copy)\n",
        "tests.append(_no_input_mutation)\n",
        "\n",
        "# Run tests\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "# Print only PASS/FAIL lines (one per test in defined order)\n",
        "for r in results:\n",
        "    print(r)\n",
        "# =======================================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecd8d21",
      "metadata": {
        "id": "0ecd8d21"
      },
      "source": [
        "\n",
        "## Part D — Loss Functions (Evaluation Only)\n",
        "\n",
        "- **Regression:** `MSE`, `MAE` on predictions vs targets.  \n",
        "- **Classification:** **Cross-Entropy** using softmax probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e301c886",
      "metadata": {
        "id": "e301c886"
      },
      "outputs": [],
      "source": [
        "# TODO: Mean Squared Error (MSE)\n",
        "# def mse(y_true, y_pred):\n",
        "#     # Low-value: convert inputs to float numpy arrays\n",
        "#     # TODO: compute elementwise squared error: (y_pred - y_true) ** 2\n",
        "#     # TODO: take the mean over all elements\n",
        "#     # TODO: return the scalar as a Python float\n",
        "\n",
        "\n",
        "# TODO: Mean Absolute Error (MAE)\n",
        "# def mae(y_true, y_pred):\n",
        "#     # Low-value: convert inputs to float numpy arrays\n",
        "#     # TODO: compute elementwise absolute error: abs(y_pred - y_true)\n",
        "#     # TODO: take the mean over all elements\n",
        "#     # TODO: return the scalar as a Python float\n",
        "\n",
        "\n",
        "# TODO: Multiclass Cross-Entropy (negative log-likelihood)\n",
        "# def ce_multiclass(y_true_int, p_pred, eps=1e-12):\n",
        "#     # Assumptions:\n",
        "#     #   - y_true_int: integer class labels in [0, C-1], shape (N,)\n",
        "#     #   - p_pred: predicted probabilities, shape (N, C), rows sum to ~1\n",
        "#     #\n",
        "#     # Low-value: ensure p_pred is a NumPy array of floats and y_true_int is integer array\n",
        "#     # TODO: clip probabilities to [eps, 1 - eps] for numerical stability\n",
        "#     # TODO: gather the predicted probability of the true class for each sample\n",
        "#     #       (advanced indexing using (np.arange(N), y_true_int))\n",
        "#     # TODO: compute -mean(log(p_true_class))\n",
        "#     # TODO: return the scalar as a Python float\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780f9088",
      "metadata": {
        "id": "780f9088"
      },
      "outputs": [],
      "source": [
        "# ======================= Unit Tests for Metrics (print-only PASS/FAIL) =======================\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# --- Sanity data ---\n",
        "y_true_1d = np.array([0.0, 1.0, 2.0, -1.0, 3.5])\n",
        "y_pred_1d = np.array([0.0, 1.5, 1.0, -2.0, 3.0])\n",
        "\n",
        "# 1) Functions exist and are callable\n",
        "tests.append(lambda: all(name in globals() and callable(globals()[name])\n",
        "                         for name in [\"mse\",\"mae\",\"ce_multiclass\"]))\n",
        "\n",
        "# 2) MSE on simple known values\n",
        "def _mse_known():\n",
        "    # manual: ((0-0)^2 + (1.5-1)^2 + (1-2)^2 + (-2+1)^2 + (3-3.5)^2)/5\n",
        "    manual = (0**2 + 0.5**2 + (-1)**2 + (-1)**2 + (-0.5)**2)/5\n",
        "    return np.isclose(mse(y_true_1d, y_pred_1d), manual, rtol=0, atol=1e-12)\n",
        "tests.append(_mse_known)\n",
        "\n",
        "# 3) MAE on simple known values\n",
        "def _mae_known():\n",
        "    # manual: (|0-0| + |1.5-1| + |1-2| + |-2+1| + |3-3.5|)/5\n",
        "    manual = (0 + 0.5 + 1 + 1 + 0.5)/5\n",
        "    return np.isclose(mae(y_true_1d, y_pred_1d), manual, rtol=0, atol=1e-12)\n",
        "tests.append(_mae_known)\n",
        "\n",
        "# 4) MSE is zero when predictions equal truths\n",
        "tests.append(lambda: np.isclose(mse([1,2,3],[1,2,3]), 0.0))\n",
        "\n",
        "# 5) MAE is zero when predictions equal truths\n",
        "tests.append(lambda: np.isclose(mae([1,2,3],[1,2,3]), 0.0))\n",
        "\n",
        "# 6) MSE/MAE accept lists and different dtypes (broadcastability in numpy sense not required)\n",
        "tests.append(lambda: isinstance(mse([0,1],[0.0,1.0]), float) and isinstance(mae([0,1],[0.0,1.0]), float))\n",
        "\n",
        "# 7) MSE/MAE handle negative and non-integer values\n",
        "tests.append(lambda: np.isfinite(mse([-1.2, 3.4], [0.8, 3.0])) and np.isfinite(mae([-1.2, 3.4], [0.8, 3.0])))\n",
        "\n",
        "# --- Cross-entropy tests ---\n",
        "# Prepare small classification example (N=4, C=3)\n",
        "y_true_int = np.array([0, 2, 1, 2], dtype=int)\n",
        "P = np.array([\n",
        "    [0.7, 0.2, 0.1],  # correct: class 0\n",
        "    [0.1, 0.2, 0.7],  # correct: class 2\n",
        "    [0.2, 0.6, 0.2],  # correct: class 1\n",
        "    [0.05,0.15,0.80], # correct: class 2\n",
        "], dtype=float)\n",
        "\n",
        "# 8) ce_multiclass returns finite scalar and type float\n",
        "tests.append(lambda: isinstance(ce_multiclass(y_true_int, P), float) and np.isfinite(ce_multiclass(y_true_int, P)))\n",
        "\n",
        "# 9) ce_multiclass matches manual computation for known example\n",
        "def _ce_known():\n",
        "    # manual negative log-likelihood\n",
        "    p_true = np.array([0.7, 0.7, 0.6, 0.80])\n",
        "    manual = -np.mean(np.log(p_true))\n",
        "    return np.isclose(ce_multiclass(y_true_int, P), manual, atol=1e-12)\n",
        "tests.append(_ce_known)\n",
        "\n",
        "# 10) ce_multiclass clips near 0/1 safely (no -inf or nan)\n",
        "def _ce_clip():\n",
        "    y = np.array([0, 1], dtype=int)\n",
        "    P_bad = np.array([[1.0, 0.0],   # exact 0 and 1\n",
        "                      [0.0, 1.0]], dtype=float)\n",
        "    val = ce_multiclass(y, P_bad, eps=1e-12)\n",
        "    return np.isfinite(val) and val >= 0.0\n",
        "tests.append(_ce_clip)\n",
        "\n",
        "# 11) ce_multiclass decreases when true-class probability increases\n",
        "def _ce_monotone():\n",
        "    y = np.array([1], dtype=int)\n",
        "    p_low  = np.array([[0.8, 0.2]])\n",
        "    p_high = np.array([[0.2, 0.8]])\n",
        "    return ce_multiclass(y, p_high) < ce_multiclass(y, p_low)\n",
        "tests.append(_ce_monotone)\n",
        "\n",
        "# 12) ce_multiclass supports different eps values without crashing\n",
        "tests.append(lambda: np.isfinite(ce_multiclass(y_true_int, P, eps=1e-6)))\n",
        "\n",
        "# 13) Inputs not mutated by metrics (no in-place ops)\n",
        "def _no_mutation():\n",
        "    yt = y_true_1d.copy()\n",
        "    yp = y_pred_1d.copy()\n",
        "    _ = mse(yt, yp); _ = mae(yt, yp)\n",
        "    return np.array_equal(yt, y_true_1d) and np.array_equal(yp, y_pred_1d)\n",
        "tests.append(_no_mutation)\n",
        "\n",
        "# 14) Works with column-vector vs 1D vector shapes (common pitfall)\n",
        "def _shape_flex():\n",
        "    yt = np.array([[0.0],[1.0],[2.0]])\n",
        "    yp = np.array([0.0, 1.0, 2.0])\n",
        "    # Broadcasting to float arrays inside functions should make this finite\n",
        "    return np.isfinite(mse(yt, yp)) and np.isfinite(mae(yt, yp))\n",
        "tests.append(_shape_flex)\n",
        "\n",
        "# Run tests\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "# Print only PASS/FAIL\n",
        "for r in results:\n",
        "    print(r)\n",
        "# ===========================================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e178e8b2",
      "metadata": {
        "id": "e178e8b2"
      },
      "source": [
        "\n",
        "## Part E — Evaluation & Basic Tables\n",
        "\n",
        "We forward the MLP on each split and compute the appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d616e166",
      "metadata": {
        "id": "d616e166"
      },
      "outputs": [],
      "source": [
        "# ============================== TODOs for Forward & Metrics ==============================\n",
        "\n",
        "# TODO: If MODE == \"regression\":\n",
        "#   1) Run forward passes with intermediates:\n",
        "#        - yhat_train, inter_train = mlp_forward(X_train, ACT, return_intermediates=True)\n",
        "#        - yhat_dev,   inter_dev   = mlp_forward(X_dev,   ACT, return_intermediates=True)\n",
        "#        - yhat_test,  inter_test  = mlp_forward(X_test,  ACT, return_intermediates=True)\n",
        "#      (Low-value detail: 'inter_*' holds (z1, h1, z2); keep API consistent with earlier.)\n",
        "#\n",
        "#   2) Compute metrics per split using previously defined mse/mae:\n",
        "#        - train: MSE = mse(y_train, yhat_train), MAE = mae(y_train, yhat_train)\n",
        "#        - dev:   MSE = mse(y_dev,   yhat_dev),   MAE = mae(y_dev,   yhat_dev)\n",
        "#        - test:  MSE = mse(y_test,  yhat_test),  MAE = mae(y_test,  yhat_test)\n",
        "#\n",
        "#   3) Build a small table (list of dicts) and create a DataFrame:\n",
        "#        rows = [\n",
        "#          {\"split\": \"train\", \"MSE\": <...>, \"MAE\": <...>},\n",
        "#          {\"split\": \"dev\",   \"MSE\": <...>, \"MAE\": <...>},\n",
        "#          {\"split\": \"test\",  \"MSE\": <...>, \"MAE\": <...>}\n",
        "#        ]\n",
        "#        df_metrics = pd.DataFrame(rows)\n",
        "\n",
        "# TODO: Else (classification):\n",
        "#   1) Run forward passes with intermediates to get class probabilities:\n",
        "#        - p_train, inter_train = mlp_forward(X_train, ACT, return_intermediates=True)\n",
        "#        - p_dev,   inter_dev   = mlp_forward(X_dev,   ACT, return_intermediates=True)\n",
        "#        - p_test,  inter_test  = mlp_forward(X_test,  ACT, return_intermediates=True)\n",
        "#\n",
        "#   2) Define trivial utilities (low educational value; keep concise):\n",
        "#        - pred_from_probs(p): argmax over axis=1\n",
        "#        - acc(y_true, y_pred): mean equality\n",
        "#\n",
        "#   3) Compute metrics per split using ce_multiclass + accuracy:\n",
        "#        - train: CE = ce_multiclass(y_train, p_train), ACC = acc(y_train, pred_from_probs(p_train))\n",
        "#        - dev:   CE = ce_multiclass(y_dev,   p_dev),   ACC = acc(y_dev,   pred_from_probs(p_dev))\n",
        "#        - test:  CE = ce_multiclass(y_test,  p_test),  ACC = acc(y_test,  pred_from_probs(p_test))\n",
        "#\n",
        "#   4) Build a table and create a DataFrame:\n",
        "#        rows = [\n",
        "#          {\"split\": \"train\", \"CE\": <...>, \"ACC\": <...>},\n",
        "#          {\"split\": \"dev\",   \"CE\": <...>, \"ACC\": <...>},\n",
        "#          {\"split\": \"test\",  \"CE\": <...>, \"ACC\": <...>}\n",
        "#        ]\n",
        "#        df_metrics = pd.DataFrame(rows)\n",
        "\n",
        "# TODO: Finally, display df_metrics (e.g., last-line expression to show the DataFrame)\n",
        "# df_metrics\n",
        "# =========================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9354cfec",
      "metadata": {
        "id": "9354cfec"
      },
      "outputs": [],
      "source": [
        "# ================== Unit Tests for Forward Passes & Metrics (print-only PASS/FAIL) ==================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# 0) df_metrics exists and is a DataFrame with 3 rows\n",
        "tests.append(lambda: 'df_metrics' in globals() and isinstance(df_metrics, pd.DataFrame) and len(df_metrics) == 3)\n",
        "\n",
        "# 1) Required split names present and in expected order\n",
        "tests.append(lambda: 'split' in df_metrics.columns and list(df_metrics['split']) == [\"train\",\"dev\",\"test\"])\n",
        "\n",
        "# 2) Branch-dependent artifacts exist and have sensible shapes\n",
        "def _branch_shapes_ok():\n",
        "    if MODE == \"regression\":\n",
        "        condA = all(k in globals() for k in ['yhat_train','yhat_dev','yhat_test','inter_train','inter_dev','inter_test'])\n",
        "        if not condA: return False\n",
        "        a = len(yhat_train) == len(y_train)\n",
        "        b = len(yhat_dev)   == len(y_dev)\n",
        "        c = len(yhat_test)  == len(y_test)\n",
        "        # intermediates shape check: (z1,h1,z2)\n",
        "        z1,h1,z2 = inter_train\n",
        "        ok_int = z1.shape[0] == len(y_train) and h1.shape[0] == len(y_train) and z2.shape[0] == len(y_train)\n",
        "        return a and b and c and ok_int\n",
        "    else:\n",
        "        condB = all(k in globals() for k in ['p_train','p_dev','p_test','inter_train','inter_dev','inter_test'])\n",
        "        if not condB: return False\n",
        "        a = p_train.shape[0] == len(y_train)\n",
        "        b = p_dev.shape[0]   == len(y_dev)\n",
        "        c = p_test.shape[0]  == len(y_test)\n",
        "        z1,h1,z2 = inter_train\n",
        "        ok_int = z1.shape[0] == len(y_train) and h1.shape[0] == len(y_train) and z2.shape[0] == len(y_train)\n",
        "        return a and b and c and ok_int\n",
        "tests.append(_branch_shapes_ok)\n",
        "\n",
        "# 3) Columns of df_metrics are correct for the MODE\n",
        "tests.append(lambda:\n",
        "    (MODE == \"regression\" and set(df_metrics.columns) == {\"split\",\"MSE\",\"MAE\"}) or\n",
        "    (MODE == \"classification\" and set(df_metrics.columns) == {\"split\",\"CE\",\"ACC\"})\n",
        ")\n",
        "\n",
        "# 4) All metric values are finite numbers\n",
        "def _finite_metrics():\n",
        "    if MODE == \"regression\":\n",
        "        return np.isfinite(df_metrics[\"MSE\"]).all() and np.isfinite(df_metrics[\"MAE\"]).all()\n",
        "    else:\n",
        "        return np.isfinite(df_metrics[\"CE\"]).all() and np.isfinite(df_metrics[\"ACC\"]).all()\n",
        "tests.append(_finite_metrics)\n",
        "\n",
        "# 5) Metrics in valid ranges\n",
        "def _metric_ranges():\n",
        "    if MODE == \"regression\":\n",
        "        return (df_metrics[\"MSE\"] >= 0).all() and (df_metrics[\"MAE\"] >= 0).all()\n",
        "    else:\n",
        "        acc_ok = ((df_metrics[\"ACC\"] >= 0) & (df_metrics[\"ACC\"] <= 1)).all()\n",
        "        ce_ok  = (df_metrics[\"CE\"] >= 0).all()\n",
        "        return acc_ok and ce_ok\n",
        "tests.append(_metric_ranges)\n",
        "\n",
        "# 6) Consistency check: recompute one split to match (train)\n",
        "def _consistency_one_split():\n",
        "    if MODE == \"regression\":\n",
        "        # recompute MSE/MAE for train\n",
        "        i = df_metrics.index[df_metrics[\"split\"]==\"train\"][0]\n",
        "        mse_calc = float(np.mean((np.asarray(yhat_train, float) - np.asarray(y_train, float))**2))\n",
        "        mae_calc = float(np.mean(np.abs(np.asarray(yhat_train, float) - np.asarray(y_train, float))))\n",
        "        return np.isclose(df_metrics.loc[i,\"MSE\"], mse_calc, atol=1e-12) and np.isclose(df_metrics.loc[i,\"MAE\"], mae_calc, atol=1e-12)\n",
        "    else:\n",
        "        i = df_metrics.index[df_metrics[\"split\"]==\"train\"][0]\n",
        "        preds = np.argmax(p_train, axis=1)\n",
        "        acc_calc = float(np.mean(preds == y_train))\n",
        "        ce_calc = float(-np.mean(np.log(np.clip(p_train[np.arange(len(y_train)), y_train], 1e-12, 1 - 1e-12))))\n",
        "        return np.isclose(df_metrics.loc[i,\"ACC\"], acc_calc, atol=1e-12) and np.isclose(df_metrics.loc[i,\"CE\"], ce_calc, atol=1e-12)\n",
        "tests.append(_consistency_one_split)\n",
        "\n",
        "# 7) Intermediates shapes: z1,h1 have width m, z2 has width out_dim\n",
        "def _intermediate_widths():\n",
        "    z1,h1,z2 = inter_train\n",
        "    if not (z1.shape[1] == m and h1.shape[1] == m): return False\n",
        "    if MODE == \"regression\":\n",
        "        return z2.shape[1] == 1\n",
        "    else:\n",
        "        return z2.shape[1] == out_dim\n",
        "tests.append(_intermediate_widths)\n",
        "\n",
        "# 8) Forward with different activation names still works for a tiny batch (no exception, correct shapes)\n",
        "def _forward_other_acts():\n",
        "    try:\n",
        "        X_small = X_train[:min(5, len(X_train))]\n",
        "        for name in [\"relu\",\"tanh\",\"sigmoid\",\"gelu\"]:\n",
        "            out = mlp_forward(X_small, name)\n",
        "            if MODE == \"regression\":\n",
        "                if not (isinstance(out, np.ndarray) and out.shape == (len(X_small),)): return False\n",
        "            else:\n",
        "                if not (isinstance(out, np.ndarray) and out.shape == (len(X_small), out_dim) and\n",
        "                        np.allclose(out.sum(axis=1), 1.0, atol=1e-5)): return False\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "tests.append(_forward_other_acts)\n",
        "\n",
        "# 9) df_metrics sums to n samples across splits (sanity vs previously defined splits)\n",
        "tests.append(lambda: len(y_train) + len(y_dev) + len(y_test) == len(X))\n",
        "\n",
        "# 10) No NaNs in intermediates or outputs for the train split\n",
        "def _no_nans():\n",
        "    if MODE == \"regression\":\n",
        "        return np.isfinite(yhat_train).all() and all(np.isfinite(a).all() for a in inter_train)\n",
        "    else:\n",
        "        return np.isfinite(p_train).all() and all(np.isfinite(a).all() for a in inter_train)\n",
        "tests.append(_no_nans)\n",
        "\n",
        "# Run tests\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "# Print only PASS/FAIL lines (one per test in order)\n",
        "for r in results:\n",
        "    print(r)\n",
        "# ==================================================================================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410b85a5",
      "metadata": {
        "id": "410b85a5"
      },
      "source": [
        "\n",
        "## Part F — Visualizations: Saturation & Contributions\n",
        "\n",
        "We visualize:\n",
        "- Histograms of **pre-activation** `z1` and **activation** `h1` to see saturation (e.g., Sigmoid edges, ReLU dead units).\n",
        "- For regression: per-example **MSE vs MAE contributions**.  \n",
        "- For classification: per-example **−log p_true** (CE contributions).\n",
        "\n",
        "**TODO ideas:**\n",
        "- Increase `w_scale_1` to push `z1` into saturation for Sigmoid/Tanh; compare losses.\n",
        "- Compare activations side-by-side by looping over `ACT in {...}` and building a table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "641b2338",
      "metadata": {
        "id": "641b2338"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Grab intermediates on TEST split for plots\n",
        "z1_test, h1_test, z2_test = inter_test\n",
        "\n",
        "# Histograms of z1 and h1\n",
        "plt.figure()\n",
        "plt.hist(z1_test.ravel(), bins=40)\n",
        "plt.title(f\"Pre-activation z1 histogram (ACT={ACT})\")\n",
        "plt.xlabel(\"z1\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.hist(h1_test.ravel(), bins=40)\n",
        "plt.title(f\"Activation h1 histogram (ACT={ACT})\")\n",
        "plt.xlabel(\"h1\"); plt.ylabel(\"count\")\n",
        "plt.show()\n",
        "\n",
        "if MODE == \"regression\":\n",
        "    yhat_test = yhat_test  # from above\n",
        "    mse_contrib = (yhat_test - y_test)**2\n",
        "    mae_contrib = np.abs(yhat_test - y_test)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(y_test, mse_contrib, label=\"MSE contrib\")\n",
        "    plt.scatter(y_test, mae_contrib, label=\"MAE contrib\")\n",
        "    plt.title(\"Per-example contributions (test)\")\n",
        "    plt.xlabel(\"y_true\"); plt.ylabel(\"loss contribution\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # CE contributions = -log p_true\n",
        "    idx = (np.arange(len(y_test)), y_test)\n",
        "    ce_contrib = -np.log(np.clip(p_test[idx], 1e-12, 1-1e-12))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(np.arange(len(ce_contrib)), ce_contrib)\n",
        "    plt.title(\"Cross-Entropy per-example contributions (test)\")\n",
        "    plt.xlabel(\"example index\"); plt.ylabel(\"-log p_true\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d787296",
      "metadata": {
        "id": "2d787296"
      },
      "source": [
        "### Analysis: Activation Saturation and Loss Contributions\n",
        "\n",
        "**Instructions:**  \n",
        "Examine the histograms and scatter plots you generated above.\n",
        "\n",
        "- **For activation histograms:**  \n",
        "  - Compare the shapes of `z1` (pre-activation) and `h1` (post-activation).  \n",
        "  - Are most values centered around zero, or are they saturating near the limits of the activation function (e.g., 0/1 for Sigmoid, -1/1 for Tanh, or zeros for ReLU)?  \n",
        "  - What might this imply about gradient flow and neuron activity?\n",
        "\n",
        "- **For regression plots:**  \n",
        "  - How do individual examples contribute differently to MSE vs. MAE?  \n",
        "  - Do outliers dominate one loss more than the other?\n",
        "\n",
        "- **For classification plots:**  \n",
        "  - Observe the distribution of `−log p_true` values (per-example cross-entropy).  \n",
        "  - Which examples have the largest contributions, and what does that suggest about model confidence or misclassifications?\n",
        "\n",
        "**Your task:**  \n",
        "Write a short discussion (5–10 sentences) summarizing your observations.  \n",
        "Reflect on how **activation choice** and **weight scaling (`w_scale_1`)** affect saturation and model behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4368e4db",
      "metadata": {
        "id": "4368e4db"
      },
      "source": [
        "\n",
        "## Part G — Multi-Activation Benchmark (TODO)\n",
        "\n",
        "Loop over several activations and compare metrics in a single table.  \n",
        "**Tip:** keep the **same fixed weights** `W1, b1, W2, b2` while changing only the activation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec00f213",
      "metadata": {
        "id": "ec00f213"
      },
      "outputs": [],
      "source": [
        "\n",
        "ACT_LIST = [\"relu\", \"tanh\", \"sigmoid\", \"gelu\"]  # TODO: extend/trim\n",
        "\n",
        "def eval_activation(act_name):\n",
        "    if MODE == \"regression\":\n",
        "        yhat = mlp_forward(X_test, act_name)\n",
        "        return {\"ACT\": act_name, \"MSE_test\": mse(y_test, yhat), \"MAE_test\": mae(y_test, yhat)}\n",
        "    else:\n",
        "        p = mlp_forward(X_test, act_name)\n",
        "        def acc(y_true, y_pred): return float(np.mean(y_true == y_pred))\n",
        "        return {\"ACT\": act_name, \"CE_test\": ce_multiclass(y_test, p), \"ACC_test\": acc(y_test, np.argmax(p, axis=1))}\n",
        "\n",
        "bench_rows = [eval_activation(a) for a in ACT_LIST]\n",
        "df_bench = pd.DataFrame(bench_rows)\n",
        "df_bench.sort_values(df_bench.columns[1], inplace=True)  # sort by the metric column\n",
        "df_bench\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b76137",
      "metadata": {
        "id": "30b76137"
      },
      "source": [
        "### Analysis: Comparing Activation Functions\n",
        "\n",
        "**Instructions:**  \n",
        "Review the table above that summarizes test performance for different activation functions (`ACT_LIST`).\n",
        "\n",
        "- **For regression:**\n",
        "  - Compare the **MSE** and **MAE** values across activations.\n",
        "  - Which activation yields the lowest error?  \n",
        "  - How might nonlinearities like `tanh` or `sigmoid` behave differently from `ReLU` or `GELU` in this dataset?\n",
        "\n",
        "- **For classification:**\n",
        "  - Examine the **Cross-Entropy (CE)** and **Accuracy (ACC)** metrics.\n",
        "  - Which activation produces the best balance between loss and accuracy?  \n",
        "  - Are there signs of under- or over-saturation for certain activations (e.g., sigmoid outputs too flat)?\n",
        "\n",
        "**Your task:**  \n",
        "Write a concise analysis (around 5–8 sentences) interpreting these results.  \n",
        "Discuss how activation choice affects model expressiveness, stability, and performance — and whether the observed ranking aligns with your expectations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1dcd9bf",
      "metadata": {
        "id": "d1dcd9bf"
      },
      "source": [
        "\n",
        "## Part H — Weight-Scale Sweep (Evaluation-Only Experiment)\n",
        "\n",
        "Increase/decrease `w_scale_1` (and/or input scale) to push activations into **saturation** or **linear** regimes.  \n",
        "Observe how metrics change. Keep parameters **fixed per setting**; you are NOT training anything.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd60c5c",
      "metadata": {
        "id": "bcd60c5c"
      },
      "outputs": [],
      "source": [
        "# NOTE: We want to sweep w_scale_1 while avoiding unintended drift in globals.\n",
        "#       We'll (re)sample fresh weights per scale from a fixed RNG seed stream.\n",
        "\n",
        "# TODO: Define the list of scales to sweep for W1 (e.g., [0.2, 0.5, 0.8, 1.2, 2.0])\n",
        "# scales = [...]\n",
        "\n",
        "# TODO: Initialize a master RNG with a fixed seed (e.g., 999) so each run is reproducible\n",
        "# rng_master = np.random.default_rng(999)\n",
        "\n",
        "# TODO: Implement a helper that (re)initializes W1,b1,W2,b2 for a given scale1\n",
        "#       and evaluates performance on the TEST split using the current ACT.\n",
        "# def resample_and_eval(scale1):\n",
        "#     # Low-value: declare globals (W1, b1, W2, b2) since we reassign them\n",
        "#     # global W1, b1, W2, b2\n",
        "#\n",
        "#     # 1) Re-sample first-layer params with scale1:\n",
        "#     #    - W1 ~ N(0,1) with shape (d, m), scaled by scale1\n",
        "#     #    - b1 ~ N(0,1) with shape (m,), scaled by b_scale_1\n",
        "#     #    Use rng_master for both so the sequence is deterministic across scales\n",
        "#\n",
        "#     # 2) Re-sample second-layer params with fixed scales (w_scale_2, b_scale_2):\n",
        "#     #    - If MODE == \"regression\": W2 shape (m,1), b2 shape (1,)\n",
        "#     #    - Else (classification):   W2 shape (m,C), b2 shape (C,)\n",
        "#\n",
        "#     # 3) Forward on TEST split with current ACT:\n",
        "#     #    - If regression: yhat = mlp_forward(X_test, ACT)\n",
        "#     #      * Compute MSE_test and MAE_test via mse/mae\n",
        "#     #      * Return a dict: {\"w_scale_1\": scale1, \"MSE_test\": ..., \"MAE_test\": ...}\n",
        "#     #    - Else: p = mlp_forward(X_test, ACT)\n",
        "#     #      * Compute CE_test via ce_multiclass(y_test, p)\n",
        "#     #      * Compute ACC_test via argmax + mean equality\n",
        "#     #      * Return a dict: {\"w_scale_1\": scale1, \"CE_test\": ..., \"ACC_test\": ...}\n",
        "#     # return {...}\n",
        "\n",
        "# TODO: Build a DataFrame by mapping resample_and_eval over all scales:\n",
        "#       - rows = [resample_and_eval(s) for s in scales]\n",
        "#       - df_sweep = pd.DataFrame(rows)\n",
        "\n",
        "# TODO: Display df_sweep as the final line to show results\n",
        "# df_sweep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86aec79a",
      "metadata": {
        "id": "86aec79a"
      },
      "outputs": [],
      "source": [
        "# ================== Unit Tests for w_scale_1 Sweep (print-only PASS/FAIL) ==================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "results = []\n",
        "\n",
        "def run_test(fn):\n",
        "    try:\n",
        "        ok = bool(fn())\n",
        "    except Exception:\n",
        "        ok = False\n",
        "    results.append(\"PASS\" if ok else \"FAIL\")\n",
        "\n",
        "tests = []\n",
        "\n",
        "# 1) Required globals exist and are of expected types\n",
        "tests.append(lambda: all(k in globals() for k in [\n",
        "    'scales','rng_master','resample_and_eval','df_sweep','MODE'\n",
        "]) and isinstance(scales, (list, tuple)) and callable(resample_and_eval) and isinstance(df_sweep, pd.DataFrame))\n",
        "\n",
        "# 2) df_sweep has one row per scale\n",
        "tests.append(lambda: len(df_sweep) == len(scales))\n",
        "\n",
        "# 3) df_sweep contains a 'w_scale_1' column matching the provided scales (order preserved)\n",
        "tests.append(lambda: 'w_scale_1' in df_sweep.columns and list(df_sweep['w_scale_1']) == list(scales))\n",
        "\n",
        "# 4) Correct metric columns are present depending on MODE\n",
        "tests.append(lambda:\n",
        "    (MODE == \"regression\" and set(df_sweep.columns) == {\"w_scale_1\",\"MSE_test\",\"MAE_test\"}) or\n",
        "    (MODE == \"classification\" and set(df_sweep.columns) == {\"w_scale_1\",\"CE_test\",\"ACC_test\"})\n",
        ")\n",
        "\n",
        "# 5) All metric values are finite\n",
        "def _finite_metrics():\n",
        "    if MODE == \"regression\":\n",
        "        return np.isfinite(df_sweep[\"MSE_test\"]).all() and np.isfinite(df_sweep[\"MAE_test\"]).all()\n",
        "    else:\n",
        "        return np.isfinite(df_sweep[\"CE_test\"]).all() and np.isfinite(df_sweep[\"ACC_test\"]).all()\n",
        "tests.append(_finite_metrics)\n",
        "\n",
        "# 6) Metric ranges valid\n",
        "def _metric_ranges():\n",
        "    if MODE == \"regression\":\n",
        "        return (df_sweep[\"MSE_test\"] >= 0).all() and (df_sweep[\"MAE_test\"] >= 0).all()\n",
        "    else:\n",
        "        ce_ok = (df_sweep[\"CE_test\"] >= 0).all()\n",
        "        acc_ok = ((df_sweep[\"ACC_test\"] >= 0) & (df_sweep[\"ACC_test\"] <= 1)).all()\n",
        "        return ce_ok and acc_ok\n",
        "tests.append(_metric_ranges)\n",
        "\n",
        "# 7) resample_and_eval returns dict with expected keys for a valid scale\n",
        "def _ree_keys_ok():\n",
        "    s = scales[0]\n",
        "    out = resample_and_eval(s)\n",
        "    if MODE == \"regression\":\n",
        "        return set(out.keys()) == {\"w_scale_1\",\"MSE_test\",\"MAE_test\"} and out[\"w_scale_1\"] == s\n",
        "    else:\n",
        "        return set(out.keys()) == {\"w_scale_1\",\"CE_test\",\"ACC_test\"} and out[\"w_scale_1\"] == s\n",
        "tests.append(_ree_keys_ok)\n",
        "\n",
        "# 8) Calling resample_and_eval on another scale returns finite metrics (does not crash)\n",
        "def _ree_finite_ok():\n",
        "    s = scales[-1]\n",
        "    out = resample_and_eval(s)\n",
        "    vals = [v for k,v in out.items() if k != \"w_scale_1\"]\n",
        "    return all(np.isfinite(v) for v in vals)\n",
        "tests.append(_ree_finite_ok)\n",
        "\n",
        "# 9) df_sweep has unique scales (no duplicates)\n",
        "tests.append(lambda: df_sweep[\"w_scale_1\"].is_unique)\n",
        "\n",
        "# 10) No NaNs present anywhere in df_sweep\n",
        "tests.append(lambda: not df_sweep.isna().any().any())\n",
        "\n",
        "# 11) rng_master exists and is a Generator (sanity)\n",
        "tests.append(lambda: isinstance(rng_master, np.random.Generator))\n",
        "\n",
        "# 12) resample_and_eval respects MODE by producing correctly shaped forward outputs\n",
        "def _forward_shape_check():\n",
        "    # Just verify mlp_forward still works with the current globals after resampling\n",
        "    X_small = X_test[:min(5, len(X_test))]\n",
        "    out = mlp_forward(X_small, ACT)\n",
        "    if MODE == \"regression\":\n",
        "        return isinstance(out, np.ndarray) and out.shape == (len(X_small),)\n",
        "    else:\n",
        "        return isinstance(out, np.ndarray) and out.shape[1] >= 2 and out.shape[0] == len(X_small) \\\n",
        "               and np.allclose(out.sum(axis=1), 1.0, atol=1e-5)\n",
        "tests.append(_forward_shape_check)\n",
        "\n",
        "# Run tests\n",
        "for t in tests:\n",
        "    run_test(t)\n",
        "\n",
        "# Print only PASS/FAIL lines (in order)\n",
        "for r in results:\n",
        "    print(r)\n",
        "# =========================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf821763",
      "metadata": {
        "id": "cf821763"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plot sweep results (pick the primary metric column automatically)\n",
        "metric_cols = [c for c in df_sweep.columns if c.endswith(\"_test\") and c != \"ACC_test\"]\n",
        "primary = metric_cols[0] if metric_cols else \"CE_test\"\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(df_sweep[\"w_scale_1\"], df_sweep[primary], marker=\"o\")\n",
        "plt.title(f\"Metric vs First-Layer Weight Scale (ACT={ACT})\")\n",
        "plt.xlabel(\"w_scale_1\"); plt.ylabel(primary)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf7cce4",
      "metadata": {
        "id": "9cf7cce4"
      },
      "source": [
        "### Analysis: Effect of First-Layer Weight Scale\n",
        "\n",
        "**Instructions:**  \n",
        "Analyze the curve shown in the plot above.\n",
        "\n",
        "- How does the model’s **test metric** (e.g., MSE, CE) change as `w_scale_1` increases?  \n",
        "- Is there a clear **optimal scale** that minimizes error or loss?  \n",
        "- What happens at very small vs. very large scales — do you observe signs of **underactivation** (too small weights) or **saturation/exploding activations** (too large weights)?  \n",
        "- How does the activation function (`ACT`) influence this relationship?  \n",
        "\n",
        "**Your task:**  \n",
        "Write a short reflection (5–8 sentences) discussing how the first-layer weight scale affects model behavior and performance stability.  \n",
        "Relate your explanation to the concepts of **activation saturation**, **gradient flow**, and **initialization sensitivity**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abde3c9b",
      "metadata": {
        "id": "abde3c9b"
      },
      "source": [
        "\n",
        "## Part I — (Optional) Load Fixed Weights from `.npy` (TODO)\n",
        "\n",
        "If you want reproducible fixed parameters from disk, load them here:\n",
        "- `W1.npy` with shape `(d, m)`, `b1.npy` with shape `(m,)`\n",
        "- `W2.npy` with shape `(m, out_dim)`, `b2.npy` with shape `(out_dim,)`\n",
        "\n",
        "> After loading, **do not** modify weights during the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6cffb6",
      "metadata": {
        "id": "2e6cffb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Example hooks (commented):\n",
        "# W1 = np.load(\"W1.npy\"); b1 = np.load(\"b1.npy\")\n",
        "# W2 = np.load(\"W2.npy\"); b2 = np.load(\"b2.npy\")\n",
        "# print(W1.shape, b1.shape, W2.shape, b2.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba0edec",
      "metadata": {
        "id": "2ba0edec"
      },
      "source": [
        "\n",
        "## Summary & Reflection\n",
        "\n",
        "- A **forward-only MLP** with fixed weights lets us isolate the effect of **activations** and **weight scales**.\n",
        "- **Saturation** (Sigmoid/Tanh) or **dead ReLUs** can flatten outputs and worsen loss—even without training.\n",
        "- The **weight-scale sweep** and **multi-activation benchmark** help you design informative *evaluation-only* experiments.\n",
        "\n",
        "**Required write-up (2–3 paragraphs):**  \n",
        "Summarize your findings with at least one figure (histogram or sweep plot) and one table (benchmark). Point out two cases where activation choice changes the ranking of models by the chosen metric.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}