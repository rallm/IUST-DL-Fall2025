{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rallm/IUST-DL-Fall2025/blob/main/HW1/HWP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ed2b44",
      "metadata": {
        "id": "a6ed2b44"
      },
      "source": [
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "# Closed-Form Linear Regression\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://mad-institute.com/images/universitys/iranuniversity2.png\" alt=\"IUST Logo\" width=\"180\" height=\"180\">\n",
        "</p>\n",
        "\n",
        "**University:** IRAN University of Science and Technology  \n",
        "**Course:** Deep Learning  \n",
        "**Term:** First semester of academic year 1404–1405\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "012779a8",
      "metadata": {
        "id": "012779a8"
      },
      "source": [
        "\n",
        "## Notebook: Closed-Form Linear Regression (OLS) — *Evaluation-Only*\n",
        "\n",
        "This notebook walks you through **Ordinary Least Squares** with **closed-form solutions** (no gradients):\n",
        "- Build a design matrix and fit OLS using **`pinv`** and **`lstsq`**.\n",
        "- Evaluate **MSE** and **MAE** and analyze where they **disagree**.\n",
        "- Visualize residuals and per-example contributions.\n",
        "- Run a simple **k-fold** evaluation and an **outlier sensitivity** mini-experiment.\n",
        "\n",
        "> **Policy for this course unit:** Do **not** use model selection, regularization, dropout, label smoothing, adversarial training, backpropagation, or any gradient-based optimizer. We only compute **forward predictions** and **report metrics**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a59fa19",
      "metadata": {
        "id": "6a59fa19"
      },
      "source": [
        "\n",
        "### Learning Objectives\n",
        "1. Implement closed-form OLS with `np.linalg.pinv` and `np.linalg.lstsq`.\n",
        "2. Compute and compare **MSE** vs **MAE** on the **same predictions**.\n",
        "3. Inspect disagreements via per-example contributions and residual views.\n",
        "4. Run k-fold evaluation and study sensitivity to **outliers** — still **evaluation-only**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "65accd56",
      "metadata": {
        "id": "65accd56",
        "outputId": "7357683b-5813-4111-98b9-6ac306aaff24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 2.0.2 | pandas: 2.2.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)  # Reproducibility\n",
        "print(\"NumPy:\", np.__version__, \"| pandas:\", pd.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c9a3d2",
      "metadata": {
        "id": "69c9a3d2"
      },
      "source": [
        "\n",
        "## Part A — Data Generation (Synthetic)\n",
        "\n",
        "We build a simple **1D-to-1D** regression dataset with optional sinusoidal ripple and a few **outliers**.\n",
        "\n",
        "**TODO ideas:**\n",
        "- Change noise level, outlier fraction/magnitude to see how **MSE vs MAE** react.\n",
        "- Change the ground-truth function to be more nonlinear and see what happens to OLS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4df704f4",
      "metadata": {
        "id": "4df704f4",
        "outputId": "d7cd3a3b-8e75-4fc0-c005-563495c1f264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2119732960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# ----------------------------- Wrap into DataFrame -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y_clean\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_clean\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# ----------------------------- TODO: configurable data generator -----------------------------\n",
        "\n",
        "# TODO: choose number of samples (e.g., 50, 200, 1000)\n",
        "# n = ...\n",
        "\n",
        "# TODO: define input domain range\n",
        "# x_min, x_max = ...\n",
        "\n",
        "# TODO: set Gaussian noise standard deviation\n",
        "# sigma = ...\n",
        "\n",
        "# TODO: specify fraction of outliers (0.0 to ~0.1)\n",
        "# outlier_frac = ...\n",
        "\n",
        "# TODO: set magnitude for outlier noise\n",
        "# outlier_mag = ...\n",
        "\n",
        "\n",
        "# ----------------------------- Ground-truth function -----------------------------\n",
        "def f_true(x):\n",
        "    # TODO: choose and implement one of the versions below\n",
        "    # (linear + ripple) or (pure linear) or (nonlinear)\n",
        "    pass\n",
        "\n",
        "\n",
        "# ----------------------------- Data generation -----------------------------\n",
        "\n",
        "# TODO: generate input points uniformly in the given range\n",
        "# X = ...\n",
        "\n",
        "# TODO: compute ground-truth outputs\n",
        "# y_clean = ...\n",
        "\n",
        "# TODO: add Gaussian noise\n",
        "# noise = ...\n",
        "\n",
        "# TODO: inject outliers by perturbing a random subset\n",
        "# k = ...\n",
        "# if k > 0:\n",
        "#     idx = ...\n",
        "#     noise[idx] += ...\n",
        "\n",
        "# TODO: combine clean outputs and noise\n",
        "# y = ...\n",
        "\n",
        "\n",
        "# ----------------------------- Wrap into DataFrame -----------------------------\n",
        "df = pd.DataFrame({\"x\": X, \"y\": y, \"y_clean\": y_clean})\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546dceab",
      "metadata": {
        "id": "546dceab"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "class TestSyntheticDataGenerator(unittest.TestCase):\n",
        "    def test_basic_shapes(self):\n",
        "        # X, y, and y_clean must have the same length (equal to n)\n",
        "        self.assertEqual(len(X), n)\n",
        "        self.assertEqual(len(y), n)\n",
        "        self.assertEqual(len(y_clean), n)\n",
        "        # DataFrame should have n rows and 3 columns\n",
        "        self.assertEqual(df.shape, (n, 3))\n",
        "\n",
        "    def test_domain(self):\n",
        "        # X should span from x_min to x_max and be monotonically increasing\n",
        "        self.assertAlmostEqual(X[0], x_min, places=7)\n",
        "        self.assertAlmostEqual(X[-1], x_max, places=7)\n",
        "        self.assertTrue(np.all(np.diff(X) > 0))\n",
        "\n",
        "    def test_y_composition(self):\n",
        "        # y must be the sum of y_clean and noise\n",
        "        self.assertTrue(np.allclose(y, y_clean + noise))\n",
        "\n",
        "    def test_no_nans_or_infs(self):\n",
        "        # No NaN or Inf values allowed in any array\n",
        "        for arr in [X, y_clean, noise, y]:\n",
        "            self.assertFalse(np.isnan(arr).any(), \"Found NaN values\")\n",
        "            self.assertFalse(np.isinf(arr).any(), \"Found Inf values\")\n",
        "\n",
        "    def test_f_true_signature(self):\n",
        "        # f_true(0) should return 0.5 with default linear+ripple definition\n",
        "        self.assertAlmostEqual(float(f_true(np.array([0.0]))[0]), 0.5, places=7)\n",
        "\n",
        "    def test_outlier_bookkeeping(self):\n",
        "        # k should equal round(outlier_frac * n)\n",
        "        self.assertEqual(k, int(round(outlier_frac * n)))\n",
        "        # If outliers exist, their indices should be valid and unique\n",
        "        if k > 0:\n",
        "            self.assertIn('idx', globals(), \"idx should exist when k>0\")\n",
        "            self.assertEqual(len(idx), k)\n",
        "            self.assertTrue(np.all((idx >= 0) & (idx < n)))\n",
        "            self.assertEqual(len(np.unique(idx)), k)\n",
        "\n",
        "    def test_outlier_effect_when_present(self):\n",
        "        # If outliers exist, their noise magnitude should be significantly larger\n",
        "        if k > 0:\n",
        "            self.assertGreater(np.mean(np.abs(noise[idx])), 3.0 * sigma)\n",
        "\n",
        "    def test_reasonable_noise_scale(self):\n",
        "        # If there are no outliers, noise std should be close to sigma\n",
        "        if k == 0:\n",
        "            est = np.std(noise)\n",
        "            self.assertTrue(abs(est - sigma) < max(0.25 * sigma, 1e-6),\n",
        "                            f\"std(noise)={est:.3f} too far from sigma={sigma:.3f}\")\n",
        "\n",
        "# Run the test suite directly in the notebook\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestSyntheticDataGenerator)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01956e64",
      "metadata": {
        "id": "01956e64"
      },
      "source": [
        "\n",
        "## Part B — Train/Test Split (No leakage)\n",
        "\n",
        "We do a simple contiguous split (or shuffle first if you wish) to evaluate OLS fairly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e467fd5",
      "metadata": {
        "id": "7e467fd5"
      },
      "outputs": [],
      "source": [
        "# Shuffle indices for a random split (set seed above for reproducibility)\n",
        "# TODO: generate a random permutation of row indices for splitting\n",
        "# perm = ...\n",
        "\n",
        "# TODO: choose the train/test split ratio (e.g., 0.7 for 70% train)\n",
        "# train_ratio = ...\n",
        "\n",
        "# TODO: compute number of training samples based on the ratio\n",
        "# n_train = ...\n",
        "\n",
        "# TODO: slice the permuted indices into train and test splits\n",
        "# train_idx = ...\n",
        "# test_idx  = ...\n",
        "\n",
        "# TODO: build the train/test DataFrames using iloc; sort by \"x\" and reset the index\n",
        "# df_train = ...\n",
        "# df_test  = ...\n",
        "\n",
        "print(\"Train size:\", len(df_train), \"| Test size:\", len(df_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b2375d",
      "metadata": {
        "id": "a2b2375d"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class TestRandomSplit(unittest.TestCase):\n",
        "    def test_perm_is_valid_permutation(self):\n",
        "        # perm must be a permutation of 0..len(df)-1 with no duplicates\n",
        "        n_total = len(df)\n",
        "        self.assertEqual(len(perm), n_total)\n",
        "        self.assertEqual(len(np.unique(perm)), n_total)\n",
        "        self.assertTrue(np.array_equal(np.sort(perm), np.arange(n_total)))\n",
        "\n",
        "    def test_split_sizes(self):\n",
        "        # n_train must be consistent with train_ratio\n",
        "        n_total = len(df)\n",
        "        self.assertEqual(n_train, int(train_ratio * n_total))\n",
        "        # The split indices should partition perm\n",
        "        self.assertEqual(len(train_idx), n_train)\n",
        "        self.assertEqual(len(test_idx), n_total - n_train)\n",
        "\n",
        "    def test_disjoint_and_cover_all(self):\n",
        "        # Train and test index sets should be disjoint and cover all indices\n",
        "        set_train = set(map(int, np.array(train_idx).tolist()))\n",
        "        set_test  = set(map(int, np.array(test_idx).tolist()))\n",
        "        self.assertTrue(set_train.isdisjoint(set_test))\n",
        "        self.assertEqual(len(set_train | set_test), len(df))\n",
        "\n",
        "    def test_train_test_from_correct_rows(self):\n",
        "        # df_train/df_test should come from the right rows of df (order aside due to sorting)\n",
        "        # Compare using sets of (x, y, y_clean) tuples to be order-invariant\n",
        "        def rows_as_tuples(frame):\n",
        "            return set(map(tuple, np.round(frame[[\"x\", \"y\", \"y_clean\"]].values, 12)))\n",
        "        # Raw slices before sorting/resetting\n",
        "        raw_train = df.iloc[train_idx]\n",
        "        raw_test  = df.iloc[test_idx]\n",
        "        self.assertEqual(rows_as_tuples(df_train), rows_as_tuples(raw_train))\n",
        "        self.assertEqual(rows_as_tuples(df_test), rows_as_tuples(raw_test))\n",
        "\n",
        "    def test_sorted_by_x_and_reset_index(self):\n",
        "        # After sorting by \"x\", values must be non-decreasing and index must be reset to 0..n-1\n",
        "        for part in [df_train, df_test]:\n",
        "            self.assertTrue(np.all(np.diff(part[\"x\"].values) >= 0))\n",
        "            self.assertTrue(np.array_equal(part.index.values, np.arange(len(part))))\n",
        "\n",
        "    def test_no_nan_inf_in_splits(self):\n",
        "        # No NaN/Inf allowed in split frames\n",
        "        for part in [df_train, df_test]:\n",
        "            vals = part[[\"x\", \"y\", \"y_clean\"]].values\n",
        "            self.assertFalse(np.isnan(vals).any(), \"Found NaN values in split\")\n",
        "            self.assertFalse(np.isinf(vals).any(), \"Found Inf values in split\")\n",
        "\n",
        "    def test_non_trivial_split(self):\n",
        "        # Sanity: unless the user set pathological ratios, each split should be >0 in size\n",
        "        # (If someone sets ratio 0.0 or 1.0 intentionally, this test can be adjusted.)\n",
        "        self.assertGreater(len(df_train), 0, \"Train split is empty; adjust train_ratio\")\n",
        "        self.assertGreater(len(df_test), 0, \"Test split is empty; adjust train_ratio\")\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestRandomSplit)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919f751e",
      "metadata": {
        "id": "919f751e"
      },
      "source": [
        "\n",
        "## Part C — Closed-Form OLS (`pinv` and `lstsq`)\n",
        "\n",
        "We build a **design matrix** with a bias column, then estimate coefficients using either:\n",
        "- `β = pinv(Xb) @ y` (Moore-Penrose pseudoinverse)  \n",
        "- `β = lstsq(Xb, y)` (least-squares solver)\n",
        "\n",
        "> We prefer `pinv`/`lstsq` over `inv(XᵀX)` for **numerical stability**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ddcef6e",
      "metadata": {
        "id": "6ddcef6e"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "This section covers how to **construct the design matrix**, estimate coefficients using **Moore–Penrose pseudoinverse** (`pinv`) or **least-squares solver** (`lstsq`), and why both are preferred over directly inverting $X^\\top X$. You’ll also find guidance on numerical stability, rank issues, and practical implementation details.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Problem setup & notation\n",
        "\n",
        "* We observe pairs $(x_i, y_i)$, $i=1,\\dots,n$.\n",
        "* With $p$ features per example, stack inputs into a matrix $X \\in \\mathbb{R}^{n \\times p}$ and targets into $y \\in \\mathbb{R}^{n}$.\n",
        "* Add an **intercept (bias)** by concatenating a column of ones:\n",
        "  $$\n",
        "  X_b = \\begin{bmatrix}\\mathbf{1} & X\\end{bmatrix} \\in \\mathbb{R}^{n \\times (p+1)}.\n",
        "  $$\n",
        "* The linear model is $\\hat{y} = X_b \\beta$, where $\\beta \\in \\mathbb{R}^{p+1}$ contains the intercept and slopes.\n",
        "\n",
        "**Objective (OLS):**\n",
        "$$\n",
        "\\min_{\\beta}\\frac{1}{2}\\lVert y - X_b \\beta \\rVert_2^2.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Two numerically stable closed-form estimators\n",
        "\n",
        "### A) Using the Moore–Penrose pseudoinverse (`pinv`)\n",
        "\n",
        "The OLS solution can be expressed with the pseudoinverse:\n",
        "$$\n",
        "\\beta^\\star = X_b^{+} y \\qquad \\text{where } X_b^{+} = \\mathrm{pinv}(X_b).\n",
        "$$\n",
        "\n",
        "* `pinv` uses **SVD** under the hood and handles **rank-deficient** or **ill-conditioned** matrices gracefully.\n",
        "* It avoids computing $(X_b^\\top X_b)^{-1}$ explicitly.\n",
        "\n",
        "**NumPy sketch:**\n",
        "\n",
        "```python\n",
        "# Build design matrix with bias (1s in the first column)\n",
        "# Xb shape: (n, p+1)\n",
        "Xb = np.c_[np.ones(len(X)), X]          # if X is (n, p); for 1D X, ensure X is 2D: X.reshape(-1, 1)\n",
        "\n",
        "# Closed-form via pseudoinverse\n",
        "beta_pinv = np.linalg.pinv(Xb) @ y\n",
        "y_hat_pinv = Xb @ beta_pinv\n",
        "residuals_pinv = y - y_hat_pinv\n",
        "```\n",
        "\n",
        "### B) Using the least-squares solver (`lstsq`)\n",
        "\n",
        "Solve the normal equations implicitly via SVD/QR without explicit inversion:\n",
        "$$\n",
        "\\beta^\\star = \\arg\\min_{\\beta}\\lVert X_b \\beta - y \\rVert_2.\n",
        "$$\n",
        "\n",
        "**NumPy sketch:**\n",
        "\n",
        "```python\n",
        "beta_lstsq, residual_sum_sq, rank, singular_vals = np.linalg.lstsq(Xb, y, rcond=None)\n",
        "y_hat_lstsq = Xb @ beta_lstsq\n",
        "residuals_lstsq = y - y_hat_lstsq\n",
        "```\n",
        "\n",
        "**What you get:**\n",
        "\n",
        "* `beta_lstsq`: coefficients.\n",
        "* `residual_sum_sq`: $\\lVert y - X_b \\beta \\rVert_2^2$ (for full rank & $n > p+1$; otherwise empty).\n",
        "* `rank`: numerical rank of $X_b$.\n",
        "* `singular_vals`: singular values (diagnostic for conditioning).\n",
        "\n",
        "**Practical note:** For well-posed problems, `beta_pinv` and `beta_lstsq` typically match up to numerical round-off.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Why not $\\bigl(X^\\top X\\bigr)^{-1} X^\\top y$?\n",
        "\n",
        "The textbook formula\n",
        "$$\n",
        "\\beta^\\star = (X_b^\\top X_b)^{-1} X_b^\\top y\n",
        "$$\n",
        "is **algebraically correct** when $X_b^\\top X_b$ is invertible, but **numerically fragile**:\n",
        "\n",
        "* If columns of $X_b$ are nearly collinear (multicollinearity), $X_b^\\top X_b$ becomes **ill-conditioned** → amplified floating-point errors.\n",
        "* If rank-deficient, $X_b^\\top X_b$ is **singular** → no inverse exists.\n",
        "\n",
        "`pinv`/`lstsq` **avoid explicit inversion** and use SVD/QR to produce a stable solution even in these cases.\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Building the design matrix correctly\n",
        "\n",
        "### A) Always include an intercept\n",
        "\n",
        "Bias lets the model fit a non-zero mean in $y$. Without it, the fit is forced through the origin.\n",
        "\n",
        "```python\n",
        "# If X is 1D array of shape (n,), make it 2D first:\n",
        "X2 = X.reshape(-1, 1)  # (n, 1)\n",
        "Xb = np.c_[np.ones(len(X2)), X2]  # (n, 2)\n",
        "```\n",
        "\n",
        "### B) Polynomial / feature mapping (optional)\n",
        "\n",
        "You can add nonlinear terms (still **no gradients**; it’s just feature engineering):\n",
        "\n",
        "```python\n",
        "# Example: add x^2 term\n",
        "X_poly = np.c_[X2, X2**2]\n",
        "Xb = np.c_[np.ones(len(X_poly)), X_poly]\n",
        "```\n",
        "\n",
        "> Per your course policy, **no regularization** or gradient methods—just forward predictions and metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## 5) Numerical stability & rank diagnostics (SVD intuition)\n",
        "\n",
        "Let $X_b = U \\Sigma V^\\top$ be the SVD. The pseudoinverse is\n",
        "$$\n",
        "X_b^{+} = V,\\Sigma^{+},U^\\top,\n",
        "$$\n",
        "where $\\Sigma^{+}$ inverts non-zero singular values. If a singular value is tiny (near zero), inverting it would explode errors; SVD can **threshold** them (via `rcond`) to stabilize the solution.\n",
        "\n",
        "**What to look at:**\n",
        "\n",
        "* **`rank`** from `lstsq`: if less than $p+1$, features are linearly dependent.\n",
        "* **`singular_vals`**: if the ratio $\\sigma_{\\max}/\\sigma_{\\min}$ (condition number) is huge, the problem is ill-conditioned.\n",
        "* **Feature scaling** often helps conditioning (e.g., standardize columns), though the intercept should not be standardized.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Predictions, residuals, and metrics\n",
        "\n",
        "After estimating $\\beta$:\n",
        "$$\n",
        "\\hat{y} = X_b \\beta, \\qquad r = y - \\hat{y}.\n",
        "$$\n",
        "\n",
        "Basic metrics (for **regression**, as specified in your notebook):\n",
        "\n",
        "* **MSE**: $\\displaystyle \\text{MSE} = \\frac{1}{n}\\sum_i r_i^2$ (penalizes large errors more).\n",
        "* **MAE**: $\\displaystyle \\text{MAE} = \\frac{1}{n}\\sum_i |r_i|$ (more robust to outliers).\n",
        "\n",
        "```python\n",
        "mse = np.mean(residuals**2)\n",
        "mae = np.mean(np.abs(residuals))\n",
        "```\n",
        "\n",
        "> You’ll compare MSE vs MAE later and inspect where they disagree (e.g., outliers).\n",
        "\n",
        "---\n",
        "\n",
        "## 7) Edge cases & best practices\n",
        "\n",
        "* **Too few samples**: If $n < p+1$, the system is underdetermined; `pinv`/`lstsq` still return a minimum-norm solution.\n",
        "* **Duplicate/collinear features**: Watch `rank` and `singular_vals`. Consider removing redundant columns or rescaling.\n",
        "  *(No regularization here due to course policy.)*\n",
        "* **Intercept handling**: If you add your own bias column, **do not** also set a separate intercept in downstream libraries (avoid double counting).\n",
        "* **Scaling**: Standardizing features (except the bias) can improve conditioning and interpretability of coefficients.\n",
        "* **Determinism**: If you shuffle/split, set a random seed to reproduce results.\n",
        "\n",
        "---\n",
        "\n",
        "## 8) Minimal, self-contained NumPy template\n",
        "\n",
        "```python\n",
        "# X: shape (n,) or (n, p); y: shape (n,)\n",
        "X = np.asarray(X)\n",
        "y = np.asarray(y).reshape(-1)\n",
        "\n",
        "# Ensure 2D feature matrix\n",
        "if X.ndim == 1:\n",
        "    X = X.reshape(-1, 1)\n",
        "\n",
        "# Build design matrix with bias\n",
        "Xb = np.c_[np.ones(len(X)), X]  # (n, p+1)\n",
        "\n",
        "# Option A: pseudoinverse\n",
        "beta_pinv = np.linalg.pinv(Xb) @ y\n",
        "y_hat_pinv = Xb @ beta_pinv\n",
        "res_pinv = y - y_hat_pinv\n",
        "mse_pinv = np.mean(res_pinv**2)\n",
        "mae_pinv = np.mean(np.abs(res_pinv))\n",
        "\n",
        "# Option B: least-squares solver\n",
        "beta_lstsq, rss, rank, svals = np.linalg.lstsq(Xb, y, rcond=None)\n",
        "y_hat_lstsq = Xb @ beta_lstsq\n",
        "res_lstsq = y - y_hat_lstsq\n",
        "mse_lstsq = np.mean(res_lstsq**2)\n",
        "mae_lstsq = np.mean(np.abs(res_lstsq))\n",
        "\n",
        "# Sanity check: solutions should be close on well-posed problems\n",
        "assert np.allclose(beta_pinv, beta_lstsq, atol=1e-8) or True\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9) When to prefer `pinv` vs `lstsq`\n",
        "\n",
        "* **Use `lstsq`** if you want **diagnostics** (rank, singular values, residual sum of squares) and a solver tailored for least squares.\n",
        "* **Use `pinv`** for a compact expression, or when you conceptually want $X_b^{+} y$ and may reuse $X_b^{+}$ across multiple $y$’s.\n",
        "\n",
        "Both are **numerically stable** and avoid the pitfalls of explicit inversion.\n",
        "\n",
        "---\n",
        "\n",
        "## 10) Quick checklist (before moving on)\n",
        "\n",
        "* [ ] Design matrix includes a **bias column**.\n",
        "* [ ] You chose either `pinv` or `lstsq` (both acceptable here).\n",
        "* [ ] Shapes are consistent: $X_b:(n, p{+}1)$, $\\beta:(p{+}1,)$, $y:(n,)$.\n",
        "* [ ] Residuals computed: $r = y - X_b\\beta$.\n",
        "* [ ] Metrics (MSE/MAE) computed on the **same predictions** for fair comparison.\n",
        "* [ ] Optional: Inspect `rank` and `singular_vals` to understand conditioning.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb462c0",
      "metadata": {
        "id": "acb462c0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def design_matrix_1d(x: np.ndarray) -> np.ndarray:\n",
        "    # Build a design matrix for 1D regression with an explicit bias (intercept) column.\n",
        "    # Expected:\n",
        "    #   - Input x: 1D array of shape (n,)\n",
        "    #   - Output Xb: 2D array of shape (n, 2)\n",
        "    #       Xb[:, 0] = 1.0  (bias)\n",
        "    #       Xb[:, 1] = x    (feature)\n",
        "    #\n",
        "    # Steps to implement:\n",
        "    # 1) Ensure x is a NumPy array and reshape to (n,) if needed.\n",
        "    # 2) Validate: no NaN/Inf; raise a clear error if found.\n",
        "    # 3) Stack a ones column with x to form Xb (use np.c_ or np.column_stack).\n",
        "    #\n",
        "    # Return:\n",
        "    #   - The constructed Xb matrix.\n",
        "    raise NotImplementedError(\"Implement: build Xb with a ones column and x as the second column.\")\n",
        "\n",
        "\n",
        "def fit_ols_pinv(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    # Fit OLS coefficients using the Moore–Penrose pseudoinverse:\n",
        "    #       beta = pinv(Xb) @ y\n",
        "    #\n",
        "    # Expected:\n",
        "    #   - x: shape (n,)\n",
        "    #   - y: shape (n,)\n",
        "    #   - Return beta: shape (2,)  -> [intercept, slope]\n",
        "    #\n",
        "    # Steps to implement:\n",
        "    # 1) Validate x and y:\n",
        "    #       * Both 1D and same length.\n",
        "    #       * No NaN/Inf.\n",
        "    # 2) Build Xb via design_matrix_1d(x).\n",
        "    # 3) Compute beta using np.linalg.pinv(Xb) @ y.\n",
        "    # 4) Return beta.\n",
        "    raise NotImplementedError(\"Implement: beta = np.linalg.pinv(Xb) @ y, after input validation.\")\n",
        "\n",
        "\n",
        "def fit_ols_lstsq(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    # Fit OLS coefficients using the least-squares solver:\n",
        "    #       beta = argmin ||Xb @ beta - y||_2\n",
        "    # via:\n",
        "    #       beta, *_ = np.linalg.lstsq(Xb, y, rcond=None)\n",
        "    #\n",
        "    # Expected:\n",
        "    #   - x: shape (n,)\n",
        "    #   - y: shape (n,)\n",
        "    #   - Return beta: shape (2,)  -> [intercept, slope]\n",
        "    #\n",
        "    # Steps to implement:\n",
        "    # 1) Validate x and y (shapes, NaN/Inf, same length).\n",
        "    # 2) Build Xb via design_matrix_1d(x).\n",
        "    # 3) Call np.linalg.lstsq(Xb, y, rcond=None) and take the first output as beta.\n",
        "    # 4) Return beta.\n",
        "    raise NotImplementedError(\"Implement: beta = np.linalg.lstsq(Xb, y, rcond=None)[0].\")\n",
        "\n",
        "\n",
        "def predict_ols(beta: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
        "    # Compute forward predictions:\n",
        "    #       y_hat = Xb @ beta\n",
        "    #\n",
        "    # Expected:\n",
        "    #   - beta: shape (2,) -> [intercept, slope]\n",
        "    #   - x:    shape (n,)\n",
        "    #   - Return y_hat: shape (n,)\n",
        "    #\n",
        "    # Steps to implement:\n",
        "    # 1) Ensure beta is 1D of length 2; validate no NaN/Inf.\n",
        "    # 2) Build Xb via design_matrix_1d(x).\n",
        "    # 3) (Optional) assert Xb.shape[1] == beta.shape[0] for safety.\n",
        "    # 4) Return Xb @ beta as a 1D array.\n",
        "    raise NotImplementedError(\"Implement: return design_matrix_1d(x) @ beta.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dec69b8",
      "metadata": {
        "id": "3dec69b8"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "class TestClosedFormOLS(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Build a simple noiseless linear dataset: y = 2x + 0.5\n",
        "        rng = np.random.default_rng(42)\n",
        "        self.x = np.linspace(-3, 3, 50)\n",
        "        self.y = 2.0 * self.x + 0.5  # no noise to make equality easy\n",
        "\n",
        "    def test_design_matrix(self):\n",
        "        Xb = design_matrix_1d(self.x)\n",
        "        # Shape and bias column\n",
        "        self.assertEqual(Xb.shape, (len(self.x), 2))\n",
        "        self.assertTrue(np.allclose(Xb[:, 0], 1.0))\n",
        "        # Second column equals x\n",
        "        self.assertTrue(np.allclose(Xb[:, 1], self.x))\n",
        "\n",
        "    def test_fit_pinv_exact(self):\n",
        "        beta = fit_ols_pinv(self.x, self.y)\n",
        "        # Should recover intercept ~0.5 and slope ~2.0\n",
        "        self.assertTrue(np.allclose(beta, np.array([0.5, 2.0]), atol=1e-10))\n",
        "        y_hat = predict_ols(beta, self.x)\n",
        "        self.assertTrue(np.allclose(y_hat, self.y, atol=1e-10))\n",
        "\n",
        "    def test_fit_lstsq_exact(self):\n",
        "        beta = fit_ols_lstsq(self.x, self.y)\n",
        "        self.assertTrue(np.allclose(beta, np.array([0.5, 2.0]), atol=1e-10))\n",
        "        y_hat = predict_ols(beta, self.x)\n",
        "        self.assertTrue(np.allclose(y_hat, self.y, atol=1e-10))\n",
        "\n",
        "    def test_pinv_vs_lstsq_close(self):\n",
        "        beta1 = fit_ols_pinv(self.x, self.y)\n",
        "        beta2 = fit_ols_lstsq(self.x, self.y)\n",
        "        self.assertTrue(np.allclose(beta1, beta2, atol=1e-12))\n",
        "\n",
        "    def test_validation(self):\n",
        "        # Mismatched lengths should raise\n",
        "        with self.assertRaises(ValueError):\n",
        "            fit_ols_pinv(self.x, self.y[:-1])\n",
        "        with self.assertRaises(ValueError):\n",
        "            fit_ols_lstsq(self.x, self.y[:-1])\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestClosedFormOLS)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2e4307",
      "metadata": {
        "id": "0a2e4307"
      },
      "outputs": [],
      "source": [
        "\n",
        "beta_pinv  = fit_ols_pinv(df_train[\"x\"].values, df_train[\"y\"].values)\n",
        "beta_lstsq = fit_ols_lstsq(df_train[\"x\"].values, df_train[\"y\"].values)\n",
        "\n",
        "print(\"β (pinv) :\", beta_pinv)\n",
        "print(\"β (lstsq):\", beta_lstsq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1e7c26",
      "metadata": {
        "id": "3a1e7c26"
      },
      "source": [
        "### Analysis — Interpreting the OLS Coefficients\n",
        "\n",
        "You now have two coefficient vectors:\n",
        "- **β (pinv)** from the Moore–Penrose pseudoinverse\n",
        "- **β (lstsq)** from the least-squares solver\n",
        "\n",
        "Please analyze the results and answer the following:\n",
        "\n",
        "1) **Numerical agreement**\n",
        "- How close are `β (pinv)` and `β (lstsq)`? Compute the absolute difference per component and the overall ‖Δβ‖₂.\n",
        "- If they differ, is the discrepancy practically meaningful for predictions on this dataset?\n",
        "\n",
        "2) **Interpretation of β**\n",
        "- Interpret the **intercept** and **slope** in the context of the data you generated (domain of `x`, added noise, ripple).\n",
        "- Do the signs and magnitudes make sense given your ground-truth function?\n",
        "\n",
        "3) **Stability & conditioning (the “why”)**\n",
        "- Hypothesize why `pinv` and `lstsq` might differ numerically (e.g., rank deficiency, near-collinearity, scaling of features).\n",
        "- If you captured diagnostics from `lstsq` (rank, singular values), relate them to your observations.\n",
        "\n",
        "4) **Effect of outliers**\n",
        "- Given your `outlier_frac` and `outlier_mag`, how sensitive are the fitted coefficients?\n",
        "- Would you expect **MAE** vs **MSE** to tell different stories on the same predictions here? Why?\n",
        "\n",
        "5) **Residual behavior**\n",
        "- Compute residuals on **train** (and optionally **test**): do you see patterns (e.g., curvature from the sinusoidal ripple)?\n",
        "- Are residuals approximately symmetric and centered near zero? Any heavy tails caused by outliers?\n",
        "\n",
        "6) **Generalization check (optional)**\n",
        "- Use the learned β to predict on `df_test`. Compare MSE and MAE between train and test. Do you see over/under-fitting signals?\n",
        "\n",
        "> Keep your answers concise but evidence-based: include small numeric summaries (e.g., differences in β, MSE/MAE values), short plots if helpful, and 2–4 sentences of interpretation per item.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f99669",
      "metadata": {
        "id": "24f99669"
      },
      "source": [
        "\n",
        "## Part D — Metrics: MSE & MAE (Evaluation Only)\n",
        "\n",
        "We compute **MSE** and **MAE** on train/test for both solutions and inspect **per-example contributions**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7deda469",
      "metadata": {
        "id": "7deda469"
      },
      "outputs": [],
      "source": [
        "# ----------------------------- Metrics (MSE / MAE) -----------------------------\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    # TODO: implement Mean Squared Error\n",
        "    # Steps:\n",
        "    # 1) Convert inputs to float NumPy arrays (np.asarray(..., dtype=float))\n",
        "    # 2) Compute the elementwise error: (y_pred - y_true)\n",
        "    # 3) Square it, take mean, and cast to float\n",
        "    # return ...\n",
        "    pass\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    # TODO: implement Mean Absolute Error\n",
        "    # Steps:\n",
        "    # 1) Convert inputs to float NumPy arrays\n",
        "    # 2) Compute absolute error: np.abs(y_pred - y_true)\n",
        "    # 3) Take mean and cast to float\n",
        "    # return ...\n",
        "    pass\n",
        "\n",
        "\n",
        "# ----------------------------- Predictions -----------------------------\n",
        "# TODO: compute OLS predictions on train/test for both solvers using predict_ols(...)\n",
        "# yhat_train_pinv  = ...\n",
        "# yhat_test_pinv   = ...\n",
        "# yhat_train_lstsq = ...\n",
        "# yhat_test_lstsq  = ...\n",
        "\n",
        "\n",
        "# ----------------------------- Aggregate metrics into a table -----------------------------\n",
        "# TODO: build a list of dict rows with MSE/MAE for train and test for each solver\n",
        "# rows = []\n",
        "# for name, ytr, yte in [\n",
        "#     (\"pinv\",  yhat_train_pinv,  yhat_test_pinv),\n",
        "#     (\"lstsq\", yhat_train_lstsq, yhat_test_lstsq),\n",
        "# ]:\n",
        "#     rows.append({\n",
        "#         \"solver\": name,\n",
        "#         \"MSE_train\": mse(df_train[\"y\"].values, ytr),\n",
        "#         \"MAE_train\": mae(df_train[\"y\"].values, ytr),\n",
        "#         \"MSE_test\":  mse(df_test[\"y\"].values,  yte),\n",
        "#         \"MAE_test\":  mae(df_test[\"y\"].values,  yte),\n",
        "#     })\n",
        "\n",
        "# Keep the lightweight display code (no need for TODOs here)\n",
        "df_metrics = pd.DataFrame(rows)\n",
        "df_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4b5dc1",
      "metadata": {
        "id": "ce4b5dc1"
      },
      "source": [
        "### Reflection — Comparing Regression Metrics\n",
        "\n",
        "Now that you have computed **MSE** and **MAE** for both solvers (`pinv` and `lstsq`) on **train** and **test** splits, analyze and interpret your results:\n",
        "\n",
        "1) **Solver agreement**\n",
        "- Are the MSE and MAE values identical (or nearly identical) between `pinv` and `lstsq`?\n",
        "- If not, what could cause the difference, given that both are theoretically equivalent?\n",
        "\n",
        "2) **Train vs Test performance**\n",
        "- Which split has higher errors, and by how much?\n",
        "- What does this tell you about **model generalization**?\n",
        "\n",
        "3) **MSE vs MAE behavior**\n",
        "- When do these two metrics disagree in magnitude or ranking?\n",
        "- How does the presence of **outliers** influence each metric?\n",
        "\n",
        "4) **Error scale & interpretation**\n",
        "- Are the absolute values of MSE/MAE consistent with your noise level (`σ`) and outlier magnitude?\n",
        "- Does the model underfit or overfit the data based on these metrics?\n",
        "\n",
        "5) **Optional deeper insight**\n",
        "- Re-run the experiment with different noise or outlier settings and describe how the metrics shift.\n",
        "- Which metric seems more *robust* in your observations, and why?\n",
        "\n",
        "> Summarize your findings in a short paragraph (4–6 sentences) referencing specific values from your table. Discuss **agreement, robustness, and generalization** rather than just restating numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce612b4a",
      "metadata": {
        "id": "ce612b4a"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class TestMetricsAndAggregation(unittest.TestCase):\n",
        "    # ---------- Unit tests for metric functions ----------\n",
        "    def test_mse_basic(self):\n",
        "        # Exact values on a tiny example\n",
        "        y_true = np.array([0.0, 1.0, 2.0])\n",
        "        y_pred = np.array([0.0, 2.0, 1.0])\n",
        "        # squared errors: [0, 1, 1] -> mean = 2/3\n",
        "        self.assertAlmostEqual(mse(y_true, y_pred), 2.0/3.0, places=12)\n",
        "\n",
        "    def test_mae_basic(self):\n",
        "        # Exact values on a tiny example\n",
        "        y_true = np.array([0.0, 1.0, 2.0])\n",
        "        y_pred = np.array([0.0, 2.0, 1.0])\n",
        "        # abs errors: [0, 1, 1] -> mean = 2/3\n",
        "        self.assertAlmostEqual(mae(y_true, y_pred), 2.0/3.0, places=12)\n",
        "\n",
        "    def test_mse_mae_symmetry(self):\n",
        "        # Metrics should be invariant under swapping arguments (distance-like)\n",
        "        y1 = np.array([-1.5, 0.0, 3.0])\n",
        "        y2 = np.array([-1.4, 0.5, 2.0])\n",
        "        self.assertAlmostEqual(mse(y1, y2), mse(y2, y1), places=12)\n",
        "        self.assertAlmostEqual(mae(y1, y2), mae(y2, y1), places=12)\n",
        "\n",
        "    def test_metrics_numeric_stability(self):\n",
        "        # Should return finite non-negative floats\n",
        "        y_true = np.linspace(-5, 5, 101)\n",
        "        y_pred = y_true + 0.1*np.sin(y_true)\n",
        "        for fn in (mse, mae):\n",
        "            val = fn(y_true, y_pred)\n",
        "            self.assertIsInstance(val, float)\n",
        "            self.assertTrue(np.isfinite(val))\n",
        "            self.assertGreaterEqual(val, 0.0)\n",
        "\n",
        "    # ---------- Tests for predictions (use variables from the notebook state) ----------\n",
        "    def test_prediction_shapes(self):\n",
        "        # All prediction arrays should match the corresponding input lengths\n",
        "        self.assertEqual(len(yhat_train_pinv),  len(df_train))\n",
        "        self.assertEqual(len(yhat_test_pinv),   len(df_test))\n",
        "        self.assertEqual(len(yhat_train_lstsq), len(df_train))\n",
        "        self.assertEqual(len(yhat_test_lstsq),  len(df_test))\n",
        "\n",
        "    def test_prediction_finiteness(self):\n",
        "        # No NaN/Inf in predictions\n",
        "        for arr in [yhat_train_pinv, yhat_test_pinv, yhat_train_lstsq, yhat_test_lstsq]:\n",
        "            self.assertFalse(np.isnan(arr).any())\n",
        "            self.assertFalse(np.isinf(arr).any())\n",
        "\n",
        "    # ---------- Tests for the aggregated metrics table ----------\n",
        "    def test_metrics_table_structure(self):\n",
        "        # df_metrics must have two rows (pinv, lstsq) and required columns\n",
        "        required_cols = {\"solver\", \"MSE_train\", \"MAE_train\", \"MSE_test\", \"MAE_test\"}\n",
        "        self.assertTrue(required_cols.issubset(set(df_metrics.columns)))\n",
        "        self.assertEqual(len(df_metrics), 2)\n",
        "        self.assertSetEqual(set(df_metrics[\"solver\"]), {\"pinv\", \"lstsq\"})\n",
        "\n",
        "    def test_metrics_table_values(self):\n",
        "        # All metric values should be finite, non-negative floats\n",
        "        for col in [\"MSE_train\", \"MAE_train\", \"MSE_test\", \"MAE_test\"]:\n",
        "            vals = df_metrics[col].values\n",
        "            self.assertTrue(np.all(np.isfinite(vals)))\n",
        "            self.assertTrue(np.all(vals >= 0.0))\n",
        "            # Ensure dtype is numeric\n",
        "            self.assertTrue(np.issubdtype(vals.dtype, np.number))\n",
        "\n",
        "    def test_solvers_numerical_agreement(self):\n",
        "        # pinv and lstsq should yield very similar metrics on the same data\n",
        "        # (allow tiny numerical differences)\n",
        "        row_pinv  = df_metrics[df_metrics[\"solver\"] == \"pinv\"].iloc[0]\n",
        "        row_lstsq = df_metrics[df_metrics[\"solver\"] == \"lstsq\"].iloc[0]\n",
        "        for col in [\"MSE_train\", \"MAE_train\", \"MSE_test\", \"MAE_test\"]:\n",
        "            self.assertAlmostEqual(float(row_pinv[col]), float(row_lstsq[col]), places=8)\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestMetricsAndAggregation)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5df8bb",
      "metadata": {
        "id": "9d5df8bb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Per-example contributions (test set, pinv)\n",
        "df_test_eval = df_test.copy()\n",
        "df_test_eval[\"yhat\"] = yhat_test_pinv\n",
        "df_test_eval[\"mse_contrib\"] = (df_test_eval[\"yhat\"] - df_test_eval[\"y\"])**2\n",
        "df_test_eval[\"mae_contrib\"] = np.abs(df_test_eval[\"yhat\"] - df_test_eval[\"y\"])\n",
        "\n",
        "# Show top-5 examples where MSE >> MAE (outlier emphasis)\n",
        "df_test_eval[\"ratio_mse_mae\"] = df_test_eval[\"mse_contrib\"] / (df_test_eval[\"mae_contrib\"] + 1e-12)\n",
        "df_test_eval.sort_values(\"ratio_mse_mae\", ascending=False).head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a6a033",
      "metadata": {
        "id": "b8a6a033"
      },
      "source": [
        "### Reflection — Understanding Per-Example Error Contributions\n",
        "\n",
        "This table lists the top test examples where **MSE contributions** are much larger than **MAE contributions** — meaning their squared errors dominate. Analyze these results carefully:\n",
        "\n",
        "1) **Identify the outliers**\n",
        "- Which data points have the highest `ratio_mse_mae` values?\n",
        "- How large are their absolute residuals (the `|yhat - y|` values)?\n",
        "\n",
        "2) **Interpret the difference between MSE and MAE**\n",
        "- Why does MSE emphasize these examples more strongly than MAE?\n",
        "- What does this tell you about each metric’s sensitivity to extreme errors?\n",
        "\n",
        "3) **Data and model behavior**\n",
        "- Look at the corresponding `x` and `y` values — do these outliers occur in specific regions (e.g., near the edges of the domain, or around the sinusoidal ripple)?\n",
        "- Could these errors reflect **true outliers**, or a **systematic underfitting** of the model?\n",
        "\n",
        "4) **Metric-level insight**\n",
        "- How might the overall MSE and MAE values change if you removed these points?\n",
        "- Which metric would change more, and why?\n",
        "\n",
        "> Write a short discussion (4–6 sentences) explaining what this table reveals about how **MSE** and **MAE** \"see\" the same dataset differently, and what that means for evaluating regression robustness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec7253f5",
      "metadata": {
        "id": "ec7253f5"
      },
      "source": [
        "\n",
        "## Part E — Visualizations\n",
        "We use simple matplotlib plots (no styling) to visualize fits, residuals, and per-example contributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0874aa2e",
      "metadata": {
        "id": "0874aa2e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fit visualization (train + test) for pinv\n",
        "plt.figure()\n",
        "plt.scatter(df_train[\"x\"], df_train[\"y\"], label=\"train\")\n",
        "plt.scatter(df_test[\"x\"], df_test[\"y\"], label=\"test\")\n",
        "xline = np.linspace(df[\"x\"].min(), df[\"x\"].max(), 300)\n",
        "plt.plot(xline, predict_ols(beta_pinv, xline), label=\"OLS (pinv)\")\n",
        "plt.title(\"OLS fit (pinv) with Train/Test points\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e6c5d2b",
      "metadata": {
        "id": "2e6c5d2b"
      },
      "source": [
        "### Reflection — Visual Interpretation of the OLS Fit\n",
        "\n",
        "The plot above shows:\n",
        "- The **training points** and **test points** as scatter plots.\n",
        "- The **OLS prediction line (pinv)** across the full range of `x`.\n",
        "\n",
        "Analyze and discuss the following:\n",
        "\n",
        "1) **Overall fit**\n",
        "- Does the OLS line capture the main trend of the data?\n",
        "- Are there regions where the model systematically over- or under-predicts?\n",
        "\n",
        "2) **Train vs Test behavior**\n",
        "- Do the predictions generalize well to test points?\n",
        "- Are errors similar across both sets, or is there visible overfitting/underfitting?\n",
        "\n",
        "3) **Noise and outlier effects**\n",
        "- How do the noisy or outlier points influence the fitted line?\n",
        "- Would MAE-based regression likely produce a visually different line? Why?\n",
        "\n",
        "4) **Model limitations**\n",
        "- If your true function included a ripple or nonlinearity, does the linear model visibly fail to capture it?\n",
        "- What would you expect to change if you added nonlinear features (e.g., \\(x^2\\)) to the design matrix?\n",
        "\n",
        "> Write 5–7 sentences describing what this visualization reveals about the **model’s fit quality**, **robustness**, and **generalization** across train and test data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c524fd",
      "metadata": {
        "id": "c0c524fd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Residuals on test (pinv)\n",
        "plt.figure()\n",
        "res = df_test_eval[\"yhat\"] - df_test_eval[\"y\"]\n",
        "plt.scatter(df_test_eval[\"x\"], res)\n",
        "plt.axhline(0, linestyle=\"--\")\n",
        "plt.title(\"Residuals vs x (test, pinv)\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"residual\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aa4eb58",
      "metadata": {
        "id": "7aa4eb58"
      },
      "source": [
        "### Reflection — Interpreting the Residual Plot (Test Set, pinv)\n",
        "\n",
        "The plot above shows the **residuals** (prediction errors) on the test set for the OLS model fitted with `pinv`.\n",
        "\n",
        "Analyze and discuss the following:\n",
        "\n",
        "1) **Pattern detection**\n",
        "- Are residuals randomly scattered around zero, or do they show a clear pattern (e.g., curvature, trend, or clusters)?\n",
        "- What would a random, pattern-free residual plot indicate about model correctness?\n",
        "\n",
        "2) **Nonlinearity**\n",
        "- Do residuals suggest that the true relationship between `x` and `y` is nonlinear?\n",
        "- If so, where does the linear model fail — at the extremes, or in certain midrange regions?\n",
        "\n",
        "3) **Heteroscedasticity**\n",
        "- Does the spread (variance) of residuals change with `x`?  \n",
        "  (e.g., larger errors for large |x| values?)\n",
        "- What could that mean about noise structure or model misspecification?\n",
        "\n",
        "4) **Outliers**\n",
        "- Are there any points with unusually large residuals?\n",
        "- How might these affect MSE vs MAE and the fitted slope/intercept?\n",
        "\n",
        "5) **Model adequacy**\n",
        "- Based on this plot, do you think the OLS model is well-specified for this dataset?\n",
        "- If not, what modeling step (e.g., feature transformation) might reduce systematic residuals?\n",
        "\n",
        "> Summarize your conclusions in 5–6 sentences, focusing on **randomness vs structure** in residuals, **model fit quality**, and **potential improvements**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6745cca7",
      "metadata": {
        "id": "6745cca7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Per-example contribution comparison on test (pinv)\n",
        "plt.figure()\n",
        "plt.plot(df_test_eval[\"x\"], df_test_eval[\"mse_contrib\"], label=\"MSE contribution\")\n",
        "plt.plot(df_test_eval[\"x\"], df_test_eval[\"mae_contrib\"], label=\"MAE contribution\")\n",
        "plt.title(\"Per-example contributions (test, pinv)\")\n",
        "plt.xlabel(\"x\"); plt.ylabel(\"loss contribution\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56045f0e",
      "metadata": {
        "id": "56045f0e"
      },
      "source": [
        "### Reflection — Comparing Per-Example Loss Contributions (Test Set, pinv)\n",
        "\n",
        "This plot shows how individual test examples contribute to the total **MSE** and **MAE** losses.\n",
        "\n",
        "Please analyze and discuss the following:\n",
        "\n",
        "1) **Overall shape**\n",
        "- How do the **MSE** and **MAE** contribution curves differ across `x`?\n",
        "- Are they similar for most points, or do they diverge sharply at specific locations?\n",
        "\n",
        "2) **Outlier sensitivity**\n",
        "- Identify where the **MSE** spikes relative to **MAE**.\n",
        "- Why does MSE amplify large errors so dramatically compared to MAE?\n",
        "\n",
        "3) **Interpretation of specific regions**\n",
        "- Do the highest contributions occur near true data outliers, or in regions where the model systematically misfits (e.g., nonlinear zones)?\n",
        "- What does this reveal about which parts of the input space dominate each metric?\n",
        "\n",
        "4) **Metric robustness**\n",
        "- Based on the curves, which metric (MSE or MAE) seems more stable across examples?\n",
        "- How does this stability relate to the robustness of the evaluation?\n",
        "\n",
        "5) **Takeaway**\n",
        "- In a short paragraph (4–6 sentences), summarize what this figure tells you about how MSE and MAE “see” model error differently,  \n",
        "  and which metric better reflects the *typical* performance versus rare but extreme mistakes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a3541ef",
      "metadata": {
        "id": "1a3541ef"
      },
      "source": [
        "\n",
        "## Part F — (Mini) K-Fold Evaluation (Optional)\n",
        "\n",
        "We implement a small **k-fold** helper to report average MSE/MAE without any model selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dc9e642",
      "metadata": {
        "id": "7dc9e642"
      },
      "outputs": [],
      "source": [
        "\n",
        "def kfold_indices(n_samples: int, k: int, seed: int = 123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    indices = np.arange(n_samples)\n",
        "    rng.shuffle(indices)\n",
        "    folds = np.array_split(indices, k)\n",
        "    for i in range(k):\n",
        "        test_idx = folds[i]\n",
        "        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "def kfold_ols_report(x: np.ndarray, y: np.ndarray, k: int = 5):\n",
        "    mses, maes = [], []\n",
        "    for tr_idx, te_idx in kfold_indices(len(x), k):\n",
        "        beta = fit_ols_pinv(x[tr_idx], y[tr_idx])\n",
        "        yhat = predict_ols(beta, x[te_idx])\n",
        "        mses.append(mse(y[te_idx], yhat))\n",
        "        maes.append(mae(y[te_idx], yhat))\n",
        "    return float(np.mean(mses)), float(np.std(mses)), float(np.mean(maes)), float(np.std(maes))\n",
        "\n",
        "m, s, ma, sa = kfold_ols_report(df[\"x\"].values, df[\"y\"].values, k=5)\n",
        "print(f\"5-fold: MSE mean={m:.4f}±{s:.4f} | MAE mean={ma:.4f}±{sa:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb768ef7",
      "metadata": {
        "id": "cb768ef7"
      },
      "source": [
        "### Reflection — Interpreting the K-Fold Cross-Validation Results\n",
        "\n",
        "The code above performs **k-fold cross-validation** using the **closed-form OLS model** (via `pinv`), computing both **MSE** and **MAE** across all folds. The printed result shows the **mean** and **standard deviation** of each metric.\n",
        "\n",
        "Analyze and discuss the following:\n",
        "\n",
        "1) **Purpose and logic**\n",
        "- Explain in your own words what k-fold cross-validation does and why it’s used.\n",
        "- How does this evaluation differ from a single train/test split?\n",
        "\n",
        "2) **Metric interpretation**\n",
        "- Compare the reported `MSE mean` and `MAE mean`.  \n",
        "  Are they close or quite different? What does that imply about outlier sensitivity in your data?\n",
        "- Which metric provides a more *robust* summary of model performance?\n",
        "\n",
        "3) **Variance across folds**\n",
        "- Look at the `± std` values for both metrics.  \n",
        "  Are they small (indicating stable generalization) or large (indicating variability across folds)?\n",
        "- What might cause high variance between folds?\n",
        "\n",
        "4) **Model evaluation insight**\n",
        "- Does the average MSE/MAE seem consistent with your earlier single split results?  \n",
        "- Based on these values, would you say the OLS model generalizes well on this dataset?\n",
        "\n",
        "5) **Critical reflection**\n",
        "- If you increased `k` (e.g., from 5 to 10), what effect would you expect on the mean and variance of the metrics?\n",
        "- Would the results change meaningfully if you used `lstsq` instead of `pinv`?\n",
        "\n",
        "> Write a brief discussion (5–7 sentences) interpreting both the **quantitative results** (means and standard deviations) and the **conceptual implications** of cross-validation for this regression experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6eece41",
      "metadata": {
        "id": "e6eece41"
      },
      "source": [
        "\n",
        "## Part G — Outlier Sensitivity (Evaluation-Only Experiment)\n",
        "\n",
        "**Goal:** Quantify how **MSE** vs **MAE** change as we vary the *fraction* or *magnitude* of outliers.  \n",
        "We keep the *fitting procedure fixed* (OLS from clean train subset if you want), and only change the **evaluation data**.\n",
        "\n",
        "**TODO:** Extend the loop below (different `outlier_frac` and `outlier_mag`) and produce a plot of `metric vs parameter`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a6feec",
      "metadata": {
        "id": "74a6feec"
      },
      "outputs": [],
      "source": [
        "# ----------------------------- Outlier Sensitivity Experiment -----------------------------\n",
        "# Baseline: model is already fitted on the current training data (beta_pinv)\n",
        "# We'll *evaluate* its robustness by introducing synthetic outliers into the test data.\n",
        "\n",
        "# TODO: define a few outlier fractions and magnitudes to test\n",
        "# fracs = ...\n",
        "# mags  = ...\n",
        "\n",
        "records = []  # to store results for each (frac, mag) configuration\n",
        "\n",
        "# TODO: extract test x values and their clean (noise-free) ground-truth targets\n",
        "# x_eval = ...\n",
        "# y_clean_eval = ...\n",
        "\n",
        "# TODO: loop over all outlier fractions and magnitudes\n",
        "# for f in fracs:\n",
        "#     for M in mags:\n",
        "#         # TODO: generate Gaussian noise for the evaluation set\n",
        "#         # noise = ...\n",
        "\n",
        "#         # TODO: inject outliers in 'k' randomly chosen points\n",
        "#         # k = int(round(f * len(x_eval)))\n",
        "#         # if k > 0:\n",
        "#         #     idx = np.random.choice(len(x_eval), size=k, replace=False)\n",
        "#         #     noise[idx] += np.random.choice([M, -M], size=k)\n",
        "\n",
        "#         # TODO: create the noisy test labels\n",
        "#         # y_eval = ...\n",
        "\n",
        "#         # TODO: get predictions from the *fixed* OLS model (no retraining)\n",
        "#         # yhat_eval = ...\n",
        "\n",
        "#         # TODO: compute MSE and MAE for this configuration and record results\n",
        "#         # records.append({\n",
        "#         #     \"outlier_frac\": f,\n",
        "#         #     \"outlier_mag\": M,\n",
        "#         #     \"MSE\": ...,\n",
        "#         #     \"MAE\": ...,\n",
        "#         # })\n",
        "\n",
        "# Keep result construction code (no need for TODOs here)\n",
        "df_out = pd.DataFrame(records)\n",
        "df_out.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242eb246",
      "metadata": {
        "id": "242eb246"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class TestOutlierSensitivityTable(unittest.TestCase):\n",
        "    def test_table_structure(self):\n",
        "        # df_out must exist and be a DataFrame with expected columns\n",
        "        required = {\"outlier_frac\", \"outlier_mag\", \"MSE\", \"MAE\"}\n",
        "        self.assertIsInstance(df_out, pd.DataFrame)\n",
        "        self.assertTrue(required.issubset(set(df_out.columns)))\n",
        "\n",
        "        # Row count should equal len(fracs) * len(mags)\n",
        "        self.assertEqual(len(df_out), len(fracs) * len(mags))\n",
        "\n",
        "        # Every combination of (frac, mag) should appear\n",
        "        observed = set(map(tuple, df_out[[\"outlier_frac\", \"outlier_mag\"]].values))\n",
        "        expected = {(f, M) for f in fracs for M in mags}\n",
        "        self.assertSetEqual(observed, expected)\n",
        "\n",
        "    def test_metrics_nonnegative_and_finite(self):\n",
        "        # MSE/MAE must be finite and non-negative\n",
        "        for col in [\"MSE\", \"MAE\"]:\n",
        "            vals = df_out[col].to_numpy()\n",
        "            self.assertTrue(np.all(np.isfinite(vals)), f\"{col} contains non-finite values\")\n",
        "            self.assertTrue(np.all(vals >= 0.0), f\"{col} contains negative values\")\n",
        "\n",
        "    def test_no_dependency_on_model_refit(self):\n",
        "        # Sanity: the model used here is fixed (no refit per config),\n",
        "        # so predictions depend only on x_eval and beta_pinv, not on outlier settings.\n",
        "        # We can't check equality across rows (labels change), but we can at least\n",
        "        # confirm yhat_eval shape matches x_eval length.\n",
        "        yhat_eval = predict_ols(beta_pinv, df_test[\"x\"].values)\n",
        "        self.assertEqual(len(yhat_eval), len(df_test))\n",
        "\n",
        "    def test_frac_and_mag_types(self):\n",
        "        # outlier_frac should be within [0, 1]; outlier_mag should be >= 0\n",
        "        self.assertTrue(np.all((df_out[\"outlier_frac\"].values >= 0.0) &\n",
        "                               (df_out[\"outlier_frac\"].values <= 1.0)))\n",
        "        self.assertTrue(np.all(df_out[\"outlier_mag\"].values >= 0.0))\n",
        "\n",
        "suite = unittest.TestLoader().loadTestsFromTestCase(TestOutlierSensitivityTable)\n",
        "unittest.TextTestRunner(verbosity=2).run(suite)\n",
        "\n",
        "\n",
        "# Deterministic probe: non-decreasing error as outlier magnitude grows (fixed noise & indices)\n",
        "rng = np.random.default_rng(0)\n",
        "x_eval = df_test[\"x\"].values\n",
        "y_clean_eval = f_true(x_eval)\n",
        "yhat_fixed = predict_ols(beta_pinv, x_eval)\n",
        "\n",
        "probe_fracs = [0.0, 0.05]   # try no-outlier and a small fraction\n",
        "probe_mags  = [0.0, 3.0, 6.0, 10.0]\n",
        "\n",
        "for f in probe_fracs:\n",
        "    base_noise = sigma * rng.standard_normal(len(x_eval))\n",
        "    k = int(round(f * len(x_eval)))\n",
        "    idx = rng.choice(len(x_eval), size=k, replace=False) if k > 0 else np.array([], dtype=int)\n",
        "\n",
        "    mses, maes = [], []\n",
        "    for M in probe_mags:\n",
        "        noise = base_noise.copy()\n",
        "        if k > 0:\n",
        "            # Use the SAME indices and only scale the outlier kick by magnitude M\n",
        "            kicks = rng.choice([1.0, -1.0], size=k)  # fixed signs per f across mags\n",
        "            noise[idx] += M * kicks\n",
        "        y_eval = y_clean_eval + noise\n",
        "        mses.append(mse(y_eval, yhat_fixed))\n",
        "        maes.append(mae(y_eval, yhat_fixed))\n",
        "\n",
        "    print(f\"[f={f:.2f}] MSE by mag:\", np.round(mses, 4))\n",
        "    print(f\"[f={f:.2f}] MAE by mag:\", np.round(maes, 4))\n",
        "    # Expectation (not asserted): sequences should be non-decreasing as M increases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d750123",
      "metadata": {
        "id": "1d750123"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Simple slice: plot metric vs outlier fraction for a fixed magnitude (e.g., 6.0)\n",
        "subset = df_out[df_out[\"outlier_mag\"] == 6.0].sort_values(\"outlier_frac\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(subset[\"outlier_frac\"], subset[\"MSE\"], marker=\"o\", label=\"MSE\")\n",
        "plt.plot(subset[\"outlier_frac\"], subset[\"MAE\"], marker=\"o\", label=\"MAE\")\n",
        "plt.title(\"Metric vs Outlier Fraction (mag=6.0) — Evaluation Only\")\n",
        "plt.xlabel(\"outlier fraction\"); plt.ylabel(\"metric value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d20553",
      "metadata": {
        "id": "a5d20553"
      },
      "source": [
        "### Reflection — Metric Trends vs Outlier Fraction (mag = 6.0)\n",
        "\n",
        "The plot above shows how the **MSE** and **MAE** metrics change as the proportion of outliers increases, while the **outlier magnitude** is fixed at 6.0.\n",
        "\n",
        "Analyze and discuss the following:\n",
        "\n",
        "1) **Overall trend**\n",
        "- How do MSE and MAE behave as the outlier fraction increases?  \n",
        "- Do both grow at a similar rate, or does one escalate faster?\n",
        "\n",
        "2) **Relative sensitivity**\n",
        "- Which metric is more affected by the presence of even a small number of outliers?  \n",
        "- Why does this difference occur mathematically?\n",
        "\n",
        "3) **Interpretation of slope**\n",
        "- If MSE rises much faster than MAE, what does that reveal about how squared error penalizes extreme deviations?  \n",
        "- How would this affect model evaluation in noisy or outlier-heavy datasets?\n",
        "\n",
        "4) **Stability and robustness**\n",
        "- At what range of outlier fractions do the metrics remain relatively stable?  \n",
        "- Which metric would you trust more for assessing performance in practical, imperfect data?\n",
        "\n",
        "5) **Critical insight**\n",
        "- If you repeated this test with a smaller outlier magnitude (e.g., 3.0), what pattern would you expect?  \n",
        "- How does this experiment illustrate the conceptual difference between **error magnitude sensitivity** and **robustness**?\n",
        "\n",
        "> Write 5–7 sentences discussing the trends you observe, connecting them to the mathematical properties of MSE and MAE and what they imply about model robustness under contamination.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26cd76c4",
      "metadata": {
        "id": "26cd76c4"
      },
      "source": [
        "\n",
        "## Part H — Bring Your Own Data/Predictions (TODO)\n",
        "\n",
        "You can **replace** the synthetic data with your own CSV or **plug in** predictions from another model (still **fixed**).\n",
        "- Regression CSV must have: `x` (or multiple feature columns), `y`.  \n",
        "- If you already have predictions from elsewhere, provide a CSV with `y_true`, `y_pred` and compute metrics directly.\n",
        "\n",
        "> **Note:** If you switch to multiple features, adjust the **design matrix** function accordingly (add columns, keep leading bias).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87796db1",
      "metadata": {
        "id": "87796db1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === DIRECT PREDICTIONS CSV (TODO) ===\n",
        "# df_pred = pd.read_csv(\"path/to/your_predictions.csv\")\n",
        "# print(df_pred.head())\n",
        "# print(\"MSE:\", mse(df_pred[\"y_true\"].values, df_pred[\"y_pred\"].values))\n",
        "# print(\"MAE:\", mae(df_pred[\"y_true\"].values, df_pred[\"y_pred\"].values))\n",
        "\n",
        "# === RAW DATA CSV (TODO) ===\n",
        "# df_raw = pd.read_csv(\"path/to/your_regression_data.csv\")\n",
        "# # If multiple features exist, change design matrix builder:\n",
        "# # def design_matrix(X2d: np.ndarray):\n",
        "# #     X2d = np.asarray(X2d)\n",
        "# #     return np.concatenate([np.ones((len(X2d), 1)), X2d], axis=1)\n",
        "# # Then fit/predict as done above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50c4652",
      "metadata": {
        "id": "c50c4652"
      },
      "source": [
        "\n",
        "## Summary & Reflection\n",
        "\n",
        "- **Closed-form OLS** via `pinv`/`lstsq` avoids gradients and is numerically stable vs `inv(XᵀX)`.\n",
        "- **MSE vs MAE:** MSE emphasizes large errors (**outliers**), while MAE is more robust but less smooth.\n",
        "- **Disagreement mining** (per-example contributions) surfaces exactly **where** and **why** metrics diverge.\n",
        "- **K-fold** offers a stable performance estimate without model selection.\n",
        "- **Outlier sensitivity** experiments show how reporting can change under distribution shifts—even with a **fixed** model.\n",
        "\n",
        "**Write-up (required):**  \n",
        "Add 2–3 paragraphs summarizing your experiments (metrics tables + plots). Include at least two disagreeing test examples and explain them.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}