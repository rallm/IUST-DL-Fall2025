{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6Ep5tFkFg3Mazn4RJRTkw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rallm/IUST-DL-Fall2025/blob/main/HW3/helper/p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7sYO7oIWjnLw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Dataset Initialization ---\n",
        "# Data points based on your request\n",
        "x1_data = np.array([1, 2, 0, -1])\n",
        "x2_data = np.array([-1, 0, 2, 1])\n",
        "y_data = np.array([10, 13, 11, 4])\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "beta = 0.9\n",
        "batch_size = 2\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def predict(w1, w2, w3, b, x1, x2):\n",
        "    \"\"\"\n",
        "    Computes the model output: y = w1*x1^2 + w2*x2^2 + w3*x1*x2 + b\n",
        "    \"\"\"\n",
        "    return w1 * (x1**2) + w2 * (x2**2) + w3 * (x1 * x2) + b\n",
        "\n",
        "def compute_gradients(w1, w2, w3, b, x1_batch, x2_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Computes the gradients for MSE loss.\n",
        "    \"\"\"\n",
        "    n = len(y_batch)\n",
        "    y_pred = predict(w1, w2, w3, b, x1_batch, x2_batch)\n",
        "    errors = y_pred - y_batch\n",
        "\n",
        "    # MSE Derivative: (2/n) * sum(error * derivative_of_term)\n",
        "    grad_w1 = (2/n) * np.sum(errors * (x1_batch**2))\n",
        "    grad_w2 = (2/n) * np.sum(errors * (x2_batch**2))\n",
        "    grad_w3 = (2/n) * np.sum(errors * (x1_batch * x2_batch))\n",
        "    grad_b  = (2/n) * np.sum(errors * 1)\n",
        "\n",
        "    return grad_w1, grad_w2, grad_w3, grad_b, np.mean(errors**2) # returning MSE as well"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting SGD Optimization ---\")\n",
        "\n",
        "# Reset weights to initial values\n",
        "w1, w2, w3, b = 1.0, -1.0, -1.0, 1.0\n",
        "\n",
        "# Number of batches\n",
        "num_batches = len(y_data) // batch_size\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Slice the data for the current batch\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "\n",
        "    x1_batch = x1_data[start_idx:end_idx]\n",
        "    x2_batch = x2_data[start_idx:end_idx]\n",
        "    y_batch = y_data[start_idx:end_idx]\n",
        "\n",
        "    # Compute Gradients\n",
        "    gw1, gw2, gw3, gb, loss = compute_gradients(w1, w2, w3, b, x1_batch, x2_batch, y_batch)\n",
        "\n",
        "    print(f\"\\nBatch {i+1}:\")\n",
        "    print(f\"  Samples used (indices): {start_idx} to {end_idx-1}\")\n",
        "    print(f\"  Gradients -> w1: {gw1:.4f}, w2: {gw2:.4f}, w3: {gw3:.4f}, b: {gb:.4f}\")\n",
        "\n",
        "    # Update Weights (SGD Rule: w = w - lr * grad)\n",
        "    w1 = w1 - learning_rate * gw1\n",
        "    w2 = w2 - learning_rate * gw2\n",
        "    w3 = w3 - learning_rate * gw3\n",
        "    b  = b  - learning_rate * gb\n",
        "\n",
        "    print(f\"  New Weights -> w1: {w1:.4f}, w2: {w2:.4f}, w3: {w3:.4f}, b: {b:.4f}\")\n",
        "\n",
        "print(\"\\n--- Final Weights (SGD) ---\")\n",
        "print(f\"w1: {w1:.4f}, w2: {w2:.4f}, w3: {w3:.4f}, b: {b:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R3WBMLSk8lK",
        "outputId": "dd90aaea-538f-4269-b437-3ed5e89bd0f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting SGD Optimization ---\n",
            "\n",
            "Batch 1:\n",
            "  Samples used (indices): 0 to 1\n",
            "  Gradients -> w1: -40.0000, w2: -8.0000, w3: 8.0000, b: -16.0000\n",
            "  New Weights -> w1: 5.0000, w2: -0.2000, w3: -1.8000, b: 2.6000\n",
            "\n",
            "Batch 2:\n",
            "  Samples used (indices): 2 to 3\n",
            "  Gradients -> w1: 5.2000, w2: -31.6000, w3: -5.2000, b: -4.0000\n",
            "  New Weights -> w1: 4.4800, w2: 2.9600, w3: -1.2800, b: 3.0000\n",
            "\n",
            "--- Final Weights (SGD) ---\n",
            "w1: 4.4800, w2: 2.9600, w3: -1.2800, b: 3.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Starting SGD + Momentum Optimization ---\")\n",
        "\n",
        "# Reset weights to initial values\n",
        "w1, w2, w3, b = 1.0, -1.0, -1.0, 1.0\n",
        "\n",
        "# Initialize velocities (v) to 0\n",
        "v_w1, v_w2, v_w3, v_b = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Slice the data for the current batch\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "\n",
        "    x1_batch = x1_data[start_idx:end_idx]\n",
        "    x2_batch = x2_data[start_idx:end_idx]\n",
        "    y_batch = y_data[start_idx:end_idx]\n",
        "\n",
        "    # Compute Gradients\n",
        "    gw1, gw2, gw3, gb, loss = compute_gradients(w1, w2, w3, b, x1_batch, x2_batch, y_batch)\n",
        "\n",
        "    print(f\"\\nBatch {i+1}:\")\n",
        "    print(f\"  Gradients -> w1: {gw1:.4f}, w2: {gw2:.4f}, w3: {gw3:.4f}, b: {gb:.4f}\")\n",
        "\n",
        "    # Update Velocities (EMA Formula: v = beta * v + (1-beta) * grad)\n",
        "    v_w1 = beta * v_w1 + (1 - beta) * gw1\n",
        "    v_w2 = beta * v_w2 + (1 - beta) * gw2\n",
        "    v_w3 = beta * v_w3 + (1 - beta) * gw3\n",
        "    v_b  = beta * v_b  + (1 - beta) * gb\n",
        "\n",
        "    print(f\"  Velocities -> v_w1: {v_w1:.4f}, v_w2: {v_w2:.4f}, v_w3: {v_w3:.4f}, v_b: {v_b:.4f}\")\n",
        "\n",
        "    # Update Weights (Momentum Rule: w = w - lr * v)\n",
        "    w1 = w1 - learning_rate * v_w1\n",
        "    w2 = w2 - learning_rate * v_w2\n",
        "    w3 = w3 - learning_rate * v_w3\n",
        "    b  = b  - learning_rate * v_b\n",
        "\n",
        "    print(f\"  New Weights -> w1: {w1:.4f}, w2: {w2:.4f}, w3: {w3:.4f}, b: {b:.4f}\")\n",
        "\n",
        "print(\"\\n--- Final Weights (SGD + Momentum) ---\")\n",
        "print(f\"w1: {w1:.4f}, w2: {w2:.4f}, w3: {w3:.4f}, b: {b:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCKLzhDzlEBo",
        "outputId": "f66f0a40-ecda-4af9-d7b4-08dea85189c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting SGD + Momentum Optimization ---\n",
            "\n",
            "Batch 1:\n",
            "  Gradients -> w1: -40.0000, w2: -8.0000, w3: 8.0000, b: -16.0000\n",
            "  Velocities -> v_w1: -4.0000, v_w2: -0.8000, v_w3: 0.8000, v_b: -1.6000\n",
            "  New Weights -> w1: 1.4000, w2: -0.9200, w3: -1.0800, b: 1.1600\n",
            "\n",
            "Batch 2:\n",
            "  Gradients -> w1: -1.2800, w2: -55.3600, w3: 1.2800, b: -14.8000\n",
            "  Velocities -> v_w1: -3.7280, v_w2: -6.2560, v_w3: 0.8480, v_b: -2.9200\n",
            "  New Weights -> w1: 1.7728, w2: -0.2944, w3: -1.1648, b: 1.4520\n",
            "\n",
            "--- Final Weights (SGD + Momentum) ---\n",
            "w1: 1.7728, w2: -0.2944, w3: -1.1648, b: 1.4520\n"
          ]
        }
      ]
    }
  ]
}