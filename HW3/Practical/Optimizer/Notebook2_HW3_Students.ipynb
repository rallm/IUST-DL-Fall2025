{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Optimization Assignment - Part 2\n",
        "\n",
        "Adaptive Learning Rate Optimizers: Adagrad, RMSProp, Adam\n",
        "\n",
        "Students: Complete the sections marked with TODO comments"
      ],
      "metadata": {
        "id": "4F5rJswTbZB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw73Pb11bIfw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.decomposition import PCA\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. DATA LOADING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "AYLE-8zkbtpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_fashion_mnist(batch_size=128):\n",
        "    \"\"\"\n",
        "    Load Fashion-MNIST dataset\n",
        "\n",
        "    Args:\n",
        "        batch_size: Batch size for training\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    ########## TODO ##########\n",
        "    trainset = ...\n",
        "    testset = ...\n",
        "\n",
        "    trainloader = ...\n",
        "    testloader = ...\n",
        "    ######### End TODO #########\n",
        "\n",
        "    print(f\"Training samples: {len(trainset)}\")\n",
        "    print(f\"Test samples: {len(testset)}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Batches per epoch: {len(trainloader)}\")\n",
        "\n",
        "    return trainloader, testloader\n",
        "\n",
        "# Fashion-MNIST classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n"
      ],
      "metadata": {
        "id": "DLdChtQHbwkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MODEL DEFINITION"
      ],
      "metadata": {
        "id": "npruXdbBy1DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Multi-Layer Perceptron for Fashion-MNIST\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size=784, hidden_sizes=[512, 256, 128, 64], num_classes=10):\n",
        "        super(DeepMLP, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Hidden layers\n",
        "        ...\n",
        "\n",
        "        # Output layer\n",
        "        ...\n",
        "\n",
        "        self.network = ...\n",
        "        ######### End TODO #########\n",
        "        self.layer_names = [f\"layer_{i}\" for i in range(len(hidden_sizes) + 1)]\n",
        "\n",
        "    def forward(self, x):\n",
        "      ########## TODO ##########\n",
        "        ...\n",
        "      ######### End TODO #########\n",
        "\n",
        "    def get_layer_params(self):\n",
        "        \"\"\"Get parameters grouped by layer for visualization\"\"\"\n",
        "        layer_params = {}\n",
        "        for name, param in self.named_parameters():\n",
        "            layer_name = name.split('.')[1] if '.' in name else name\n",
        "            if layer_name not in layer_params:\n",
        "                layer_params[layer_name] = []\n",
        "            layer_params[layer_name].append(param)\n",
        "        return layer_params\n"
      ],
      "metadata": {
        "id": "fmfQhvXly2IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ADAPTIVE OPTIMIZER IMPLEMENTATIONS"
      ],
      "metadata": {
        "id": "WYJiyFWhy6A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdagradOptimizer:\n",
        "    \"\"\"\n",
        "    Adagrad: Adaptive Gradient Algorithm\n",
        "    Adapts learning rate based on historical gradient information\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01, eps=1e-10, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params: Model parameters\n",
        "            lr: Initial learning rate\n",
        "            eps: Small constant for numerical stability\n",
        "            weight_decay: L2 regularization factor\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Initialize sum of squared gradients\n",
        "        self.sum_squared_grads = ...\n",
        "\n",
        "        # Track effective learning rates for visualization\n",
        "        self.effective_lrs = []\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Clear gradients\"\"\"\n",
        "        ########## TODO ##########\n",
        "        ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Perform Adagrad optimization step\"\"\"\n",
        "        with torch.no_grad():\n",
        "            layer_lrs = []\n",
        "\n",
        "            for i, p in enumerate(self.params):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                ########## TODO ##########\n",
        "                # Implement Adagrad update\n",
        "                # 1. Get gradient\n",
        "                # 2. Apply weight decay if needed\n",
        "                # 3. Accumulate squared gradients\n",
        "                # 4. Compute adaptive learning rate\n",
        "                # 5. Update parameter\n",
        "\n",
        "                d_p = ...\n",
        "\n",
        "                if self.weight_decay != 0:\n",
        "                    ...\n",
        "\n",
        "                # Accumulate squared gradients\n",
        "                ...\n",
        "\n",
        "                # Compute adaptive learning rate\n",
        "                ...\n",
        "\n",
        "                # Update parameters\n",
        "                p.add_(...)\n",
        "\n",
        "                ######### End TODO #########\n",
        "\n",
        "                # Track average effective LR for this layer\n",
        "                layer_lrs.append(adapted_lr.mean().item())\n",
        "\n",
        "            self.effective_lrs.append(np.mean(layer_lrs) if layer_lrs else 0)\n",
        "\n",
        "class RMSPropOptimizer:\n",
        "    \"\"\"\n",
        "    RMSProp: Root Mean Square Propagation\n",
        "    Uses moving average of squared gradients\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01, alpha=0.9, eps=1e-8, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params: Model parameters\n",
        "            lr: Learning rate\n",
        "            alpha: Smoothing constant (decay rate)\n",
        "            eps: Small constant for numerical stability\n",
        "            weight_decay: L2 regularization factor\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Initialize moving average of squared gradients\n",
        "        self.sq_avg = ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        # Track effective learning rates\n",
        "        self.effective_lrs = []\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Clear gradients\"\"\"\n",
        "        ########## TODO ##########\n",
        "        ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Perform RMSProp optimization step\"\"\"\n",
        "        with torch.no_grad():\n",
        "            layer_lrs = []\n",
        "\n",
        "            for i, p in enumerate(self.params):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                ########## TODO ##########\n",
        "                # Implement RMSProp update\n",
        "                # 1. Get gradient\n",
        "                # 2. Apply weight decay if needed\n",
        "                # 3. Update moving average of squared gradients\n",
        "                # 4. Compute adaptive learning rate\n",
        "                # 5. Update parameter\n",
        "\n",
        "                d_p = ...\n",
        "\n",
        "                if self.weight_decay != 0:\n",
        "                    ...\n",
        "\n",
        "                # Update moving average of squared gradients\n",
        "                ...\n",
        "\n",
        "                # Compute adaptive learning rate\n",
        "                ...\n",
        "\n",
        "                # Update parameters\n",
        "                p.add_(...)\n",
        "\n",
        "                ######### End TODO #########\n",
        "\n",
        "                layer_lrs.append(adapted_lr.mean().item())\n",
        "\n",
        "            self.effective_lrs.append(np.mean(layer_lrs) if layer_lrs else 0)\n",
        "\n",
        "class AdamOptimizer:\n",
        "    \"\"\"\n",
        "    Adam: Adaptive Moment Estimation\n",
        "    Combines momentum and RMSProp\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params: Model parameters\n",
        "            lr: Learning rate\n",
        "            beta1: Exponential decay rate for first moment (momentum)\n",
        "            beta2: Exponential decay rate for second moment (RMSProp)\n",
        "            eps: Small constant for numerical stability\n",
        "            weight_decay: L2 regularization factor\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Initialize first moment (momentum) and second moment (RMSProp)\n",
        "        self.m = ...\n",
        "        self.v = ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        # Time step\n",
        "        self.t = 0\n",
        "\n",
        "        # Track effective learning rates\n",
        "        self.effective_lrs = []\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Clear gradients\"\"\"\n",
        "        ########## TODO ##########\n",
        "        ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Perform Adam optimization step\"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            layer_lrs = []\n",
        "\n",
        "            for i, p in enumerate(self.params):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                ########## TODO ##########\n",
        "                # Implement Adam update\n",
        "                # 1. Get gradient\n",
        "                # 2. Apply weight decay if needed\n",
        "                # 3. Update biased first moment (momentum)\n",
        "                # 4. Update biased second moment (RMSProp)\n",
        "                # 5. Compute bias-corrected first moment\n",
        "                # 6. Compute bias-corrected second moment\n",
        "                # 7. Compute adaptive learning rate and update\n",
        "\n",
        "                d_p = ...\n",
        "\n",
        "                if self.weight_decay != 0:\n",
        "                    ...\n",
        "\n",
        "                # Update biased first moment\n",
        "                ...\n",
        "\n",
        "                # Update biased second moment\n",
        "                ...\n",
        "\n",
        "                # Bias correction\n",
        "                ...\n",
        "\n",
        "                # Compute adaptive learning rate\n",
        "                ...\n",
        "\n",
        "                # Update parameters\n",
        "                p.add_(...)\n",
        "\n",
        "                ######### End TODO #########\n",
        "\n",
        "                layer_lrs.append(adapted_lr.mean().item())\n",
        "\n",
        "            self.effective_lrs.append(np.mean(layer_lrs) if layer_lrs else 0)\n"
      ],
      "metadata": {
        "id": "iIUmpYTLy8au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. TRAINING AND EVALUATION FUNCTIONS"
      ],
      "metadata": {
        "id": "W_otA-3szAnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, trainloader, optimizer, criterion, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Track gradient and parameter statistics\n",
        "    grad_means = []\n",
        "    grad_vars = []\n",
        "    param_update_norms = []\n",
        "\n",
        "    for inputs, targets in trainloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Save old parameters for update magnitude calculation\n",
        "        old_params = ...\n",
        "\n",
        "        # Implement training step\n",
        "        # 1. Zero gradients\n",
        "        # 2. Forward pass\n",
        "        # 3. Compute loss\n",
        "        # 4. Backward pass\n",
        "        # 5. Optimization step\n",
        "\n",
        "        ...\n",
        "\n",
        "        ######### End TODO #########\n",
        "\n",
        "        # Calculate gradient statistics\n",
        "        grads = [p.grad.view(-1) for p in model.parameters() if p.grad is not None]\n",
        "        all_grads = torch.cat(grads)\n",
        "        grad_means.append(all_grads.mean().item())\n",
        "        grad_vars.append(all_grads.var().item())\n",
        "\n",
        "        # Calculate parameter update magnitudes\n",
        "        update_norm = 0.0\n",
        "        for old_p, new_p in zip(old_params, model.parameters()):\n",
        "            update_norm += (new_p - old_p).norm().item() ** 2\n",
        "        param_update_norms.append(update_norm ** 0.5)\n",
        "\n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    stats = {\n",
        "        'loss': epoch_loss,\n",
        "        'acc': epoch_acc,\n",
        "        'grad_mean': np.mean(grad_means),\n",
        "        'grad_var': np.mean(grad_vars),\n",
        "        'param_update_norm': np.mean(param_update_norms)\n",
        "    }\n",
        "\n",
        "    return stats\n",
        "\n",
        "def evaluate(model, testloader, criterion, device):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Per-class accuracy\n",
        "    class_correct = [0] * 10\n",
        "    class_total = [0] * 10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            ########## TODO ##########\n",
        "            # Implement evaluation\n",
        "            # 1. Forward pass\n",
        "            # 2. Compute loss\n",
        "\n",
        "            ...\n",
        "\n",
        "            ######### End TODO #########\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Per-class accuracy\n",
        "            c = (predicted == targets)\n",
        "            for i in range(len(targets)):\n",
        "                label = targets[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    test_loss = running_loss / len(testloader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    per_class_acc = [100. * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "                     for i in range(10)]\n",
        "\n",
        "    return test_loss, test_acc, per_class_acc\n",
        "\n",
        "def train_model(model, trainloader, testloader, optimizer, optimizer_name,\n",
        "                num_epochs, checkpoint_epochs, device):\n",
        "    \"\"\"\n",
        "    Complete training pipeline with detailed tracking\n",
        "    \"\"\"\n",
        "    ########## TODO ##########\n",
        "    criterion = ...\n",
        "\n",
        "    # History tracking\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'grad_means': [],\n",
        "        'grad_vars': [],\n",
        "        'param_update_norms': [],\n",
        "        'effective_lrs': [],\n",
        "        'time_per_epoch': [],\n",
        "        'checkpoints': {}\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTraining with {optimizer_name}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train\n",
        "        ...\n",
        "\n",
        "        # Evaluate\n",
        "        ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_stats['loss'])\n",
        "        history['train_acc'].append(train_stats['acc'])\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        history['grad_means'].append(train_stats['grad_mean'])\n",
        "        history['grad_vars'].append(train_stats['grad_var'])\n",
        "        history['param_update_norms'].append(train_stats['param_update_norm'])\n",
        "        history['time_per_epoch'].append(epoch_time)\n",
        "\n",
        "        # Track effective learning rates (if available)\n",
        "        if hasattr(optimizer, 'effective_lrs') and optimizer.effective_lrs:\n",
        "            history['effective_lrs'].append(optimizer.effective_lrs[-1])\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch in checkpoint_epochs:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'train_loss': train_stats['loss'],\n",
        "                'train_acc': train_stats['acc'],\n",
        "                'test_loss': test_loss,\n",
        "                'test_acc': test_acc,\n",
        "                'per_class_acc': per_class_acc\n",
        "            }\n",
        "            history['checkpoints'][epoch] = checkpoint\n",
        "            print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n",
        "                  f\"Train Loss: {train_stats['loss']:.4f} | Train Acc: {train_stats['acc']:.2f}% | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
        "                  f\"Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "Uw23BfsAzCua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. VISUALIZATION FUNCTIONS"
      ],
      "metadata": {
        "id": "49FJA1rUzEYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(histories, title=\"Training Curves\"):\n",
        "    \"\"\"Plot comprehensive training curves\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        axes[0, 0].plot(history['train_loss'], label=name, linewidth=2)\n",
        "        axes[0, 1].plot(history['test_loss'], label=name, linewidth=2)\n",
        "        axes[1, 0].plot(history['train_acc'], label=name, linewidth=2)\n",
        "        axes[1, 1].plot(history['test_acc'], label=name, linewidth=2)\n",
        "\n",
        "    axes[0, 0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].set_title('Test Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].set_title('Training Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].set_title('Test Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_gradient_statistics(histories, title=\"Gradient Statistics\"):\n",
        "    \"\"\"Plot gradient mean and variance over time\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Plot gradient statistics\n",
        "    # 1. Gradient means over time (axes[0])\n",
        "    # 2. Gradient variances over time (axes[1])\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        ...\n",
        "    ######### End TODO ##########\n",
        "\n",
        "    axes[0].set_title('Gradient Mean Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Mean Gradient')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1].set_title('Gradient Variance Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Gradient Variance')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_effective_learning_rates(histories, title=\"Effective Learning Rates\"):\n",
        "    \"\"\"Plot how learning rates adapt over time\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Plot effective learning rates\n",
        "    # For each optimizer, plot how the effective LR changes over epochs\n",
        "\n",
        "    ...\n",
        "\n",
        "    ######### End TODO #########\n",
        "\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Effective Learning Rate', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.yscale('log')  # Log scale for better visualization\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_parameter_update_magnitudes(histories, title=\"Parameter Update Magnitudes\"):\n",
        "    \"\"\"Plot the magnitude of parameter updates over time\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Plot parameter update norms\n",
        "    # Shows how aggressively each optimizer updates parameters\n",
        "\n",
        "    ...\n",
        "\n",
        "    ######### End TODO #########\n",
        "\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Update Norm', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_convergence_speed(histories, threshold=90):\n",
        "    \"\"\"\n",
        "    Plot epochs to reach accuracy threshold for each optimizer\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    names = []\n",
        "    epochs_to_threshold = []\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        names.append(name)\n",
        "        # Find first epoch reaching threshold\n",
        "        for i, acc in enumerate(history['test_acc']):\n",
        "            if acc >= threshold:\n",
        "                epochs_to_threshold.append(i + 1)\n",
        "                break\n",
        "        else:\n",
        "            epochs_to_threshold.append(len(history['test_acc']))\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(names)))\n",
        "    ax.bar(names, epochs_to_threshold, color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    ax.set_title(f'Convergence Speed (Epochs to {threshold}% Accuracy)',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Epochs', fontsize=12)\n",
        "    ax.set_xlabel('Optimizer', fontsize=12)\n",
        "    ax.grid(True, alpha=0.3, axis='y')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_lr_robustness(all_histories, optimizer_name):\n",
        "    \"\"\"\n",
        "    Plot robustness to different initial learning rates\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Filter histories for specific optimizer\n",
        "    opt_histories = {k: v for k, v in all_histories.items() if optimizer_name in k}\n",
        "\n",
        "    # Accuracy curves\n",
        "    for name, history in opt_histories.items():\n",
        "        lr = name.split('_LR')[-1]\n",
        "        axes[0].plot(history['test_acc'], label=f'LR={lr}', linewidth=2)\n",
        "\n",
        "    axes[0].set_title(f'{optimizer_name}: Different Learning Rates',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Test Accuracy (%)')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Final accuracy vs LR\n",
        "    lrs = []\n",
        "    final_accs = []\n",
        "    for name, history in opt_histories.items():\n",
        "        lr = float(name.split('_LR')[-1])\n",
        "        lrs.append(lr)\n",
        "        final_accs.append(history['test_acc'][-1])\n",
        "\n",
        "    axes[1].plot(lrs, final_accs, 'o-', markersize=10, linewidth=2)\n",
        "    axes[1].set_title(f'{optimizer_name}: Final Accuracy vs LR',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Learning Rate')\n",
        "    axes[1].set_ylabel('Final Test Accuracy (%)')\n",
        "    axes[1].set_xscale('log')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_per_class_accuracy_heatmap(histories):\n",
        "    \"\"\"Heatmap of per-class accuracy at final epoch\"\"\"\n",
        "    final_epoch_data = []\n",
        "    optimizer_names = []\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        final_checkpoint = history['checkpoints'][max(history['checkpoints'].keys())]\n",
        "        final_epoch_data.append(final_checkpoint['per_class_acc'])\n",
        "        optimizer_names.append(name)\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Create heatmap\n",
        "    # Use seaborn to create annotated heatmap\n",
        "\n",
        "    ...\n",
        "\n",
        "    ######### End TODO #########\n",
        "\n",
        "    plt.title('Per-Class Accuracy (Final Epoch)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Class', fontsize=12)\n",
        "    plt.ylabel('Optimizer', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_loss_landscape_2d(model, testloader, histories, device):\n",
        "    \"\"\"\n",
        "    Visualize loss landscape using PCA projection\n",
        "    This shows the optimization trajectory in 2D\n",
        "    \"\"\"\n",
        "    print(\"\\nGenerating loss landscape visualization...\")\n",
        "\n",
        "    # Collect all parameter vectors from all checkpoints\n",
        "    all_param_vectors = []\n",
        "    trajectories = {}\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        trajectory = []\n",
        "        for epoch in sorted(history['checkpoints'].keys()):\n",
        "            params = []\n",
        "            state_dict = history['checkpoints'][epoch]['model_state']\n",
        "            for key in sorted(state_dict.keys()):\n",
        "                params.append(state_dict[key].cpu().flatten())\n",
        "            param_vector = torch.cat(params).numpy()\n",
        "            all_param_vectors.append(param_vector)\n",
        "            trajectory.append(len(all_param_vectors) - 1)\n",
        "        trajectories[name] = trajectory\n",
        "\n",
        "    # Apply PCA\n",
        "    if len(all_param_vectors) > 2:\n",
        "        pca = PCA(n_components=2)\n",
        "        projected = pca.fit_transform(all_param_vectors)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot trajectories\n",
        "        start_idx = 0\n",
        "        colors = plt.cm.tab10(np.linspace(0, 1, len(histories)))\n",
        "\n",
        "        for (name, traj), color in zip(trajectories.items(), colors):\n",
        "            traj_points = projected[traj]\n",
        "            plt.plot(traj_points[:, 0], traj_points[:, 1],\n",
        "                    'o-', label=name, linewidth=2, markersize=8, color=color)\n",
        "            # Mark start and end\n",
        "            plt.scatter(traj_points[0, 0], traj_points[0, 1],\n",
        "                       s=200, marker='*', color=color, edgecolor='black', linewidth=2)\n",
        "            plt.scatter(traj_points[-1, 0], traj_points[-1, 1],\n",
        "                       s=200, marker='s', color=color, edgecolor='black', linewidth=2)\n",
        "\n",
        "        plt.title('Optimization Trajectories in Loss Landscape (PCA Projection)',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
        "        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.1f}%\")"
      ],
      "metadata": {
        "id": "ChnjjQm-zGVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. EXPERIMENT RUNNERS"
      ],
      "metadata": {
        "id": "vM09J4GxzKaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_optimizer_comparison():\n",
        "    \"\"\"\n",
        "    Experiment 1: Compare Adagrad, RMSProp, and Adam\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 1: ADAPTIVE OPTIMIZER COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    num_epochs = 30\n",
        "    checkpoint_epochs = [5, 10, 20, 30]\n",
        "\n",
        "    # Load data\n",
        "    trainloader, testloader = load_fashion_mnist(batch_size=128)\n",
        "\n",
        "    # Define optimizers to test\n",
        "    optimizers_config = [\n",
        "        (\"Adagrad\", lambda p: AdagradOptimizer(p, lr=0.01)),\n",
        "        (\"RMSProp\", lambda p: RMSPropOptimizer(p, lr=0.001, alpha=0.9)),\n",
        "        (\"Adam\", lambda p: AdamOptimizer(p, lr=0.001, beta1=0.9, beta2=0.999)),\n",
        "    ]\n",
        "\n",
        "    histories = {}\n",
        "\n",
        "    for opt_name, opt_fn in optimizers_config:\n",
        "        print(f\"\\n--- Training with {opt_name} ---\")\n",
        "\n",
        "        # Create fresh model\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = opt_fn(model.parameters())\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Train\n",
        "        history = ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        histories[opt_name] = history\n",
        "\n",
        "    # Visualizations\n",
        "    plot_training_curves(histories, \"Adaptive Optimizers Comparison\")\n",
        "    plot_gradient_statistics(histories, \"Gradient Statistics\")\n",
        "    plot_effective_learning_rates(histories, \"Learning Rate Adaptation\")\n",
        "    plot_parameter_update_magnitudes(histories, \"Parameter Update Magnitudes\")\n",
        "    plot_convergence_speed(histories, threshold=85)\n",
        "    plot_per_class_accuracy_heatmap(histories)\n",
        "\n",
        "    # Loss landscape\n",
        "    visualize_loss_landscape_2d(DeepMLP().to(device), testloader, histories, device)\n",
        "\n",
        "    # Comparison table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPTIMIZER COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Optimizer':<15} {'Final Acc':<12} {'Best Acc':<12} {'Converge@85%':<15} {'Avg Time/Epoch':<15}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "\n",
        "        # Find convergence epoch\n",
        "        converge_epoch = \"N/A\"\n",
        "        for i, acc in enumerate(history['test_acc']):\n",
        "            if acc >= 85:\n",
        "                converge_epoch = str(i + 1)\n",
        "                break\n",
        "\n",
        "        avg_time = np.mean(history['time_per_epoch'])\n",
        "        print(f\"{name:<15} {final_acc:<12.2f} {best_acc:<12.2f} {converge_epoch:<15} {avg_time:<15.2f}\")\n",
        "\n",
        "    return histories\n",
        "\n",
        "def experiment_lr_robustness():\n",
        "    \"\"\"\n",
        "    Experiment 2: Test robustness to different initial learning rates\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 2: LEARNING RATE ROBUSTNESS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    num_epochs = 30\n",
        "    checkpoint_epochs = [10, 20, 30]\n",
        "    learning_rates = [0.0001, 0.001, 0.01]\n",
        "\n",
        "    # Load data\n",
        "    trainloader, testloader = load_fashion_mnist(batch_size=128)\n",
        "\n",
        "    all_histories = {}\n",
        "\n",
        "    # Test each optimizer with different LRs\n",
        "    print(\"\\n--- Testing Adagrad ---\")\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nLearning Rate: {lr}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = AdagradOptimizer(model.parameters(), lr=lr)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"Adagrad_LR{lr}\", num_epochs, checkpoint_epochs, device)\n",
        "        all_histories[f\"Adagrad_LR{lr}\"] = history\n",
        "\n",
        "    print(\"\\n--- Testing RMSProp ---\")\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nLearning Rate: {lr}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = RMSPropOptimizer(model.parameters(), lr=lr)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"RMSProp_LR{lr}\", num_epochs, checkpoint_epochs, device)\n",
        "        all_histories[f\"RMSProp_LR{lr}\"] = history\n",
        "\n",
        "    print(\"\\n--- Testing Adam ---\")\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nLearning Rate: {lr}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = AdamOptimizer(model.parameters(), lr=lr)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"Adam_LR{lr}\", num_epochs, checkpoint_epochs, device)\n",
        "        all_histories[f\"Adam_LR{lr}\"] = history\n",
        "\n",
        "    # Visualizations\n",
        "    plot_lr_robustness(all_histories, \"Adagrad\")\n",
        "    plot_lr_robustness(all_histories, \"RMSProp\")\n",
        "    plot_lr_robustness(all_histories, \"Adam\")\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LEARNING RATE SENSITIVITY ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Optimizer':<20} {'LR':<10} {'Final Acc':<12} {'Std Dev':<12} {'Stable?':<10}\")\n",
        "    print(\"-\"*80)\n",
        "\n",
        "    for name, history in all_histories.items():\n",
        "        lr = float(name.split('_LR')[1])\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        last_10_std = np.std(history['test_acc'][-10:])\n",
        "        stable = \"Yes\" if last_10_std < 1.5 else \"No\"\n",
        "        print(f\"{name:<20} {lr:<10.4f} {final_acc:<12.2f} {last_10_std:<12.2f} {stable:<10}\")\n",
        "\n",
        "    # Calculate robustness score\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ROBUSTNESS SCORE (Higher = More Robust)\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for opt_type in [\"Adagrad\", \"RMSProp\", \"Adam\"]:\n",
        "        opt_results = [all_histories[k]['test_acc'][-1] for k in all_histories if opt_type in k]\n",
        "        robustness = np.mean(opt_results) - np.std(opt_results)\n",
        "        print(f\"{opt_type:<15} Score: {robustness:.2f} (Mean: {np.mean(opt_results):.2f}, Std: {np.std(opt_results):.2f})\")\n",
        "\n",
        "    return all_histories\n",
        "\n",
        "def experiment_adam_hyperparameters():\n",
        "    \"\"\"\n",
        "    Experiment 3: Adam hyperparameter sensitivity (beta1, beta2, epsilon)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 3: ADAM HYPERPARAMETER SENSITIVITY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    num_epochs = 30\n",
        "    checkpoint_epochs = [10, 20, 30]\n",
        "    lr = 0.001\n",
        "\n",
        "    # Load data\n",
        "    trainloader, testloader = load_fashion_mnist(batch_size=128)\n",
        "\n",
        "    # Test different beta1 values\n",
        "    print(\"\\n--- Testing Beta1 (Momentum) ---\")\n",
        "    beta1_values = [0.8, 0.9, 0.95, 0.99]\n",
        "    beta1_histories = {}\n",
        "\n",
        "    for beta1 in beta1_values:\n",
        "        print(f\"\\nBeta1: {beta1}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = AdamOptimizer(model.parameters(), lr=lr, beta1=beta1, beta2=0.999)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"Adam_β1={beta1}\", num_epochs, checkpoint_epochs, device)\n",
        "        beta1_histories[f\"Adam_β1={beta1}\"] = history\n",
        "\n",
        "    # Test different beta2 values\n",
        "    print(\"\\n--- Testing Beta2 (RMSProp) ---\")\n",
        "    beta2_values = [0.9, 0.99, 0.999, 0.9999]\n",
        "    beta2_histories = {}\n",
        "\n",
        "    for beta2 in beta2_values:\n",
        "        print(f\"\\nBeta2: {beta2}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = AdamOptimizer(model.parameters(), lr=lr, beta1=0.9, beta2=beta2)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"Adam_β2={beta2}\", num_epochs, checkpoint_epochs, device)\n",
        "        beta2_histories[f\"Adam_β2={beta2}\"] = history\n",
        "\n",
        "    # Test different epsilon values\n",
        "    print(\"\\n--- Testing Epsilon (Numerical Stability) ---\")\n",
        "    epsilon_values = [1e-10, 1e-8, 1e-7, 1e-6]\n",
        "    epsilon_histories = {}\n",
        "\n",
        "    for eps in epsilon_values:\n",
        "        print(f\"\\nEpsilon: {eps}\")\n",
        "        model = DeepMLP().to(device)\n",
        "        optimizer = AdamOptimizer(model.parameters(), lr=lr, beta1=0.9, beta2=0.999, eps=eps)\n",
        "        history = train_model(model, trainloader, testloader, optimizer,\n",
        "                            f\"Adam_ε={eps}\", num_epochs, checkpoint_epochs, device)\n",
        "        epsilon_histories[f\"Adam_ε={eps}\"] = history\n",
        "\n",
        "    # Visualizations\n",
        "    plot_training_curves(beta1_histories, \"Adam: Beta1 Sensitivity\")\n",
        "    plot_training_curves(beta2_histories, \"Adam: Beta2 Sensitivity\")\n",
        "    plot_training_curves(epsilon_histories, \"Adam: Epsilon Sensitivity\")\n",
        "\n",
        "    # Analysis tables\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BETA1 (MOMENTUM) ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Beta1':<10} {'Final Acc':<12} {'Best Acc':<12} {'Convergence':<15}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in beta1_histories.items():\n",
        "        beta1 = name.split('=')[1]\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        converge = next((i+1 for i, acc in enumerate(history['test_acc']) if acc >= 85), \"N/A\")\n",
        "        print(f\"{beta1:<10} {final_acc:<12.2f} {best_acc:<12.2f} {str(converge):<15}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BETA2 (RMSPROP) ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Beta2':<10} {'Final Acc':<12} {'Best Acc':<12} {'Convergence':<15}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in beta2_histories.items():\n",
        "        beta2 = name.split('=')[1]\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        converge = next((i+1 for i, acc in enumerate(history['test_acc']) if acc >= 85), \"N/A\")\n",
        "        print(f\"{beta2:<10} {final_acc:<12.2f} {best_acc:<12.2f} {str(converge):<15}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EPSILON (NUMERICAL STABILITY) ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Epsilon':<15} {'Final Acc':<12} {'Best Acc':<12} {'Stability':<12}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in epsilon_histories.items():\n",
        "        eps = name.split('=')[1]\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        stability = np.std(history['test_acc'][-10:])\n",
        "        print(f\"{eps:<15} {final_acc:<12.2f} {best_acc:<12.2f} {stability:<12.2f}\")\n",
        "\n",
        "\n",
        "    all_adam_histories = {**beta1_histories, **beta2_histories, **epsilon_histories}\n",
        "    return all_adam_histories\n"
      ],
      "metadata": {
        "id": "LgPyYA9xzMxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. MAIN EXECUTION"
      ],
      "metadata": {
        "id": "Q9FeuPo5zPnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*80)\n",
        "    print(\"DEEP LEARNING OPTIMIZATION ASSIGNMENT - PART 2\")\n",
        "    print(\"Adaptive Learning Rate Optimizers\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Run experiments\n",
        "    print(\"\\n[1/3] Running Optimizer Comparison Experiment...\")\n",
        "    comparison_histories = experiment_optimizer_comparison()\n",
        "\n",
        "    print(\"\\n[2/3] Running Learning Rate Robustness Experiment...\")\n",
        "    lr_histories = experiment_lr_robustness()\n",
        "\n",
        "    print(\"\\n[3/3] Running Adam Hyperparameter Sensitivity Experiment...\")\n",
        "    adam_histories = experiment_adam_hyperparameters()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EXPERIMENTS COMPLETED! Review all results and write your report.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "id": "iBs9cG7kzQxL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}