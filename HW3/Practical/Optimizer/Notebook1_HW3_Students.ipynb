{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Optimization Assignment - Part 1\n",
        "\n",
        "Momentum-Based Optimizers: SGD, SGD+Momentum, SGD+Nesterov\n",
        "\n",
        "Students: Complete the sections marked with 'TODO' comments"
      ],
      "metadata": {
        "id": "yNrPGAFX31wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import copy\n",
        "from collections import defaultdict\n",
        "import gc\n",
        "\n",
        "# ============================================================================\n",
        "# CLEAR GPU MEMORY FIRST\n",
        "# ============================================================================\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "    print(\"✓ GPU memory cleared!\")\n"
      ],
      "metadata": {
        "id": "NdQc7ADOUp6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. DATA LOADING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "K0hRVG36VImu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10(batch_size_percentage=100, subset_size=50000, use_accumulation=False):\n",
        "    \"\"\"\n",
        "    Load CIFAR-10 dataset with controllable batch size\n",
        "\n",
        "    Args:\n",
        "        batch_size_percentage: Percentage of training data to use per batch\n",
        "                             100% = Full Batch GD (simulated with accumulation)\n",
        "                             1-99% = Mini-batch SGD\n",
        "        subset_size: Number of training samples to use (default: 50000)\n",
        "        use_accumulation: If True, use smaller physical batches with accumulation\n",
        "    \"\"\"\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    ########## TODO ##########\n",
        "    transform_test = ...\n",
        "\n",
        "\n",
        "    # Load full dataset\n",
        "    trainset_full = ...\n",
        "    testset = ...\n",
        "\n",
        "    # Calculate batch size\n",
        "    total_train_samples = len(trainset_full)\n",
        "    desired_batch_size = max(1, int(total_train_samples * batch_size_percentage / 100))\n",
        "\n",
        "    # For gradient accumulation with large batches\n",
        "    if use_accumulation and desired_batch_size > 500:\n",
        "        # Use smaller physical batch size and accumulate gradients\n",
        "        physical_batch_size = 256\n",
        "        accumulation_steps = max(1, desired_batch_size // physical_batch_size)\n",
        "        effective_batch_size = physical_batch_size * accumulation_steps\n",
        "\n",
        "        print(f\"Using gradient accumulation:\")\n",
        "        print(f\"  Physical batch size: {physical_batch_size}\")\n",
        "        print(f\"  Accumulation steps: {accumulation_steps}\")\n",
        "        print(f\"  Effective batch size: {effective_batch_size} (~{batch_size_percentage}% of data)\")\n",
        "    else:\n",
        "        physical_batch_size = desired_batch_size\n",
        "        accumulation_steps = 1\n",
        "        effective_batch_size = desired_batch_size\n",
        "\n",
        "    trainloader = ...\n",
        "    testloader = ...\n",
        "    ########## End TODO ##########\n",
        "    print(f\"Batches per epoch: {len(trainloader)}\")\n",
        "\n",
        "    return trainloader, testloader, accumulation_steps\n",
        "\n",
        "# CIFAR-10 classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "          'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "metadata": {
        "id": "XbzUBjsiVLOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. MODEL DEFINITION (CNN)"
      ],
      "metadata": {
        "id": "LZ8GYBvyWU2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN for CIFAR-10 - Memory efficient\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNNModel, self).__init__()\n",
        "\n",
        "        ######### TODO ##########\n",
        "        # Implement your CNN Architecture\n",
        "\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def forward(self, x):\n",
        "        ######### TODO ##########\n",
        "\n",
        "        ######### End TODO #########\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NdV2d7W8WYUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CUSTOM OPTIMIZER IMPLEMENTATIONS"
      ],
      "metadata": {
        "id": "5jdYkaczWyBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDOptimizer:\n",
        "    \"\"\"\n",
        "    Custom SGD optimizer implementation\n",
        "    Supports: vanilla SGD, SGD+Momentum, SGD+Nesterov Momentum\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=0.01, momentum=0.0, nesterov=False, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params: Model parameters\n",
        "            lr: Learning rate\n",
        "            momentum: Momentum factor (0 for vanilla SGD)\n",
        "            nesterov: Whether to use Nesterov momentum\n",
        "            weight_decay: L2 regularization factor\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.nesterov = nesterov\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        ########## TODO ##########\n",
        "\n",
        "        # Initialize velocity buffers for momentum\n",
        "        self.velocity = ...\n",
        "\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Clear gradients\"\"\"\n",
        "        ########## TODO ##########\n",
        "        ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Perform optimization step\"\"\"\n",
        "        with torch.no_grad():\n",
        "            for i, p in enumerate(self.params):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                ########## TODO ##########\n",
        "                # Implement the optimization step\n",
        "                # 1. Get gradient\n",
        "                # 2. Apply weight decay if self.weight_decay > 0\n",
        "                # 3. If self.momentum > 0:\n",
        "                #    a) Update velocity\n",
        "                #    b) If self.nesterov:\n",
        "                #       - Apply Nesterov momentum\n",
        "                #    c) Else (classical momentum)\n",
        "                # 4. Update parameter\n",
        "\n",
        "                d_p = ...\n",
        "\n",
        "                # Apply weight decay\n",
        "                if self.weight_decay != 0:\n",
        "                    ...\n",
        "\n",
        "                # Apply momentum\n",
        "                if self.momentum != 0:\n",
        "                    ...\n",
        "\n",
        "                # Update parameters\n",
        "                p.add_(...)\n",
        "\n",
        "                ######### End TODO #########\n"
      ],
      "metadata": {
        "id": "RW_7iqLiW0rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. TRAINING AND EVALUATION FUNCTIONS (WITH GRADIENT ACCUMULATION)"
      ],
      "metadata": {
        "id": "Z42BtfDvXNVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, trainloader, optimizer, criterion, device, accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    Train for one epoch with gradient accumulation support\n",
        "\n",
        "    Args:\n",
        "        accumulation_steps: Number of batches to accumulate gradients over\n",
        "                          If > 1, simulates larger batch size\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Track gradient norms\n",
        "    grad_norms = []\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Implement training step with gradient accumulation\n",
        "        # 1. Forward pass\n",
        "        # 2. Compute loss\n",
        "        # 3. Normalize loss by accumulation_steps\n",
        "        # 4. Backward pass\n",
        "        # 5. If (batch_idx + 1) % accumulation_steps == 0:\n",
        "        #    - optimizer.step()\n",
        "        #    - optimizer.zero_grad()\n",
        "\n",
        "        ...\n",
        "        loss = ...\n",
        "\n",
        "        # Normalize loss to account for accumulation\n",
        "        loss = ...\n",
        "        ...\n",
        "\n",
        "        # Update weights every accumulation_steps\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            # Calculate gradient norm before optimizer step\n",
        "            total_norm = 0.0\n",
        "            for p in model.parameters():\n",
        "                ...\n",
        "            grad_norms.append(total_norm ** 0.5)\n",
        "\n",
        "            ...\n",
        "            ...\n",
        "\n",
        "        ######### End TODO #########\n",
        "\n",
        "        # Statistics (use original loss for tracking)\n",
        "        running_loss += loss.item() * accumulation_steps\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    # Update any remaining gradients\n",
        "    if (len(trainloader)) % accumulation_steps != 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss = running_loss / len(trainloader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "    avg_grad_norm = np.mean(grad_norms) if grad_norms else 0.0\n",
        "\n",
        "    return epoch_loss, epoch_acc, avg_grad_norm\n",
        "\n",
        "def evaluate(model, testloader, criterion, device):\n",
        "    \"\"\"Evaluate model on test set\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Per-class accuracy\n",
        "    class_correct = [0] * 10\n",
        "    class_total = [0] * 10\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            ########## TODO ##########\n",
        "            # Implement evaluation step\n",
        "            # 1. Forward pass\n",
        "            # 2. Compute loss\n",
        "\n",
        "            ...\n",
        "            ...\n",
        "\n",
        "            ######### End TODO #########\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            # Per-class accuracy\n",
        "            c = (predicted == targets)\n",
        "            for i in range(len(targets)):\n",
        "                label = targets[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    test_loss = running_loss / len(testloader)\n",
        "    test_acc = 100. * correct / total\n",
        "\n",
        "    per_class_acc = [100. * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "                     for i in range(10)]\n",
        "\n",
        "    return test_loss, test_acc, per_class_acc\n",
        "\n",
        "def train_model(model, trainloader, testloader, optimizer_name,\n",
        "                lr, momentum, nesterov, num_epochs, checkpoint_epochs,\n",
        "                device, accumulation_steps=1):\n",
        "    \"\"\"\n",
        "    Complete training pipeline with checkpointing and gradient accumulation\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        trainloader: Training data loader\n",
        "        testloader: Test data loader\n",
        "        optimizer_name: Name for logging\n",
        "        lr: Learning rate\n",
        "        momentum: Momentum factor\n",
        "        nesterov: Use Nesterov momentum\n",
        "        num_epochs: Total training epochs\n",
        "        checkpoint_epochs: List of epochs to save checkpoints\n",
        "        device: Training device\n",
        "        accumulation_steps: Gradient accumulation steps\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with training history and checkpoints\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = SGDOptimizer(model.parameters(), lr=lr, momentum=momentum,\n",
        "                            nesterov=nesterov, weight_decay=5e-4)\n",
        "\n",
        "    # History tracking\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'grad_norms': [],\n",
        "        'time_per_epoch': [],\n",
        "        'checkpoints': {},\n",
        "        'accumulation_steps': accumulation_steps\n",
        "    }\n",
        "\n",
        "    print(f\"\\nTraining with {optimizer_name}\")\n",
        "    print(f\"Learning rate: {lr}, Momentum: {momentum}, Nesterov: {nesterov}\")\n",
        "    if accumulation_steps > 1:\n",
        "        print(f\"Gradient accumulation steps: {accumulation_steps}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    ########## TODO ##########\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc, avg_grad_norm = ...\n",
        "\n",
        "        # Evaluate\n",
        "        test_loss, test_acc, per_class_acc = ...\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        ########## END TODO ##########\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        history['grad_norms'].append(avg_grad_norm)\n",
        "        history['time_per_epoch'].append(epoch_time)\n",
        "\n",
        "        # Save checkpoint\n",
        "        if epoch in checkpoint_epochs:\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state': copy.deepcopy(model.state_dict()),\n",
        "                'train_loss': train_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'test_loss': test_loss,\n",
        "                'test_acc': test_acc,\n",
        "                'per_class_acc': per_class_acc\n",
        "            }\n",
        "            history['checkpoints'][epoch] = checkpoint\n",
        "            print(f\"✓ Checkpoint saved at epoch {epoch}\")\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d}/{num_epochs} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}% | \"\n",
        "                  f\"Grad Norm: {avg_grad_norm:.4f} | Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        # Clear cache periodically\n",
        "        if epoch % 10 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "MDxRUReCXPz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. VISUALIZATION FUNCTIONS"
      ],
      "metadata": {
        "id": "a6jzItOXXTwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_curves(histories, title=\"Training Curves\"):\n",
        "    \"\"\"Plot loss and accuracy curves for multiple optimizers\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        axes[0, 0].plot(history['train_loss'], label=name, linewidth=2)\n",
        "        axes[0, 1].plot(history['test_loss'], label=name, linewidth=2)\n",
        "        axes[1, 0].plot(history['train_acc'], label=name, linewidth=2)\n",
        "        axes[1, 1].plot(history['test_acc'], label=name, linewidth=2)\n",
        "\n",
        "    axes[0, 0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].set_title('Test Loss', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].set_title('Training Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].set_title('Test Accuracy', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_gradient_norms(histories, title=\"Gradient Norms Over Time\"):\n",
        "    \"\"\"Plot gradient norms for different optimizers\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Plot gradient norms\n",
        "    # For each optimizer in histories.items()\n",
        "\n",
        "    ...\n",
        "\n",
        "    ######### End TODO #########\n",
        "\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Average Gradient Norm', fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_checkpoint_comparison(histories, checkpoint_epoch):\n",
        "    \"\"\"Compare optimizer performance at specific checkpoint\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    names = []\n",
        "    train_accs = []\n",
        "    test_accs = []\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        if checkpoint_epoch in history['checkpoints']:\n",
        "            checkpoint = history['checkpoints'][checkpoint_epoch]\n",
        "            names.append(name)\n",
        "            train_accs.append(checkpoint['train_acc'])\n",
        "            test_accs.append(checkpoint['test_acc'])\n",
        "\n",
        "    x = np.arange(len(names))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0].bar(x, train_accs, width, color='skyblue', edgecolor='black')\n",
        "    axes[0].set_title(f'Training Accuracy at Epoch {checkpoint_epoch}',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[0].set_ylabel('Accuracy (%)')\n",
        "    axes[0].set_xticks(x)\n",
        "    axes[0].set_xticklabels(names, rotation=45, ha='right')\n",
        "    axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    axes[1].bar(x, test_accs, width, color='lightcoral', edgecolor='black')\n",
        "    axes[1].set_title(f'Test Accuracy at Epoch {checkpoint_epoch}',\n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[1].set_ylabel('Accuracy (%)')\n",
        "    axes[1].set_xticks(x)\n",
        "    axes[1].set_xticklabels(names, rotation=45, ha='right')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_per_class_accuracy_heatmap(histories):\n",
        "    \"\"\"Create heatmap of per-class accuracy for final epoch\"\"\"\n",
        "    final_epoch_data = []\n",
        "    optimizer_names = []\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        # Get final checkpoint\n",
        "        final_checkpoint = history['checkpoints'][max(history['checkpoints'].keys())]\n",
        "        final_epoch_data.append(final_checkpoint['per_class_acc'])\n",
        "        optimizer_names.append(name)\n",
        "\n",
        "    ########## TODO ##########\n",
        "    # Create heatmap using seaborn\n",
        "\n",
        "    ...\n",
        "\n",
        "    ######### End TODO #########\n",
        "\n",
        "    plt.title('Per-Class Accuracy Comparison (Final Epoch)',\n",
        "             fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Class', fontsize=12)\n",
        "    plt.ylabel('Optimizer', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_time_vs_accuracy(histories):\n",
        "    \"\"\"Plot trade-off between training time and accuracy\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    for name, history in histories.items():\n",
        "        total_time = sum(history['time_per_epoch'])\n",
        "        final_acc = history['test_acc'][-1]\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Create scatter plot\n",
        "        # Plot total_time vs final_acc for each optimizer\n",
        "\n",
        "        ...\n",
        "\n",
        "        ######### End TODO #########\n",
        "\n",
        "    ax.set_xlabel('Total Training Time (seconds)', fontsize=12)\n",
        "    ax.set_ylabel('Final Test Accuracy (%)', fontsize=12)\n",
        "    ax.set_title('Training Time vs Accuracy Trade-off',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_batch_size_effect_detailed(histories):\n",
        "    \"\"\"Detailed analysis of batch size effects\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Extract batch sizes and sort\n",
        "    batch_info = []\n",
        "    for name, history in histories.items():\n",
        "        if 'SGD-' in name:\n",
        "            batch_pct = int(name.split('-')[1].replace('%', ''))\n",
        "            batch_info.append((batch_pct, name, history))\n",
        "    batch_info.sort()\n",
        "\n",
        "    # Convergence curves\n",
        "    for batch_pct, name, history in batch_info:\n",
        "        axes[0, 0].plot(history['test_acc'], label=f'{batch_pct}%', linewidth=2)\n",
        "    axes[0, 0].set_title('Convergence Speed', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Test Accuracy (%)')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Final accuracy vs batch size\n",
        "    batch_sizes = [x[0] for x in batch_info]\n",
        "    final_accs = [x[2]['test_acc'][-1] for x in batch_info]\n",
        "    axes[0, 1].plot(batch_sizes, final_accs, 'o-', markersize=10, linewidth=2)\n",
        "    axes[0, 1].set_title('Final Accuracy vs Batch Size', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Batch Size (%)')\n",
        "    axes[0, 1].set_ylabel('Final Test Accuracy (%)')\n",
        "    axes[0, 1].set_xscale('log')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Training time vs batch size\n",
        "    total_times = [sum(x[2]['time_per_epoch']) for x in batch_info]\n",
        "    axes[1, 0].plot(batch_sizes, total_times, 's-', markersize=10, linewidth=2, color='green')\n",
        "    axes[1, 0].set_title('Total Training Time vs Batch Size', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Batch Size (%)')\n",
        "    axes[1, 0].set_ylabel('Total Time (seconds)')\n",
        "    axes[1, 0].set_xscale('log')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Gradient noise (stability)\n",
        "    noise_levels = [np.std(x[2]['test_acc'][-10:]) for x in batch_info]\n",
        "    axes[1, 1].plot(batch_sizes, noise_levels, '^-', markersize=10, linewidth=2, color='red')\n",
        "    axes[1, 1].set_title('Training Stability (Variance in Last 10 Epochs)',\n",
        "                        fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Batch Size (%)')\n",
        "    axes[1, 1].set_ylabel('Std Dev of Test Accuracy')\n",
        "    axes[1, 1].set_xscale('log')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle('Detailed Batch Size Effect Analysis', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "s6G9AGwIXVvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. EXPERIMENT RUNNERS"
      ],
      "metadata": {
        "id": "6QYBwckbXa0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_batch_size_effect():\n",
        "    \"\"\"\n",
        "    Experiment 1: Effect of batch size on SGD\n",
        "    Using gradient accumulation for large batches\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 1: BATCH SIZE EFFECT ON SGD\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Test different effective batch sizes\n",
        "    batch_percentages = [100, 50, 20, 5, 1]\n",
        "\n",
        "    num_epochs = 30\n",
        "    lr = 0.1\n",
        "    checkpoint_epochs = [5, 10, 20, 30]\n",
        "\n",
        "    histories = {}\n",
        "\n",
        "    for batch_pct in batch_percentages:\n",
        "        print(f\"\\n--- Training with {batch_pct}% batch size ---\")\n",
        "\n",
        "        # Clear memory before each run\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Load data with gradient accumulation for large batches\n",
        "        use_accumulation = (batch_pct >= 50)\n",
        "        trainloader, testloader, accumulation_steps = load_cifar10(\n",
        "            batch_size_percentage=batch_pct,\n",
        "            use_accumulation=use_accumulation\n",
        "        )\n",
        "\n",
        "        # Create model\n",
        "        model = CNNModel().to(device)\n",
        "\n",
        "        # Train\n",
        "        name = f\"SGD-{batch_pct}%\"\n",
        "        ########## TODO ##########\n",
        "        history = ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        histories[name] = history\n",
        "\n",
        "        # Clear model from memory\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    # Visualizations\n",
        "    plot_training_curves(histories, \"Batch Size Effect on SGD\")\n",
        "    plot_batch_size_effect_detailed(histories)\n",
        "    plot_time_vs_accuracy(histories)\n",
        "\n",
        "    # Comparison table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BATCH SIZE COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Batch Size':<15} {'Final Acc':<12} {'Best Acc':<12} {'Total Time':<12} {'Stability':<12}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in histories.items():\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        total_time = sum(history['time_per_epoch'])\n",
        "        stability = np.std(history['test_acc'][-10:])\n",
        "        print(f\"{name:<15} {final_acc:<12.2f} {best_acc:<12.2f} {total_time:<12.2f} {stability:<12.2f}\")\n",
        "\n",
        "    return histories\n",
        "\n",
        "def experiment_momentum_variants():\n",
        "    \"\"\"\n",
        "    Experiment 2: Compare SGD, SGD+Momentum, SGD+Nesterov\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 2: MOMENTUM VARIANTS COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    num_epochs = 30\n",
        "    lr = 0.1\n",
        "    momentum = 0.9\n",
        "    checkpoint_epochs = [5, 10, 20, 30]\n",
        "    batch_pct = 1  # Use mini-batch SGD\n",
        "\n",
        "    # Clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Load data once\n",
        "    trainloader, testloader, accumulation_steps = load_cifar10(\n",
        "        batch_size_percentage=batch_pct,\n",
        "        use_accumulation=False\n",
        "    )\n",
        "\n",
        "    optimizers_config = [\n",
        "        (\"SGD\", 0.0, False),\n",
        "        (\"SGD+Momentum\", momentum, False),\n",
        "        (\"SGD+Nesterov\", momentum, True),\n",
        "    ]\n",
        "\n",
        "    histories = {}\n",
        "\n",
        "    for opt_name, mom, nesterov in optimizers_config:\n",
        "        print(f\"\\n--- Training with {opt_name} ---\")\n",
        "\n",
        "        # Clear memory\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Create fresh model\n",
        "        model = CNNModel().to(device)\n",
        "\n",
        "        ########## TODO ##########\n",
        "        # Train\n",
        "        history = ...\n",
        "        ######### End TODO #########\n",
        "\n",
        "        histories[opt_name] = history\n",
        "\n",
        "        # Clear model\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    # Visualizations\n",
        "    plot_training_curves(histories, \"Momentum Variants Comparison\")\n",
        "    plot_gradient_norms(histories, \"Gradient Norms: Momentum Variants\")\n",
        "\n",
        "    # Checkpoint comparisons\n",
        "    for epoch in [10, 40, 80]:\n",
        "        if epoch in checkpoint_epochs:\n",
        "            plot_checkpoint_comparison(histories, epoch)\n",
        "\n",
        "    # Per-class accuracy\n",
        "    plot_per_class_accuracy_heatmap(histories)\n",
        "\n",
        "    # Final comparison table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MOMENTUM VARIANTS FINAL COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Optimizer':<20} {'Final Acc':<12} {'Best Acc':<12} {'Converge@70%':<15}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in histories.items():\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        # Convergence speed: epoch to reach 70% accuracy\n",
        "        target = 70.0\n",
        "        converged_epoch = next((i+1 for i, acc in enumerate(history['test_acc']) if acc >= target), num_epochs)\n",
        "        print(f\"{name:<20} {final_acc:<12.2f} {best_acc:<12.2f} {converged_epoch:<15d}\")\n",
        "\n",
        "\n",
        "    return histories\n",
        "\n",
        "def experiment_learning_rate_sensitivity():\n",
        "    \"\"\"\n",
        "    Experiment 3: Learning rate sensitivity for each optimizer\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXPERIMENT 3: LEARNING RATE SENSITIVITY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    num_epochs = 50\n",
        "    learning_rates = [0.01, 0.1, 0.5]\n",
        "    momentum = 0.9\n",
        "    checkpoint_epochs = [10, 25, 50]\n",
        "    batch_pct = 20\n",
        "\n",
        "    # Load data once\n",
        "    trainloader, testloader, accumulation_steps = load_cifar10(\n",
        "        batch_size_percentage=batch_pct,\n",
        "        use_accumulation=False\n",
        "    )\n",
        "\n",
        "    optimizers_config = [\n",
        "        (\"SGD\", 0.0, False),\n",
        "        (\"SGD+Momentum\", momentum, False),\n",
        "        (\"SGD+Nesterov\", momentum, True),\n",
        "    ]\n",
        "\n",
        "    all_histories = {}\n",
        "\n",
        "    for opt_name, mom, nesterov in optimizers_config:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Testing {opt_name} with different learning rates\")\n",
        "        print('='*70)\n",
        "\n",
        "        for lr in learning_rates:\n",
        "            print(f\"\\n--- LR = {lr} ---\")\n",
        "\n",
        "            # Clear memory\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Create fresh model\n",
        "            model = CNNModel().to(device)\n",
        "\n",
        "            # Train\n",
        "            exp_name = f\"{opt_name}_LR{lr}\"\n",
        "\n",
        "            ########## TODO ##########\n",
        "            history = ...\n",
        "            ######### End TODO #########\n",
        "\n",
        "            all_histories[exp_name] = history\n",
        "\n",
        "            # Clear model\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "    # Group by optimizer type\n",
        "    for opt_name, _, _ in optimizers_config:\n",
        "        opt_histories = {k: v for k, v in all_histories.items() if k.startswith(opt_name)}\n",
        "        plot_training_curves(opt_histories, f\"LR Sensitivity: {opt_name}\")\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LEARNING RATE SENSITIVITY SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Optimizer':<25} {'LR':<10} {'Final Acc':<12} {'Best Acc':<12} {'Stable?':<10}\")\n",
        "    print(\"-\"*80)\n",
        "    for name, history in all_histories.items():\n",
        "        lr = float(name.split('LR')[1])\n",
        "        final_acc = history['test_acc'][-1]\n",
        "        best_acc = max(history['test_acc'])\n",
        "        # Check stability (low variance in last 10 epochs)\n",
        "        last_10_var = np.var(history['test_acc'][-10:])\n",
        "        stable = \"Yes\" if last_10_var < 2.0 else \"No\"\n",
        "        print(f\"{name:<25} {lr:<10.2f} {final_acc:<12.2f} {best_acc:<12.2f} {stable:<10}\")\n",
        "\n",
        "\n",
        "    return all_histories\n"
      ],
      "metadata": {
        "id": "jWMaO2m3XdI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. MAIN EXECUTION"
      ],
      "metadata": {
        "id": "Fz94PfUOZ1k6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*80)\n",
        "    print(\"DEEP LEARNING OPTIMIZATION ASSIGNMENT - PART 1\")\n",
        "    print(\"Momentum-Based Optimizers with Gradient Accumulation\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Run experiments\n",
        "    print(\"\\n[1/3] Running Batch Size Effect Experiment...\")\n",
        "    batch_histories = experiment_batch_size_effect()\n",
        "\n",
        "    print(\"\\n[2/3] Running Momentum Variants Comparison...\")\n",
        "    momentum_histories = experiment_momentum_variants()\n",
        "\n",
        "    print(\"\\n[3/3] Running Learning Rate Sensitivity Analysis...\")\n",
        "    lr_histories = experiment_learning_rate_sensitivity()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL EXPERIMENTS COMPLETED! Review your results and write your report.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Final memory cleanup\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "metadata": {
        "id": "byxJz21gZ24R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}